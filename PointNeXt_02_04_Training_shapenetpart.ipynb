{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Nobobi-Hasan/PointNeXt-PartSegmentation-FallenTrees/blob/main/PointNeXt_02_04_Training_shapenetpart.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "import os\n",
        "import shutil\n",
        "import torch\n",
        "import subprocess\n",
        "import sys"
      ],
      "metadata": {
        "id": "YAhNWDtNOK3P"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Mount Google Drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "yhFy7Up_OC2x",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7f18a616-9f3a-4865-f14e-c028a690dfb9"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Path to the project root in Drive\n",
        "DRIVE_PROJECT_ROOT = \"/content/drive/MyDrive/ML_Projects/PointNeXt\"\n",
        "\n",
        "# Subfolders\n",
        "DRIVE_DATA_DIR = os.path.join(DRIVE_PROJECT_ROOT, \"Data\")\n",
        "DRIVE_MODELS_DIR = os.path.join(DRIVE_PROJECT_ROOT, \"Models\")\n",
        "\n",
        "# Input paths\n",
        "DRIVE_ZIP_PATH = os.path.join(DRIVE_DATA_DIR, \"processed_data.zip\")\n",
        "LOCAL_DATA_DIR = \"/content/processed_data\"\n",
        "\n",
        "print(f\"Project Root: {DRIVE_PROJECT_ROOT}\")"
      ],
      "metadata": {
        "id": "m16azjY-OFhX",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8a2d013f-d401-485b-a43c-47a9b263ccb8"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Project Root: /content/drive/MyDrive/ML_Projects/PointNeXt\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "dEOzQvBtN1sV"
      },
      "outputs": [],
      "source": [
        "# Copy processed data from Drive\n",
        "if not os.path.exists(\"/content/processed_data\"):\n",
        "    if os.path.exists(DRIVE_ZIP_PATH):\n",
        "        print(\"Copying processed_data.zip from Drive...\")\n",
        "        shutil.copy(DRIVE_ZIP_PATH, \"/content/processed_data.zip\")\n",
        "        print(\"Unzipping...\")\n",
        "        !unzip -q -o /content/processed_data.zip -d /\n",
        "        print(\"Data Ready at /content/processed_data\")\n",
        "    else:\n",
        "        print(f\"Error: Could not find processed_data.zip at {DRIVE_ZIP_PATH}\")\n",
        "\n",
        "# Clone PointNeXt\n",
        "if not os.path.exists(\"/content/PointNeXt\"):\n",
        "    print(\"Cloning PointNeXt...\")\n",
        "    %cd /content\n",
        "    !git clone https://github.com/guochengqian/PointNeXt.git"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## For openpoints"
      ],
      "metadata": {
        "id": "uWxJD5qWrUgR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# For openpoints\n",
        "\n",
        "%cd /content/PointNeXt\n",
        "\n",
        "# 1. Replace SSH url with HTTPS url in .gitmodules\n",
        "!sed -i 's/git@github.com:/https:\\/\\/github.com\\//' .gitmodules\n",
        "\n",
        "# 2. Sync the new URL\n",
        "!git submodule sync\n",
        "\n",
        "# 3. Update the submodule (This will work now)\n",
        "print(\"Downloading openpoints via HTTPS...\")\n",
        "!git submodule update --init --recursive\n",
        "\n",
        "print(\"\\u2705 Submodule 'openpoints' downloaded successfully.\")"
      ],
      "metadata": {
        "id": "xvcHjNHdVR_m",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "506a7d9e-4b78-4a20-885f-707d538af4fc"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/PointNeXt\n",
            "Synchronizing submodule url for 'openpoints'\n",
            "Downloading openpoints via HTTPS...\n",
            "‚úÖ Submodule 'openpoints' downloaded successfully.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Config File"
      ],
      "metadata": {
        "id": "1QKZI0MtrZch"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "config_path = \"/content/PointNeXt/cfgs/shapenetpart/custom_fallen_trees.yaml\"\n",
        "\n",
        "config_content = \"\"\"\n",
        "num_classes: 4\n",
        "shape_classes: 2\n",
        "epochs: 100\n",
        "\n",
        "# --- DATA CONFIG ---\n",
        "# Explicitly define feature keys to match the custom data loader\n",
        "feature_keys: 'pos,x'\n",
        "\n",
        "model:\n",
        "  NAME: BasePartSeg\n",
        "  encoder_args:\n",
        "    NAME: PointNextEncoder\n",
        "    blocks: [1, 1, 1, 1, 1]\n",
        "    strides: [1, 2, 2, 2, 2]\n",
        "    width: 32\n",
        "    in_channels: 7\n",
        "    sa_layers: 3\n",
        "    sa_use_res: True\n",
        "    radius: 0.1\n",
        "    radius_scaling: 2.5\n",
        "    nsample: 32\n",
        "    expansion: 4\n",
        "    aggr_args:\n",
        "      feature_type: 'dp_fj'\n",
        "    reduction: 'max'\n",
        "    group_args:\n",
        "      NAME: 'ballquery'\n",
        "      normalize_dp: True\n",
        "    conv_args:\n",
        "      order: conv-norm-act\n",
        "    act_args:\n",
        "      act: 'relu'\n",
        "    norm_args:\n",
        "      norm: 'bn'\n",
        "  decoder_args:\n",
        "    NAME: PointNextPartDecoder\n",
        "    cls_map: curvenet\n",
        "  cls_args:\n",
        "    NAME: SegHead\n",
        "    global_feat: max,avg\n",
        "    num_classes: 4\n",
        "    shape_classes: 2\n",
        "    in_channels: null\n",
        "    norm_args:\n",
        "      norm: 'bn'\n",
        "\n",
        "dataset:\n",
        "  common:\n",
        "    NAME: FallenTreePart\n",
        "    data_root: /content/processed_data\n",
        "    use_normal: False\n",
        "    use_xyz: True\n",
        "    num_points: 2048\n",
        "  train:\n",
        "    split: train\n",
        "  val:\n",
        "    split: val\n",
        "\n",
        "batch_size: 16\n",
        "dataloader:\n",
        "  num_workers: 4\n",
        "\n",
        "lr: 0.001\n",
        "min_lr: null\n",
        "optimizer:\n",
        "  NAME: adamw\n",
        "  weight_decay: 1.0e-4\n",
        "\n",
        "criterion_args:\n",
        "  NAME: Poly1FocalLoss\n",
        "\n",
        "# sched:\n",
        "#   # NAME: MultiStepLR\n",
        "#   NAME: MultiStepLRScheduler\n",
        "#   milestones: [70, 90]  # Drop LR at epoch 70 and 90\n",
        "#   gamma: 0.1\n",
        "#   warmup_epochs: 0\n",
        "\n",
        "# scheduler\n",
        "epochs: 100\n",
        "sched: multistep\n",
        "decay_epochs: [70, 90]\n",
        "decay_rate: 0.1\n",
        "warmup_epochs: 0\n",
        "\n",
        "datatransforms:\n",
        "  train: [PointsToTensor, PointCloudScaling, PointCloudCenterAndNormalize, PointCloudJitter, ChromaticDropGPU]\n",
        "  val: [PointsToTensor, PointCloudCenterAndNormalize]\n",
        "  kwargs:\n",
        "    jitter_sigma: 0.001\n",
        "    jitter_clip: 0.005\n",
        "    scale: [0.8, 1.2]\n",
        "    gravity_dim: 1\n",
        "    angle: [0, 1.0, 0]\n",
        "\n",
        "log_dir: /content/PointNeXt/log/shapenetpart/custom_trees\n",
        "\"\"\"\n",
        "\n",
        "with open(config_path, 'w') as f:\n",
        "    f.write(config_content)\n",
        "print(f\"\\u2705 Config Updated.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "G2bUEH8j4F8I",
        "outputId": "a3d295fa-87d3-4f71-be32-b1e30451067f"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚úÖ Config Updated.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Dataset Handle"
      ],
      "metadata": {
        "id": "B_HEgPDlrdMo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Create the directory\n",
        "new_dataset_dir = \"/content/PointNeXt/openpoints/dataset/fallentree\"\n",
        "os.makedirs(new_dataset_dir, exist_ok=True)\n",
        "print(f\"\\U0001F4BE Created folder: {new_dataset_dir}\")\n",
        "\n",
        "# Create the '__init__.py' to make it a package\n",
        "init_path = os.path.join(new_dataset_dir, \"__init__.py\")\n",
        "with open(init_path, 'w') as f:\n",
        "    f.write(\"from .fallentree import FallenTreePart\\n\")\n",
        "print(f\"\\u2705 Created: {init_path}\")\n",
        "\n",
        "# Create the 'fallentree.py' (The Custom Loader)\n",
        "code_path = os.path.join(new_dataset_dir, \"fallentree.py\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dYNH_fgPgvC8",
        "outputId": "6547b2f1-e9a1-4cfb-a31a-9ed0a4d1fb5e"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "üíæ Created folder: /content/PointNeXt/openpoints/dataset/fallentree\n",
            "‚úÖ Created: /content/PointNeXt/openpoints/dataset/fallentree/__init__.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Define Path\n",
        "code_path = \"/content/PointNeXt/openpoints/dataset/fallentree/fallentree.py\"\n",
        "\n",
        "# Define Code\n",
        "dataset_code = \"\"\"\n",
        "import os\n",
        "import glob\n",
        "import json\n",
        "import logging\n",
        "import numpy as np\n",
        "import torch\n",
        "from torch.utils.data import Dataset\n",
        "from ..build import DATASETS\n",
        "\n",
        "# NEW PROPORTIONAL SAMPLING FUNCTION\n",
        "def proportional_sample(xyz, part_labels, npoint):\n",
        "    \\\"\\\"\\\"\n",
        "    Proportional Stratified Sampling:\n",
        "    Calculates the % of each part in the original tree and keeps that same % in the final 2048 sample.\n",
        "    \\\"\\\"\\\"\n",
        "\n",
        "    total_points = len(xyz)\n",
        "    if total_points <= npoint:\n",
        "        # If tree is small, just repeat points (Upsample)\n",
        "        return np.random.choice(total_points, npoint, replace=True)\n",
        "\n",
        "    unique_parts, counts = np.unique(part_labels, return_counts=True)\n",
        "\n",
        "    # Calculate Ratios (e.g., Root is 5% of tree, Trunk is 95%)\n",
        "    ratios = counts / total_points\n",
        "\n",
        "    # Calculate target points (e.g., 5% of 2048 = 102 points)\n",
        "    target_counts = (ratios * npoint).astype(int)\n",
        "\n",
        "    final_indices = []\n",
        "\n",
        "    # Sample points per part\n",
        "    for part, count in zip(unique_parts, target_counts):\n",
        "        count = max(1, count)   # Ensure we take at least 1 point if the part exists\n",
        "\n",
        "        part_indices = np.where(part_labels == part)[0]\n",
        "\n",
        "        # Pick random points from this specific part\n",
        "        chosen = np.random.choice(part_indices, count, replace=False)\n",
        "        final_indices.extend(chosen)\n",
        "\n",
        "    # FILLING THE GAP (due to rounding)\n",
        "    current_count = len(final_indices)\n",
        "    if current_count < npoint:\n",
        "        needed = npoint - current_count\n",
        "        # Pick random points from the WHOLE tree to fill the tiny gap\n",
        "        remaining_fill = np.random.choice(total_points, needed, replace=False)\n",
        "        final_indices.extend(remaining_fill)\n",
        "\n",
        "    # If by some edge case we have too many, trim it\n",
        "    if current_count > npoint:\n",
        "        final_indices = final_indices[:npoint]\n",
        "\n",
        "    return np.array(final_indices)\n",
        "\n",
        "@DATASETS.register_module()\n",
        "class FallenTreePart(Dataset):\n",
        "    classes = ['standing', 'fallen']\n",
        "    num_classes = 4\n",
        "    shape_classes = 2\n",
        "\n",
        "    # --- FIX: Add dummy key -1 for part_seg_refinement compatibility ---\n",
        "    # -1 points to all parts (0,1,2,3) so the code can find the max index\n",
        "    cls2parts = {\n",
        "        0: [0],\n",
        "        1: [1, 2, 3],\n",
        "        -1: [0, 1, 2, 3]\n",
        "    }\n",
        "\n",
        "    part_start = [0, 1]\n",
        "\n",
        "    # Pre-compute embedding\n",
        "    cls2partembed = torch.zeros(shape_classes, num_classes)\n",
        "    for i in [0, 1]: # Iterate only real classes\n",
        "        idx = cls2parts[i]\n",
        "        cls2partembed[i].scatter_(0, torch.LongTensor(idx), 1)\n",
        "\n",
        "    def __init__(self,\n",
        "                 data_root,\n",
        "                 split=None,\n",
        "                 num_points=2048,\n",
        "                 use_normal=False,\n",
        "                 use_xyz=True,\n",
        "                 **kwargs):\n",
        "        self.root = data_root\n",
        "        self.npoints = num_points\n",
        "        self.split = split\n",
        "\n",
        "        # if split == 'val': split_name = 'test'\n",
        "        # else: split_name = split\n",
        "\n",
        "        split_name = split\n",
        "\n",
        "        split_file = os.path.join(self.root, 'train_test_split', f'shuffled_{split_name}_file_list.json')\n",
        "        if not os.path.exists(split_file):\n",
        "             raise FileNotFoundError(f\"Split list not found: {split_file}\")\n",
        "\n",
        "        logging.info(f\"Loading {split} split from: {split_file}\")\n",
        "        with open(split_file, 'r') as f:\n",
        "            raw_list = json.load(f)\n",
        "\n",
        "        self.file_list = []\n",
        "        for item in raw_list:\n",
        "            clean_item = item.replace(\\\"\\\\\\\\\\\", \\\"/\\\")\n",
        "            fname = os.path.basename(clean_item)\n",
        "            candidates = [\n",
        "                clean_item,\n",
        "                os.path.join(self.root, clean_item),\n",
        "                os.path.join(self.root, '0', fname),\n",
        "                os.path.join(self.root, '1', fname)\n",
        "            ]\n",
        "            found = False\n",
        "            for path in candidates:\n",
        "                if os.path.exists(path):\n",
        "                    self.file_list.append(path)\n",
        "                    found = True\n",
        "                    break\n",
        "\n",
        "        logging.info(f\"Found {len(self.file_list)} valid files for {split} split.\")\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        file_path = self.file_list[index]\n",
        "        cls_idx = 0 if '/0/' in file_path.replace('\\\\\\\\', '/') else 1\n",
        "\n",
        "        data = np.load(file_path).astype(np.float32)\n",
        "        xyz = data[:, 0:3]\n",
        "        features = data[:, 3:7]\n",
        "        part_label = data[:, 7].astype(np.int64)\n",
        "        cls_label = np.array([cls_idx]).astype(np.int64)\n",
        "\n",
        "        choice = proportional_sample(xyz, part_label, self.npoints)\n",
        "\n",
        "        # if len(xyz) >= self.npoints:\n",
        "        #     choice = np.random.choice(len(xyz), self.npoints, replace=False)\n",
        "        # else:\n",
        "        #     choice = np.random.choice(len(xyz), self.npoints, replace=True)\n",
        "\n",
        "        xyz = xyz[choice]\n",
        "        features = features[choice]\n",
        "        part_label = part_label[choice]\n",
        "\n",
        "        return {'pos': xyz, 'x': features, 'y': part_label, 'cls': cls_label}\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.file_list)\n",
        "\"\"\"\n",
        "\n",
        "with open(code_path, 'w') as f:\n",
        "    f.write(dataset_code)\n",
        "print(f\"\\u2705 Updated: {code_path}.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7o_C0iiSgg7N",
        "outputId": "fe509ba8-d834-430a-a9a0-99ad13e96db1"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚úÖ Updated: /content/PointNeXt/openpoints/dataset/fallentree/fallentree.py.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Register the new folder in the Main Library\n",
        "# We need to add \"from .fallentree import FallenTreePart\" to openpoints/dataset/__init__.py\n",
        "\n",
        "main_init = \"/content/PointNeXt/openpoints/dataset/__init__.py\"\n",
        "with open(main_init, 'r') as f:\n",
        "    content = f.read()\n",
        "\n",
        "if \"fallentree\" not in content:\n",
        "    print(\"Registering new dataset in main __init__.py...\")\n",
        "    with open(main_init, 'a') as f:\n",
        "        f.write(\"\\nfrom .fallentree import FallenTreePart\\n\")\n",
        "    print(\"\\u2705 Registration Complete.\")\n",
        "else:\n",
        "    print(\"\\u2705 Already registered.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Md659eXhgnNb",
        "outputId": "478ac64f-92dd-4b81-9f1b-9974deab173d"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚úÖ Already registered.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Install Dependencies"
      ],
      "metadata": {
        "id": "XmqBEuRjriVQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Install Dependencies\n",
        "print(\"Installing Dependencies...\")\n",
        "%cd /content/PointNeXt\n",
        "\n",
        "# A. Install PyTorch 2.4.0 (Compatible)\n",
        "!pip install torch==2.4.0 torchvision==0.19.0 torchaudio==2.4.0 --index-url https://download.pytorch.org/whl/cu121\n",
        "\n",
        "# B. Install torch-scatter/sparse\n",
        "!pip install torch-scatter torch-sparse -f https://data.pyg.org/whl/torch-2.4.0+cu121.html\n",
        "\n",
        "# C. Fix requirements.txt\n",
        "!sed -i 's/==.*//g' requirements.txt\n",
        "\n",
        "# D. Install requirements\n",
        "!pip install -r requirements.txt\n",
        "\n",
        "# E. SKIP 'pip install -e .' (Because setup.py is missing)\n",
        "print(\"\\u2705 Dependencies Installed. (Skipped setup.py)\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PPA2HJBBq0ep",
        "outputId": "61ed66b6-ca83-4f65-f62c-45850eaed6be"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Installing Dependencies...\n",
            "/content/PointNeXt\n",
            "Looking in indexes: https://download.pytorch.org/whl/cu121\n",
            "Requirement already satisfied: torch==2.4.0 in /usr/local/lib/python3.12/dist-packages (2.4.0+cu121)\n",
            "Requirement already satisfied: torchvision==0.19.0 in /usr/local/lib/python3.12/dist-packages (0.19.0+cu121)\n",
            "Requirement already satisfied: torchaudio==2.4.0 in /usr/local/lib/python3.12/dist-packages (2.4.0+cu121)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from torch==2.4.0) (3.20.0)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.12/dist-packages (from torch==2.4.0) (4.15.0)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.12/dist-packages (from torch==2.4.0) (1.14.0)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.12/dist-packages (from torch==2.4.0) (3.6.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from torch==2.4.0) (3.1.6)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.12/dist-packages (from torch==2.4.0) (2025.3.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (from torch==2.4.0) (75.2.0)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.1.105 in /usr/local/lib/python3.12/dist-packages (from torch==2.4.0) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.1.105 in /usr/local/lib/python3.12/dist-packages (from torch==2.4.0) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.1.105 in /usr/local/lib/python3.12/dist-packages (from torch==2.4.0) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /usr/local/lib/python3.12/dist-packages (from torch==2.4.0) (9.1.0.70)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.1.3.1 in /usr/local/lib/python3.12/dist-packages (from torch==2.4.0) (12.1.3.1)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.0.2.54 in /usr/local/lib/python3.12/dist-packages (from torch==2.4.0) (11.0.2.54)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.2.106 in /usr/local/lib/python3.12/dist-packages (from torch==2.4.0) (10.3.2.106)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.4.5.107 in /usr/local/lib/python3.12/dist-packages (from torch==2.4.0) (11.4.5.107)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.1.0.106 in /usr/local/lib/python3.12/dist-packages (from torch==2.4.0) (12.1.0.106)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.20.5 in /usr/local/lib/python3.12/dist-packages (from torch==2.4.0) (2.20.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.1.105 in /usr/local/lib/python3.12/dist-packages (from torch==2.4.0) (12.1.105)\n",
            "Requirement already satisfied: triton==3.0.0 in /usr/local/lib/python3.12/dist-packages (from torch==2.4.0) (3.0.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.12/dist-packages (from torchvision==0.19.0) (2.0.2)\n",
            "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.12/dist-packages (from torchvision==0.19.0) (11.3.0)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12 in /usr/local/lib/python3.12/dist-packages (from nvidia-cusolver-cu12==11.4.5.107->torch==2.4.0) (12.6.85)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2->torch==2.4.0) (3.0.3)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from sympy->torch==2.4.0) (1.3.0)\n",
            "Looking in links: https://data.pyg.org/whl/torch-2.4.0+cu121.html\n",
            "Requirement already satisfied: torch-scatter in /usr/local/lib/python3.12/dist-packages (2.1.2+pt24cu121)\n",
            "Requirement already satisfied: torch-sparse in /usr/local/lib/python3.12/dist-packages (0.6.18+pt24cu121)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.12/dist-packages (from torch-sparse) (1.16.3)\n",
            "Requirement already satisfied: numpy<2.6,>=1.25.2 in /usr/local/lib/python3.12/dist-packages (from scipy->torch-sparse) (2.0.2)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.12/dist-packages (from -r requirements.txt (line 1)) (1.6.1)\n",
            "Requirement already satisfied: pickleshare in /usr/local/lib/python3.12/dist-packages (from -r requirements.txt (line 2)) (0.7.5)\n",
            "Collecting ninja (from -r requirements.txt (line 3))\n",
            "  Using cached ninja-1.13.0-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (5.1 kB)\n",
            "Requirement already satisfied: gdown in /usr/local/lib/python3.12/dist-packages (from -r requirements.txt (line 4)) (5.2.0)\n",
            "Requirement already satisfied: easydict in /usr/local/lib/python3.12/dist-packages (from -r requirements.txt (line 5)) (1.13)\n",
            "Requirement already satisfied: PyYAML in /usr/local/lib/python3.12/dist-packages (from -r requirements.txt (line 6)) (6.0.3)\n",
            "Requirement already satisfied: protobuf in /usr/local/lib/python3.12/dist-packages (from -r requirements.txt (line 7)) (5.29.5)\n",
            "Requirement already satisfied: tensorboard in /usr/local/lib/python3.12/dist-packages (from -r requirements.txt (line 8)) (2.19.0)\n",
            "Requirement already satisfied: termcolor in /usr/local/lib/python3.12/dist-packages (from -r requirements.txt (line 9)) (3.2.0)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.12/dist-packages (from -r requirements.txt (line 10)) (4.67.1)\n",
            "Collecting multimethod (from -r requirements.txt (line 11))\n",
            "  Using cached multimethod-2.0.2-py3-none-any.whl.metadata (8.4 kB)\n",
            "Requirement already satisfied: h5py in /usr/local/lib/python3.12/dist-packages (from -r requirements.txt (line 12)) (3.15.1)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.12/dist-packages (from -r requirements.txt (line 13)) (3.10.0)\n",
            "Requirement already satisfied: wandb in /usr/local/lib/python3.12/dist-packages (from -r requirements.txt (line 14)) (0.23.1)\n",
            "Collecting pyvista (from -r requirements.txt (line 15))\n",
            "  Using cached pyvista-0.46.4-py3-none-any.whl.metadata (15 kB)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (from -r requirements.txt (line 16)) (75.2.0)\n",
            "Requirement already satisfied: Cython in /usr/local/lib/python3.12/dist-packages (from -r requirements.txt (line 17)) (3.0.12)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.12/dist-packages (from -r requirements.txt (line 18)) (2.2.2)\n",
            "Collecting deepspeed (from -r requirements.txt (line 19))\n",
            "  Using cached deepspeed-0.18.3.tar.gz (1.6 MB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting shortuuid (from -r requirements.txt (line 20))\n",
            "  Using cached shortuuid-1.0.13-py3-none-any.whl.metadata (5.8 kB)\n",
            "Collecting mkdocs-material (from -r requirements.txt (line 23))\n",
            "  Using cached mkdocs_material-9.7.0-py3-none-any.whl.metadata (19 kB)\n",
            "Collecting mkdocs-awesome-pages-plugin (from -r requirements.txt (line 24))\n",
            "  Using cached mkdocs_awesome_pages_plugin-2.10.1-py3-none-any.whl.metadata (13 kB)\n",
            "Collecting mdx_truly_sane_lists (from -r requirements.txt (line 25))\n",
            "  Using cached mdx_truly_sane_lists-1.3-py3-none-any.whl.metadata (3.8 kB)\n",
            "Requirement already satisfied: numpy>=1.19.5 in /usr/local/lib/python3.12/dist-packages (from scikit-learn->-r requirements.txt (line 1)) (2.0.2)\n",
            "Requirement already satisfied: scipy>=1.6.0 in /usr/local/lib/python3.12/dist-packages (from scikit-learn->-r requirements.txt (line 1)) (1.16.3)\n",
            "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.12/dist-packages (from scikit-learn->-r requirements.txt (line 1)) (1.5.2)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.12/dist-packages (from scikit-learn->-r requirements.txt (line 1)) (3.6.0)\n",
            "Requirement already satisfied: beautifulsoup4 in /usr/local/lib/python3.12/dist-packages (from gdown->-r requirements.txt (line 4)) (4.13.5)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from gdown->-r requirements.txt (line 4)) (3.20.0)\n",
            "Requirement already satisfied: requests[socks] in /usr/local/lib/python3.12/dist-packages (from gdown->-r requirements.txt (line 4)) (2.32.4)\n",
            "Requirement already satisfied: absl-py>=0.4 in /usr/local/lib/python3.12/dist-packages (from tensorboard->-r requirements.txt (line 8)) (1.4.0)\n",
            "Requirement already satisfied: grpcio>=1.48.2 in /usr/local/lib/python3.12/dist-packages (from tensorboard->-r requirements.txt (line 8)) (1.76.0)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.12/dist-packages (from tensorboard->-r requirements.txt (line 8)) (3.10)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.12/dist-packages (from tensorboard->-r requirements.txt (line 8)) (25.0)\n",
            "Requirement already satisfied: six>1.9 in /usr/local/lib/python3.12/dist-packages (from tensorboard->-r requirements.txt (line 8)) (1.17.0)\n",
            "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /usr/local/lib/python3.12/dist-packages (from tensorboard->-r requirements.txt (line 8)) (0.7.2)\n",
            "Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.12/dist-packages (from tensorboard->-r requirements.txt (line 8)) (3.1.4)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib->-r requirements.txt (line 13)) (1.3.3)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.12/dist-packages (from matplotlib->-r requirements.txt (line 13)) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.12/dist-packages (from matplotlib->-r requirements.txt (line 13)) (4.61.0)\n",
            "Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib->-r requirements.txt (line 13)) (1.4.9)\n",
            "Requirement already satisfied: pillow>=8 in /usr/local/lib/python3.12/dist-packages (from matplotlib->-r requirements.txt (line 13)) (11.3.0)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib->-r requirements.txt (line 13)) (3.2.5)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.12/dist-packages (from matplotlib->-r requirements.txt (line 13)) (2.9.0.post0)\n",
            "Requirement already satisfied: click>=8.0.1 in /usr/local/lib/python3.12/dist-packages (from wandb->-r requirements.txt (line 14)) (8.3.1)\n",
            "Requirement already satisfied: gitpython!=3.1.29,>=1.0.0 in /usr/local/lib/python3.12/dist-packages (from wandb->-r requirements.txt (line 14)) (3.1.45)\n",
            "Requirement already satisfied: platformdirs in /usr/local/lib/python3.12/dist-packages (from wandb->-r requirements.txt (line 14)) (4.5.1)\n",
            "Requirement already satisfied: pydantic<3 in /usr/local/lib/python3.12/dist-packages (from wandb->-r requirements.txt (line 14)) (2.12.3)\n",
            "Requirement already satisfied: sentry-sdk>=2.0.0 in /usr/local/lib/python3.12/dist-packages (from wandb->-r requirements.txt (line 14)) (2.47.0)\n",
            "Requirement already satisfied: typing-extensions<5,>=4.8 in /usr/local/lib/python3.12/dist-packages (from wandb->-r requirements.txt (line 14)) (4.15.0)\n",
            "Requirement already satisfied: pooch in /usr/local/lib/python3.12/dist-packages (from pyvista->-r requirements.txt (line 15)) (1.8.2)\n",
            "Requirement already satisfied: scooby>=0.5.1 in /usr/local/lib/python3.12/dist-packages (from pyvista->-r requirements.txt (line 15)) (0.11.0)\n",
            "Collecting vtk!=9.4.0 (from pyvista->-r requirements.txt (line 15))\n",
            "  Using cached vtk-9.5.2-cp312-cp312-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (5.6 kB)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.12/dist-packages (from pandas->-r requirements.txt (line 18)) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.12/dist-packages (from pandas->-r requirements.txt (line 18)) (2025.2)\n",
            "Requirement already satisfied: einops in /usr/local/lib/python3.12/dist-packages (from deepspeed->-r requirements.txt (line 19)) (0.8.1)\n",
            "Collecting hjson (from deepspeed->-r requirements.txt (line 19))\n",
            "  Using cached hjson-3.1.0-py3-none-any.whl.metadata (2.6 kB)\n",
            "Requirement already satisfied: msgpack in /usr/local/lib/python3.12/dist-packages (from deepspeed->-r requirements.txt (line 19)) (1.1.2)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.12/dist-packages (from deepspeed->-r requirements.txt (line 19)) (5.9.5)\n",
            "Requirement already satisfied: py-cpuinfo in /usr/local/lib/python3.12/dist-packages (from deepspeed->-r requirements.txt (line 19)) (9.0.0)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.12/dist-packages (from deepspeed->-r requirements.txt (line 19)) (2.4.0+cu121)\n",
            "Requirement already satisfied: nvidia-ml-py in /usr/local/lib/python3.12/dist-packages (from deepspeed->-r requirements.txt (line 19)) (13.590.44)\n",
            "Requirement already satisfied: babel>=2.10 in /usr/local/lib/python3.12/dist-packages (from mkdocs-material->-r requirements.txt (line 23)) (2.17.0)\n",
            "Collecting backrefs>=5.7.post1 (from mkdocs-material->-r requirements.txt (line 23))\n",
            "  Using cached backrefs-6.1-py312-none-any.whl.metadata (3.0 kB)\n",
            "Collecting colorama>=0.4 (from mkdocs-material->-r requirements.txt (line 23))\n",
            "  Using cached colorama-0.4.6-py2.py3-none-any.whl.metadata (17 kB)\n",
            "Requirement already satisfied: jinja2>=3.1 in /usr/local/lib/python3.12/dist-packages (from mkdocs-material->-r requirements.txt (line 23)) (3.1.6)\n",
            "Collecting mkdocs-material-extensions>=1.3 (from mkdocs-material->-r requirements.txt (line 23))\n",
            "  Using cached mkdocs_material_extensions-1.3.1-py3-none-any.whl.metadata (6.9 kB)\n",
            "Collecting mkdocs>=1.6 (from mkdocs-material->-r requirements.txt (line 23))\n",
            "  Using cached mkdocs-1.6.1-py3-none-any.whl.metadata (6.0 kB)\n",
            "Collecting paginate>=0.5 (from mkdocs-material->-r requirements.txt (line 23))\n",
            "  Using cached paginate-0.5.7-py2.py3-none-any.whl.metadata (11 kB)\n",
            "Requirement already satisfied: pygments>=2.16 in /usr/local/lib/python3.12/dist-packages (from mkdocs-material->-r requirements.txt (line 23)) (2.19.2)\n",
            "Collecting pymdown-extensions>=10.2 (from mkdocs-material->-r requirements.txt (line 23))\n",
            "  Using cached pymdown_extensions-10.19-py3-none-any.whl.metadata (3.1 kB)\n",
            "Requirement already satisfied: natsort>=8.1.0 in /usr/local/lib/python3.12/dist-packages (from mkdocs-awesome-pages-plugin->-r requirements.txt (line 24)) (8.4.0)\n",
            "Collecting wcmatch>=7 (from mkdocs-awesome-pages-plugin->-r requirements.txt (line 24))\n",
            "  Using cached wcmatch-10.1-py3-none-any.whl.metadata (5.1 kB)\n",
            "Requirement already satisfied: gitdb<5,>=4.0.1 in /usr/local/lib/python3.12/dist-packages (from gitpython!=3.1.29,>=1.0.0->wandb->-r requirements.txt (line 14)) (4.0.12)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2>=3.1->mkdocs-material->-r requirements.txt (line 23)) (3.0.3)\n",
            "Collecting ghp-import>=1.0 (from mkdocs>=1.6->mkdocs-material->-r requirements.txt (line 23))\n",
            "  Using cached ghp_import-2.1.0-py3-none-any.whl.metadata (7.2 kB)\n",
            "Collecting mergedeep>=1.3.4 (from mkdocs>=1.6->mkdocs-material->-r requirements.txt (line 23))\n",
            "  Using cached mergedeep-1.3.4-py3-none-any.whl.metadata (4.3 kB)\n",
            "Collecting mkdocs-get-deps>=0.2.0 (from mkdocs>=1.6->mkdocs-material->-r requirements.txt (line 23))\n",
            "  Using cached mkdocs_get_deps-0.2.0-py3-none-any.whl.metadata (4.0 kB)\n",
            "Collecting pathspec>=0.11.1 (from mkdocs>=1.6->mkdocs-material->-r requirements.txt (line 23))\n",
            "  Using cached pathspec-0.12.1-py3-none-any.whl.metadata (21 kB)\n",
            "Collecting pyyaml-env-tag>=0.1 (from mkdocs>=1.6->mkdocs-material->-r requirements.txt (line 23))\n",
            "  Using cached pyyaml_env_tag-1.1-py3-none-any.whl.metadata (5.5 kB)\n",
            "Requirement already satisfied: watchdog>=2.0 in /usr/local/lib/python3.12/dist-packages (from mkdocs>=1.6->mkdocs-material->-r requirements.txt (line 23)) (6.0.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.12/dist-packages (from pydantic<3->wandb->-r requirements.txt (line 14)) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.41.4 in /usr/local/lib/python3.12/dist-packages (from pydantic<3->wandb->-r requirements.txt (line 14)) (2.41.4)\n",
            "Requirement already satisfied: typing-inspection>=0.4.2 in /usr/local/lib/python3.12/dist-packages (from pydantic<3->wandb->-r requirements.txt (line 14)) (0.4.2)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests[socks]->gdown->-r requirements.txt (line 4)) (3.4.4)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests[socks]->gdown->-r requirements.txt (line 4)) (3.11)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests[socks]->gdown->-r requirements.txt (line 4)) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests[socks]->gdown->-r requirements.txt (line 4)) (2025.11.12)\n",
            "Collecting bracex>=2.1.1 (from wcmatch>=7->mkdocs-awesome-pages-plugin->-r requirements.txt (line 24))\n",
            "  Using cached bracex-2.6-py3-none-any.whl.metadata (3.6 kB)\n",
            "Requirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.12/dist-packages (from beautifulsoup4->gdown->-r requirements.txt (line 4)) (2.8)\n",
            "Requirement already satisfied: PySocks!=1.5.7,>=1.5.6 in /usr/local/lib/python3.12/dist-packages (from requests[socks]->gdown->-r requirements.txt (line 4)) (1.7.1)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.12/dist-packages (from torch->deepspeed->-r requirements.txt (line 19)) (1.14.0)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.12/dist-packages (from torch->deepspeed->-r requirements.txt (line 19)) (3.6.1)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.12/dist-packages (from torch->deepspeed->-r requirements.txt (line 19)) (2025.3.0)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.1.105 in /usr/local/lib/python3.12/dist-packages (from torch->deepspeed->-r requirements.txt (line 19)) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.1.105 in /usr/local/lib/python3.12/dist-packages (from torch->deepspeed->-r requirements.txt (line 19)) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.1.105 in /usr/local/lib/python3.12/dist-packages (from torch->deepspeed->-r requirements.txt (line 19)) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /usr/local/lib/python3.12/dist-packages (from torch->deepspeed->-r requirements.txt (line 19)) (9.1.0.70)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.1.3.1 in /usr/local/lib/python3.12/dist-packages (from torch->deepspeed->-r requirements.txt (line 19)) (12.1.3.1)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.0.2.54 in /usr/local/lib/python3.12/dist-packages (from torch->deepspeed->-r requirements.txt (line 19)) (11.0.2.54)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.2.106 in /usr/local/lib/python3.12/dist-packages (from torch->deepspeed->-r requirements.txt (line 19)) (10.3.2.106)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.4.5.107 in /usr/local/lib/python3.12/dist-packages (from torch->deepspeed->-r requirements.txt (line 19)) (11.4.5.107)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.1.0.106 in /usr/local/lib/python3.12/dist-packages (from torch->deepspeed->-r requirements.txt (line 19)) (12.1.0.106)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.20.5 in /usr/local/lib/python3.12/dist-packages (from torch->deepspeed->-r requirements.txt (line 19)) (2.20.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.1.105 in /usr/local/lib/python3.12/dist-packages (from torch->deepspeed->-r requirements.txt (line 19)) (12.1.105)\n",
            "Requirement already satisfied: triton==3.0.0 in /usr/local/lib/python3.12/dist-packages (from torch->deepspeed->-r requirements.txt (line 19)) (3.0.0)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12 in /usr/local/lib/python3.12/dist-packages (from nvidia-cusolver-cu12==11.4.5.107->torch->deepspeed->-r requirements.txt (line 19)) (12.6.85)\n",
            "Requirement already satisfied: smmap<6,>=3.0.1 in /usr/local/lib/python3.12/dist-packages (from gitdb<5,>=4.0.1->gitpython!=3.1.29,>=1.0.0->wandb->-r requirements.txt (line 14)) (5.0.2)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from sympy->torch->deepspeed->-r requirements.txt (line 19)) (1.3.0)\n",
            "Using cached ninja-1.13.0-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (180 kB)\n",
            "Using cached multimethod-2.0.2-py3-none-any.whl (9.6 kB)\n",
            "Using cached pyvista-0.46.4-py3-none-any.whl (2.4 MB)\n",
            "Using cached shortuuid-1.0.13-py3-none-any.whl (10 kB)\n",
            "Using cached mkdocs_material-9.7.0-py3-none-any.whl (9.3 MB)\n",
            "Using cached mkdocs_awesome_pages_plugin-2.10.1-py3-none-any.whl (15 kB)\n",
            "Using cached mdx_truly_sane_lists-1.3-py3-none-any.whl (6.1 kB)\n",
            "Using cached backrefs-6.1-py312-none-any.whl (398 kB)\n",
            "Using cached colorama-0.4.6-py2.py3-none-any.whl (25 kB)\n",
            "Using cached mkdocs-1.6.1-py3-none-any.whl (3.9 MB)\n",
            "Using cached mkdocs_material_extensions-1.3.1-py3-none-any.whl (8.7 kB)\n",
            "Using cached paginate-0.5.7-py2.py3-none-any.whl (13 kB)\n",
            "Using cached pymdown_extensions-10.19-py3-none-any.whl (266 kB)\n",
            "Using cached vtk-9.5.2-cp312-cp312-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (112.3 MB)\n",
            "Using cached wcmatch-10.1-py3-none-any.whl (39 kB)\n",
            "Using cached hjson-3.1.0-py3-none-any.whl (54 kB)\n",
            "Using cached bracex-2.6-py3-none-any.whl (11 kB)\n",
            "Using cached ghp_import-2.1.0-py3-none-any.whl (11 kB)\n",
            "Using cached mergedeep-1.3.4-py3-none-any.whl (6.4 kB)\n",
            "Using cached mkdocs_get_deps-0.2.0-py3-none-any.whl (9.5 kB)\n",
            "Using cached pathspec-0.12.1-py3-none-any.whl (31 kB)\n",
            "Using cached pyyaml_env_tag-1.1-py3-none-any.whl (4.7 kB)\n",
            "Building wheels for collected packages: deepspeed\n",
            "  Building wheel for deepspeed (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for deepspeed: filename=deepspeed-0.18.3-py3-none-any.whl size=1770196 sha256=5d1b7b50373142e5e0e970eb9145db3c461840402a2800ad14c1b02c819bc418\n",
            "  Stored in directory: /root/.cache/pip/wheels/c9/9a/37/beb534d37a37cd057d48ba20b82f34d527816b7fdf0206882f\n",
            "Successfully built deepspeed\n",
            "Installing collected packages: paginate, hjson, shortuuid, pyyaml-env-tag, pymdown-extensions, pathspec, ninja, multimethod, mkdocs-material-extensions, mergedeep, mdx_truly_sane_lists, colorama, bracex, backrefs, wcmatch, mkdocs-get-deps, ghp-import, vtk, mkdocs, pyvista, mkdocs-material, mkdocs-awesome-pages-plugin, deepspeed\n",
            "Successfully installed backrefs-6.1 bracex-2.6 colorama-0.4.6 deepspeed-0.18.3 ghp-import-2.1.0 hjson-3.1.0 mdx_truly_sane_lists-1.3 mergedeep-1.3.4 mkdocs-1.6.1 mkdocs-awesome-pages-plugin-2.10.1 mkdocs-get-deps-0.2.0 mkdocs-material-9.7.0 mkdocs-material-extensions-1.3.1 multimethod-2.0.2 ninja-1.13.0 paginate-0.5.7 pathspec-0.12.1 pymdown-extensions-10.19 pyvista-0.46.4 pyyaml-env-tag-1.1 shortuuid-1.0.13 vtk-9.5.2 wcmatch-10.1\n",
            "‚úÖ Dependencies Installed. (Skipped setup.py)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 1. Search for the missing setup.py\n",
        "print(\"Searching for C++ kernel setup.py...\")\n",
        "target_dir = \"/content/PointNeXt/openpoints/cpp\"\n",
        "found_setup = False\n",
        "\n",
        "for root, dirs, files in os.walk(target_dir):\n",
        "    if \"setup.py\" in files:\n",
        "        print(f\"\\u2705 Found setup.py at: {root}\")\n",
        "        found_setup = True\n",
        "\n",
        "        # 2. Force Compile\n",
        "        print(f\"Compiling kernels in {root}...\")\n",
        "        try:\n",
        "            subprocess.check_call([sys.executable, \"setup.py\", \"install\"], cwd=root)\n",
        "            print(\"Compilation Successful!\")\n",
        "        except subprocess.CalledProcessError as e:\n",
        "            print(f\"\\u274c Compilation Failed: {e}\")\n",
        "\n",
        "if not found_setup:\n",
        "    print(\"\\u274c Critical Error: Could not find setup.py anywhere in openpoints/cpp!\")\n",
        "    print(\"Did the 'git submodule update' step finish successfully?\")"
      ],
      "metadata": {
        "id": "0FjSoHiNPWvH",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a8cd5889-60bb-4b99-a887-e79d300e5b51"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Searching for C++ kernel setup.py...\n",
            "‚úÖ Found setup.py at: /content/PointNeXt/openpoints/cpp/pointnet2_batch\n",
            "Compiling kernels in /content/PointNeXt/openpoints/cpp/pointnet2_batch...\n",
            "Compilation Successful!\n",
            "‚úÖ Found setup.py at: /content/PointNeXt/openpoints/cpp/emd\n",
            "Compiling kernels in /content/PointNeXt/openpoints/cpp/emd...\n",
            "Compilation Successful!\n",
            "‚úÖ Found setup.py at: /content/PointNeXt/openpoints/cpp/pointops\n",
            "Compiling kernels in /content/PointNeXt/openpoints/cpp/pointops...\n",
            "Compilation Successful!\n",
            "‚úÖ Found setup.py at: /content/PointNeXt/openpoints/cpp/subsampling\n",
            "Compiling kernels in /content/PointNeXt/openpoints/cpp/subsampling...\n",
            "‚ùå Compilation Failed: Command '['/usr/bin/python3', 'setup.py', 'install']' returned non-zero exit status 1.\n",
            "‚úÖ Found setup.py at: /content/PointNeXt/openpoints/cpp/chamfer_dist\n",
            "Compiling kernels in /content/PointNeXt/openpoints/cpp/chamfer_dist...\n",
            "Compilation Successful!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Fix Missing Validation List\n",
        "\n",
        "split_dir = \"/content/processed_data/train_test_split\"\n",
        "test_file = os.path.join(split_dir, \"shuffled_test_file_list.json\")\n",
        "val_file = os.path.join(split_dir, \"shuffled_val_file_list.json\")\n",
        "\n",
        "print(\"Checking validation list...\")\n",
        "\n",
        "if os.path.exists(test_file):\n",
        "    if not os.path.exists(val_file):\n",
        "        print(\"Creating dummy validation list (copy of test list)...\")\n",
        "        shutil.copy(test_file, val_file)\n",
        "        print(f\"\\u2705 Created: {val_file}\")\n",
        "    else:\n",
        "        print(\"\\u2705 Validation list already exists.\")\n",
        "else:\n",
        "    print(\"\\u274c Error: Test list not found! Did the previous copy step work?\")"
      ],
      "metadata": {
        "id": "5CMDNVe9o1HH",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "66e78c76-8223-4a4e-f040-697b2f51d9da"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Checking validation list...\n",
            "‚úÖ Validation list already exists.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Train On shapenetpart"
      ],
      "metadata": {
        "id": "veAsjknFPqV0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Patch main.py to fix Tensor Key Error\n",
        "import os\n",
        "\n",
        "target_file = \"/content/PointNeXt/examples/shapenetpart/main.py\"\n",
        "print(f\"Patching {target_file} to fix Tensor Key Error...\")\n",
        "\n",
        "with open(target_file, 'r') as f:\n",
        "    code = f.read()\n",
        "\n",
        "# The problematic line: parts = cls2parts[cls[shape_idx]]\n",
        "# We change it to: parts = cls2parts[int(cls[shape_idx])]\n",
        "\n",
        "bad_line = \"parts = cls2parts[cls[shape_idx]]\"\n",
        "good_line = \"parts = cls2parts[int(cls[shape_idx])]\"\n",
        "\n",
        "if bad_line in code:\n",
        "    code = code.replace(bad_line, good_line)\n",
        "    print(\"\\u2705 Patched: Cast tensor to int for dictionary lookup.\")\n",
        "else:\n",
        "    print(\"\\u26A0\\uFE0F Warning: Could not find exact line. Check if file changed.\")\n",
        "\n",
        "with open(target_file, 'w') as f:\n",
        "    f.write(code)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sNkJ6uQWPwRs",
        "outputId": "05ba85d0-7638-48c8-b804-bfa313371e77"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Patching /content/PointNeXt/examples/shapenetpart/main.py to fix Tensor Key Error...\n",
            "‚úÖ Patched: Cast tensor to int for dictionary lookup.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# NOW Run Training\n",
        "print(\"\\nStarting Training...\")\n",
        "%cd /content/PointNeXt\n",
        "!PYTHONPATH=. python examples/shapenetpart/main.py --cfg cfgs/shapenetpart/custom_fallen_trees.yaml mode=train"
      ],
      "metadata": {
        "id": "yTC1v-MNyjtd",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "54fb67dc-ea39-44e6-f291-65cde3cb4872"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Starting Training...\n",
            "/content/PointNeXt\n",
            "2025-12-12 09:35:21.284839: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
            "E0000 00:00:1765532121.565238    6108 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "E0000 00:00:1765532121.643725    6108 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "W0000 00:00:1765532122.241789    6108 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1765532122.241826    6108 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1765532122.241832    6108 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1765532122.241837    6108 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "2025-12-12 09:35:22.298643: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
            "To enable the following instructions: AVX2 AVX512F FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "/content/PointNeXt/openpoints/models/backbone/pointnetv2.py:79: SyntaxWarning: invalid escape sequence '\\s'\n",
            "  new_features: (B, \\sum_k(mlps[k][-1], npoint)) tensor of the new_features descriptors\n",
            "/content/PointNeXt/openpoints/models/classification/cls_base.py:99: SyntaxWarning: invalid escape sequence '\\e'\n",
            "  $\\eg$ cls_feat='max,avg' means use the concatenateion of maxpooled and avgpooled features.\n",
            "launch mp with 1 GPUs, current rank: 0\n",
            "\u001b[32m[12/12 09:35:36 FallenTreePart]: \u001b[0mdist_url: tcp://localhost:8888\n",
            "dist_backend: nccl\n",
            "multiprocessing_distributed: False\n",
            "ngpus_per_node: 1\n",
            "world_size: 1\n",
            "launcher: mp\n",
            "local_rank: 0\n",
            "use_gpu: True\n",
            "seed: 1066\n",
            "epoch: 0\n",
            "epochs: 100\n",
            "ignore_index: None\n",
            "val_fn: validate\n",
            "deterministic: False\n",
            "sync_bn: False\n",
            "criterion_args:\n",
            "  NAME: Poly1FocalLoss\n",
            "use_mask: False\n",
            "grad_norm_clip: 1\n",
            "layer_decay: 0\n",
            "step_per_update: 1\n",
            "start_epoch: 1\n",
            "sched_on_epoch: True\n",
            "wandb:\n",
            "  use_wandb: False\n",
            "  project: PointNext-ShapeNetPart\n",
            "  tags: ['shapenetpart', 'train', 'custom_fallen_trees', 'ngpus1', 'seed1066']\n",
            "  name: shapenetpart-train-custom_fallen_trees-ngpus1-seed1066-20251212-093536-S85TXDdERtSiQuih8nf377\n",
            "use_amp: False\n",
            "use_voting: False\n",
            "val_freq: 1\n",
            "resume: False\n",
            "test: False\n",
            "finetune: False\n",
            "mode: train\n",
            "logname: None\n",
            "load_path: None\n",
            "print_freq: 10\n",
            "save_freq: -1\n",
            "root_dir: log/shapenetpart\n",
            "pretrained_path: None\n",
            "datatransforms:\n",
            "  train: ['PointsToTensor', 'PointCloudScaling', 'PointCloudCenterAndNormalize', 'PointCloudJitter', 'ChromaticDropGPU']\n",
            "  val: ['PointsToTensor', 'PointCloudCenterAndNormalize']\n",
            "  vote: ['PointCloudScaling']\n",
            "  kwargs:\n",
            "    jitter_sigma: 0.001\n",
            "    jitter_clip: 0.005\n",
            "    scale: [0.8, 1.2]\n",
            "    gravity_dim: 1\n",
            "    angle: [0, 1.0, 0]\n",
            "feature_keys: pos,x\n",
            "dataset:\n",
            "  common:\n",
            "    NAME: FallenTreePart\n",
            "    data_root: /content/processed_data\n",
            "    use_normal: False\n",
            "    num_points: 2048\n",
            "    use_xyz: True\n",
            "  train:\n",
            "    split: train\n",
            "  val:\n",
            "    split: val\n",
            "    presample: True\n",
            "num_classes: 4\n",
            "shape_classes: 2\n",
            "num_points: 2048\n",
            "normal_channel: True\n",
            "batch_size: 16\n",
            "dataloader:\n",
            "  num_workers: 4\n",
            "num_votes: 10\n",
            "refine: True\n",
            "lr: 0.001\n",
            "min_lr: None\n",
            "optimizer:\n",
            "  NAME: adamw\n",
            "  weight_decay: 0.0001\n",
            "sched: multistep\n",
            "decay_epochs: [70, 90]\n",
            "decay_rate: 0.1\n",
            "warmup_epochs: 0\n",
            "model:\n",
            "  NAME: BasePartSeg\n",
            "  encoder_args:\n",
            "    NAME: PointNextEncoder\n",
            "    blocks: [1, 1, 1, 1, 1]\n",
            "    strides: [1, 2, 2, 2, 2]\n",
            "    width: 32\n",
            "    in_channels: 7\n",
            "    sa_layers: 3\n",
            "    sa_use_res: True\n",
            "    radius: 0.1\n",
            "    radius_scaling: 2.5\n",
            "    nsample: 32\n",
            "    expansion: 4\n",
            "    aggr_args:\n",
            "      feature_type: dp_fj\n",
            "    reduction: max\n",
            "    group_args:\n",
            "      NAME: ballquery\n",
            "      normalize_dp: True\n",
            "    conv_args:\n",
            "      order: conv-norm-act\n",
            "    act_args:\n",
            "      act: relu\n",
            "    norm_args:\n",
            "      norm: bn\n",
            "  decoder_args:\n",
            "    NAME: PointNextPartDecoder\n",
            "    cls_map: curvenet\n",
            "  cls_args:\n",
            "    NAME: SegHead\n",
            "    global_feat: max,avg\n",
            "    num_classes: 4\n",
            "    shape_classes: 2\n",
            "    in_channels: None\n",
            "    norm_args:\n",
            "      norm: bn\n",
            "log_dir: log/shapenetpart/shapenetpart-train-custom_fallen_trees-ngpus1-seed1066-20251212-093536-S85TXDdERtSiQuih8nf377\n",
            "rank: 0\n",
            "distributed: False\n",
            "mp: False\n",
            "task_name: shapenetpart\n",
            "cfg_basename: custom_fallen_trees\n",
            "opts: mode=train\n",
            "is_training: True\n",
            "run_name: shapenetpart-train-custom_fallen_trees-ngpus1-seed1066-20251212-093536-S85TXDdERtSiQuih8nf377\n",
            "run_dir: log/shapenetpart/shapenetpart-train-custom_fallen_trees-ngpus1-seed1066-20251212-093536-S85TXDdERtSiQuih8nf377\n",
            "exp_dir: log/shapenetpart/shapenetpart-train-custom_fallen_trees-ngpus1-seed1066-20251212-093536-S85TXDdERtSiQuih8nf377\n",
            "ckpt_dir: log/shapenetpart/shapenetpart-train-custom_fallen_trees-ngpus1-seed1066-20251212-093536-S85TXDdERtSiQuih8nf377/checkpoint\n",
            "log_path: log/shapenetpart/shapenetpart-train-custom_fallen_trees-ngpus1-seed1066-20251212-093536-S85TXDdERtSiQuih8nf377/shapenetpart-train-custom_fallen_trees-ngpus1-seed1066-20251212-093536-S85TXDdERtSiQuih8nf377.log\n",
            "cfg_path: log/shapenetpart/shapenetpart-train-custom_fallen_trees-ngpus1-seed1066-20251212-093536-S85TXDdERtSiQuih8nf377/cfg.yaml\n",
            "\u001b[32m[12/12 09:35:36 FallenTreePart]: \u001b[0mLoading val split from: /content/processed_data/train_test_split/shuffled_val_file_list.json\n",
            "\u001b[32m[12/12 09:35:36 FallenTreePart]: \u001b[0mFound 128 valid files for val split.\n",
            "\u001b[32m[12/12 09:35:36 FallenTreePart]: \u001b[0mlength of validation dataset: 128\n",
            "\u001b[32m[12/12 09:35:36 FallenTreePart]: \u001b[0mnumber of classes of the dataset: 4\n",
            "\u001b[32m[12/12 09:35:36 FallenTreePart]: \u001b[0mradius: [[0.1], [0.1], [0.25], [0.625], [1.5625]],\n",
            " nsample: [[32], [32], [32], [32], [32]]\n",
            "\u001b[32m[12/12 09:35:36 FallenTreePart]: \u001b[0mNAME: ballquery\n",
            "normalize_dp: True\n",
            "radius: 0.1\n",
            "nsample: 32\n",
            "\u001b[32m[12/12 09:35:36 FallenTreePart]: \u001b[0mNAME: ballquery\n",
            "normalize_dp: True\n",
            "radius: 0.25\n",
            "nsample: 32\n",
            "\u001b[32m[12/12 09:35:36 FallenTreePart]: \u001b[0mNAME: ballquery\n",
            "normalize_dp: True\n",
            "radius: 0.625\n",
            "nsample: 32\n",
            "\u001b[32m[12/12 09:35:36 FallenTreePart]: \u001b[0mNAME: ballquery\n",
            "normalize_dp: True\n",
            "radius: 1.5625\n",
            "nsample: 32\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m[12/12 09:35:36 FallenTreePart]: \u001b[0mkwargs: {'shape_classes': 2} are not used in SegHead\n",
            "\u001b[32m[12/12 09:35:36 FallenTreePart]: \u001b[0mBasePartSeg(\n",
            "  (encoder): PointNextEncoder(\n",
            "    (encoder): Sequential(\n",
            "      (0): Sequential(\n",
            "        (0): SetAbstraction(\n",
            "          (convs): Sequential(\n",
            "            (0): Sequential(\n",
            "              (0): Conv1d(7, 32, kernel_size=(1,), stride=(1,))\n",
            "            )\n",
            "          )\n",
            "        )\n",
            "      )\n",
            "      (1): Sequential(\n",
            "        (0): SetAbstraction(\n",
            "          (skipconv): Sequential(\n",
            "            (0): Conv1d(32, 64, kernel_size=(1,), stride=(1,))\n",
            "          )\n",
            "          (act): ReLU(inplace=True)\n",
            "          (convs): Sequential(\n",
            "            (0): Sequential(\n",
            "              (0): Conv2d(35, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "              (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "              (2): ReLU(inplace=True)\n",
            "            )\n",
            "            (1): Sequential(\n",
            "              (0): Conv2d(32, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "              (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "              (2): ReLU(inplace=True)\n",
            "            )\n",
            "            (2): Sequential(\n",
            "              (0): Conv2d(32, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "              (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "            )\n",
            "          )\n",
            "          (grouper): QueryAndGroup()\n",
            "        )\n",
            "      )\n",
            "      (2): Sequential(\n",
            "        (0): SetAbstraction(\n",
            "          (skipconv): Sequential(\n",
            "            (0): Conv1d(64, 128, kernel_size=(1,), stride=(1,))\n",
            "          )\n",
            "          (act): ReLU(inplace=True)\n",
            "          (convs): Sequential(\n",
            "            (0): Sequential(\n",
            "              (0): Conv2d(67, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "              (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "              (2): ReLU(inplace=True)\n",
            "            )\n",
            "            (1): Sequential(\n",
            "              (0): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "              (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "              (2): ReLU(inplace=True)\n",
            "            )\n",
            "            (2): Sequential(\n",
            "              (0): Conv2d(64, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "              (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "            )\n",
            "          )\n",
            "          (grouper): QueryAndGroup()\n",
            "        )\n",
            "      )\n",
            "      (3): Sequential(\n",
            "        (0): SetAbstraction(\n",
            "          (skipconv): Sequential(\n",
            "            (0): Conv1d(128, 256, kernel_size=(1,), stride=(1,))\n",
            "          )\n",
            "          (act): ReLU(inplace=True)\n",
            "          (convs): Sequential(\n",
            "            (0): Sequential(\n",
            "              (0): Conv2d(131, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "              (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "              (2): ReLU(inplace=True)\n",
            "            )\n",
            "            (1): Sequential(\n",
            "              (0): Conv2d(128, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "              (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "              (2): ReLU(inplace=True)\n",
            "            )\n",
            "            (2): Sequential(\n",
            "              (0): Conv2d(128, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "              (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "            )\n",
            "          )\n",
            "          (grouper): QueryAndGroup()\n",
            "        )\n",
            "      )\n",
            "      (4): Sequential(\n",
            "        (0): SetAbstraction(\n",
            "          (skipconv): Sequential(\n",
            "            (0): Conv1d(256, 512, kernel_size=(1,), stride=(1,))\n",
            "          )\n",
            "          (act): ReLU(inplace=True)\n",
            "          (convs): Sequential(\n",
            "            (0): Sequential(\n",
            "              (0): Conv2d(259, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "              (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "              (2): ReLU(inplace=True)\n",
            "            )\n",
            "            (1): Sequential(\n",
            "              (0): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "              (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "              (2): ReLU(inplace=True)\n",
            "            )\n",
            "            (2): Sequential(\n",
            "              (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "              (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "            )\n",
            "          )\n",
            "          (grouper): QueryAndGroup()\n",
            "        )\n",
            "      )\n",
            "    )\n",
            "  )\n",
            "  (decoder): PointNextPartDecoder(\n",
            "    (global_conv2): Sequential(\n",
            "      (0): Sequential(\n",
            "        (0): Conv1d(512, 128, kernel_size=(1,), stride=(1,))\n",
            "        (1): ReLU(inplace=True)\n",
            "      )\n",
            "    )\n",
            "    (global_conv1): Sequential(\n",
            "      (0): Sequential(\n",
            "        (0): Conv1d(256, 64, kernel_size=(1,), stride=(1,))\n",
            "        (1): ReLU(inplace=True)\n",
            "      )\n",
            "    )\n",
            "    (decoder): Sequential(\n",
            "      (0): Sequential(\n",
            "        (0): FeaturePropogation(\n",
            "          (convs): Sequential(\n",
            "            (0): Sequential(\n",
            "              (0): Conv1d(304, 32, kernel_size=(1,), stride=(1,), bias=False)\n",
            "              (1): BatchNorm1d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "              (2): ReLU(inplace=True)\n",
            "            )\n",
            "            (1): Sequential(\n",
            "              (0): Conv1d(32, 32, kernel_size=(1,), stride=(1,), bias=False)\n",
            "              (1): BatchNorm1d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "              (2): ReLU(inplace=True)\n",
            "            )\n",
            "          )\n",
            "        )\n",
            "      )\n",
            "      (1): Sequential(\n",
            "        (0): FeaturePropogation(\n",
            "          (convs): Sequential(\n",
            "            (0): Sequential(\n",
            "              (0): Conv1d(192, 64, kernel_size=(1,), stride=(1,), bias=False)\n",
            "              (1): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "              (2): ReLU(inplace=True)\n",
            "            )\n",
            "            (1): Sequential(\n",
            "              (0): Conv1d(64, 64, kernel_size=(1,), stride=(1,), bias=False)\n",
            "              (1): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "              (2): ReLU(inplace=True)\n",
            "            )\n",
            "          )\n",
            "        )\n",
            "      )\n",
            "      (2): Sequential(\n",
            "        (0): FeaturePropogation(\n",
            "          (convs): Sequential(\n",
            "            (0): Sequential(\n",
            "              (0): Conv1d(384, 128, kernel_size=(1,), stride=(1,), bias=False)\n",
            "              (1): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "              (2): ReLU(inplace=True)\n",
            "            )\n",
            "            (1): Sequential(\n",
            "              (0): Conv1d(128, 128, kernel_size=(1,), stride=(1,), bias=False)\n",
            "              (1): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "              (2): ReLU(inplace=True)\n",
            "            )\n",
            "          )\n",
            "        )\n",
            "      )\n",
            "      (3): Sequential(\n",
            "        (0): FeaturePropogation(\n",
            "          (convs): Sequential(\n",
            "            (0): Sequential(\n",
            "              (0): Conv1d(768, 256, kernel_size=(1,), stride=(1,), bias=False)\n",
            "              (1): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "              (2): ReLU(inplace=True)\n",
            "            )\n",
            "            (1): Sequential(\n",
            "              (0): Conv1d(256, 256, kernel_size=(1,), stride=(1,), bias=False)\n",
            "              (1): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "              (2): ReLU(inplace=True)\n",
            "            )\n",
            "          )\n",
            "        )\n",
            "      )\n",
            "    )\n",
            "  )\n",
            "  (head): SegHead(\n",
            "    (head): Sequential(\n",
            "      (0): Sequential(\n",
            "        (0): Conv1d(96, 96, kernel_size=(1,), stride=(1,), bias=False)\n",
            "        (1): BatchNorm1d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (2): ReLU(inplace=True)\n",
            "      )\n",
            "      (1): Dropout(p=0.5, inplace=False)\n",
            "      (2): Sequential(\n",
            "        (0): Conv1d(96, 4, kernel_size=(1,), stride=(1,))\n",
            "      )\n",
            "    )\n",
            "  )\n",
            ")\n",
            "\u001b[32m[12/12 09:35:36 FallenTreePart]: \u001b[0mNumber of params: 0.9774 M\n",
            "\u001b[32m[12/12 09:35:36 FallenTreePart]: \u001b[0mParam groups = {\n",
            "  \"decay\": {\n",
            "    \"weight_decay\": 0.0001,\n",
            "    \"params\": [\n",
            "      \"encoder.encoder.0.0.convs.0.0.weight\",\n",
            "      \"encoder.encoder.1.0.skipconv.0.weight\",\n",
            "      \"encoder.encoder.1.0.convs.0.0.weight\",\n",
            "      \"encoder.encoder.1.0.convs.1.0.weight\",\n",
            "      \"encoder.encoder.1.0.convs.2.0.weight\",\n",
            "      \"encoder.encoder.2.0.skipconv.0.weight\",\n",
            "      \"encoder.encoder.2.0.convs.0.0.weight\",\n",
            "      \"encoder.encoder.2.0.convs.1.0.weight\",\n",
            "      \"encoder.encoder.2.0.convs.2.0.weight\",\n",
            "      \"encoder.encoder.3.0.skipconv.0.weight\",\n",
            "      \"encoder.encoder.3.0.convs.0.0.weight\",\n",
            "      \"encoder.encoder.3.0.convs.1.0.weight\",\n",
            "      \"encoder.encoder.3.0.convs.2.0.weight\",\n",
            "      \"encoder.encoder.4.0.skipconv.0.weight\",\n",
            "      \"encoder.encoder.4.0.convs.0.0.weight\",\n",
            "      \"encoder.encoder.4.0.convs.1.0.weight\",\n",
            "      \"encoder.encoder.4.0.convs.2.0.weight\",\n",
            "      \"decoder.global_conv2.0.0.weight\",\n",
            "      \"decoder.global_conv1.0.0.weight\",\n",
            "      \"decoder.decoder.0.0.convs.0.0.weight\",\n",
            "      \"decoder.decoder.0.0.convs.1.0.weight\",\n",
            "      \"decoder.decoder.1.0.convs.0.0.weight\",\n",
            "      \"decoder.decoder.1.0.convs.1.0.weight\",\n",
            "      \"decoder.decoder.2.0.convs.0.0.weight\",\n",
            "      \"decoder.decoder.2.0.convs.1.0.weight\",\n",
            "      \"decoder.decoder.3.0.convs.0.0.weight\",\n",
            "      \"decoder.decoder.3.0.convs.1.0.weight\",\n",
            "      \"head.head.0.0.weight\",\n",
            "      \"head.head.2.0.weight\"\n",
            "    ],\n",
            "    \"lr_scale\": 1.0\n",
            "  },\n",
            "  \"no_decay\": {\n",
            "    \"weight_decay\": 0.0,\n",
            "    \"params\": [\n",
            "      \"encoder.encoder.0.0.convs.0.0.bias\",\n",
            "      \"encoder.encoder.1.0.skipconv.0.bias\",\n",
            "      \"encoder.encoder.1.0.convs.0.1.weight\",\n",
            "      \"encoder.encoder.1.0.convs.0.1.bias\",\n",
            "      \"encoder.encoder.1.0.convs.1.1.weight\",\n",
            "      \"encoder.encoder.1.0.convs.1.1.bias\",\n",
            "      \"encoder.encoder.1.0.convs.2.1.weight\",\n",
            "      \"encoder.encoder.1.0.convs.2.1.bias\",\n",
            "      \"encoder.encoder.2.0.skipconv.0.bias\",\n",
            "      \"encoder.encoder.2.0.convs.0.1.weight\",\n",
            "      \"encoder.encoder.2.0.convs.0.1.bias\",\n",
            "      \"encoder.encoder.2.0.convs.1.1.weight\",\n",
            "      \"encoder.encoder.2.0.convs.1.1.bias\",\n",
            "      \"encoder.encoder.2.0.convs.2.1.weight\",\n",
            "      \"encoder.encoder.2.0.convs.2.1.bias\",\n",
            "      \"encoder.encoder.3.0.skipconv.0.bias\",\n",
            "      \"encoder.encoder.3.0.convs.0.1.weight\",\n",
            "      \"encoder.encoder.3.0.convs.0.1.bias\",\n",
            "      \"encoder.encoder.3.0.convs.1.1.weight\",\n",
            "      \"encoder.encoder.3.0.convs.1.1.bias\",\n",
            "      \"encoder.encoder.3.0.convs.2.1.weight\",\n",
            "      \"encoder.encoder.3.0.convs.2.1.bias\",\n",
            "      \"encoder.encoder.4.0.skipconv.0.bias\",\n",
            "      \"encoder.encoder.4.0.convs.0.1.weight\",\n",
            "      \"encoder.encoder.4.0.convs.0.1.bias\",\n",
            "      \"encoder.encoder.4.0.convs.1.1.weight\",\n",
            "      \"encoder.encoder.4.0.convs.1.1.bias\",\n",
            "      \"encoder.encoder.4.0.convs.2.1.weight\",\n",
            "      \"encoder.encoder.4.0.convs.2.1.bias\",\n",
            "      \"decoder.global_conv2.0.0.bias\",\n",
            "      \"decoder.global_conv1.0.0.bias\",\n",
            "      \"decoder.decoder.0.0.convs.0.1.weight\",\n",
            "      \"decoder.decoder.0.0.convs.0.1.bias\",\n",
            "      \"decoder.decoder.0.0.convs.1.1.weight\",\n",
            "      \"decoder.decoder.0.0.convs.1.1.bias\",\n",
            "      \"decoder.decoder.1.0.convs.0.1.weight\",\n",
            "      \"decoder.decoder.1.0.convs.0.1.bias\",\n",
            "      \"decoder.decoder.1.0.convs.1.1.weight\",\n",
            "      \"decoder.decoder.1.0.convs.1.1.bias\",\n",
            "      \"decoder.decoder.2.0.convs.0.1.weight\",\n",
            "      \"decoder.decoder.2.0.convs.0.1.bias\",\n",
            "      \"decoder.decoder.2.0.convs.1.1.weight\",\n",
            "      \"decoder.decoder.2.0.convs.1.1.bias\",\n",
            "      \"decoder.decoder.3.0.convs.0.1.weight\",\n",
            "      \"decoder.decoder.3.0.convs.0.1.bias\",\n",
            "      \"decoder.decoder.3.0.convs.1.1.weight\",\n",
            "      \"decoder.decoder.3.0.convs.1.1.bias\",\n",
            "      \"head.head.0.1.weight\",\n",
            "      \"head.head.0.1.bias\",\n",
            "      \"head.head.2.0.bias\"\n",
            "    ],\n",
            "    \"lr_scale\": 1.0\n",
            "  }\n",
            "}\n",
            "\u001b[32m[12/12 09:35:36 FallenTreePart]: \u001b[0mTraining from scratch\n",
            "\u001b[32m[12/12 09:35:36 FallenTreePart]: \u001b[0mLoading train split from: /content/processed_data/train_test_split/shuffled_train_file_list.json\n",
            "\u001b[32m[12/12 09:35:36 FallenTreePart]: \u001b[0mFound 721 valid files for train split.\n",
            "\u001b[32m[12/12 09:35:36 FallenTreePart]: \u001b[0mlength of training dataset: 721\n",
            "Train Epoch [1/100] Loss 0.131 : 100% 45/45 [00:16<00:00,  2.80it/s]\n",
            "100% 8/8 [00:00<00:00,  8.32it/s]\n",
            "\u001b[32m[12/12 09:35:53 FallenTreePart]: \u001b[0mTest Epoch [1/100],Instance mIoU 86.06, Class mIoU 84.62, \n",
            " Class mIoUs tensor([100.0000,  69.2372], device='cuda:0')\n",
            "\u001b[32m[12/12 09:35:53 FallenTreePart]: \u001b[0mFind a better ckpt @E1, val_ins_miou 86.06 val_cls_miou 84.62, \n",
            "cls_mious: tensor([100.0000,  69.2372], device='cuda:0')\n",
            "\u001b[32m[12/12 09:35:53 FallenTreePart]: \u001b[0mFound the best model and saved in log/shapenetpart/shapenetpart-train-custom_fallen_trees-ngpus1-seed1066-20251212-093536-S85TXDdERtSiQuih8nf377/checkpoint/shapenetpart-train-custom_fallen_trees-ngpus1-seed1066-20251212-093536-S85TXDdERtSiQuih8nf377_ckpt_best.pth\n",
            "Train Epoch [2/100] Loss 0.040 : 100% 45/45 [00:07<00:00,  6.01it/s]\n",
            "100% 8/8 [00:00<00:00,  9.90it/s]\n",
            "\u001b[32m[12/12 09:36:02 FallenTreePart]: \u001b[0mTest Epoch [2/100],Instance mIoU 85.47, Class mIoU 83.96, \n",
            " Class mIoUs tensor([100.0000,  67.9285], device='cuda:0')\n",
            "Train Epoch [3/100] Loss 0.028 : 100% 45/45 [00:07<00:00,  5.92it/s]\n",
            "100% 8/8 [00:00<00:00,  9.15it/s]\n",
            "\u001b[32m[12/12 09:36:11 FallenTreePart]: \u001b[0mTest Epoch [3/100],Instance mIoU 89.61, Class mIoU 88.54, \n",
            " Class mIoUs tensor([100.0000,  77.0798], device='cuda:0')\n",
            "\u001b[32m[12/12 09:36:11 FallenTreePart]: \u001b[0mFind a better ckpt @E3, val_ins_miou 89.61 val_cls_miou 88.54, \n",
            "cls_mious: tensor([100.0000,  77.0798], device='cuda:0')\n",
            "\u001b[32m[12/12 09:36:11 FallenTreePart]: \u001b[0mFound the best model and saved in log/shapenetpart/shapenetpart-train-custom_fallen_trees-ngpus1-seed1066-20251212-093536-S85TXDdERtSiQuih8nf377/checkpoint/shapenetpart-train-custom_fallen_trees-ngpus1-seed1066-20251212-093536-S85TXDdERtSiQuih8nf377_ckpt_best.pth\n",
            "Train Epoch [4/100] Loss 0.020 : 100% 45/45 [00:07<00:00,  6.00it/s]\n",
            "100% 8/8 [00:00<00:00, 10.89it/s]\n",
            "\u001b[32m[12/12 09:36:20 FallenTreePart]: \u001b[0mTest Epoch [4/100],Instance mIoU 87.33, Class mIoU 86.31, \n",
            " Class mIoUs tensor([97.1429, 75.4839], device='cuda:0')\n",
            "Train Epoch [5/100] Loss 0.017 : 100% 45/45 [00:07<00:00,  5.99it/s]\n",
            "100% 8/8 [00:00<00:00,  9.71it/s]\n",
            "\u001b[32m[12/12 09:36:28 FallenTreePart]: \u001b[0mTest Epoch [5/100],Instance mIoU 89.89, Class mIoU 88.99, \n",
            " Class mIoUs tensor([98.5714, 79.4025], device='cuda:0')\n",
            "\u001b[32m[12/12 09:36:28 FallenTreePart]: \u001b[0mFind a better ckpt @E5, val_ins_miou 89.89 val_cls_miou 88.99, \n",
            "cls_mious: tensor([98.5714, 79.4025], device='cuda:0')\n",
            "\u001b[32m[12/12 09:36:28 FallenTreePart]: \u001b[0mFound the best model and saved in log/shapenetpart/shapenetpart-train-custom_fallen_trees-ngpus1-seed1066-20251212-093536-S85TXDdERtSiQuih8nf377/checkpoint/shapenetpart-train-custom_fallen_trees-ngpus1-seed1066-20251212-093536-S85TXDdERtSiQuih8nf377_ckpt_best.pth\n",
            "Train Epoch [6/100] Loss 0.019 : 100% 45/45 [00:07<00:00,  5.95it/s]\n",
            "100% 8/8 [00:00<00:00,  8.12it/s]\n",
            "\u001b[32m[12/12 09:36:37 FallenTreePart]: \u001b[0mTest Epoch [6/100],Instance mIoU 88.54, Class mIoU 87.36, \n",
            " Class mIoUs tensor([100.0000,  74.7144], device='cuda:0')\n",
            "Train Epoch [7/100] Loss 0.016 : 100% 45/45 [00:07<00:00,  5.95it/s]\n",
            "100% 8/8 [00:00<00:00, 11.15it/s]\n",
            "\u001b[32m[12/12 09:36:46 FallenTreePart]: \u001b[0mTest Epoch [7/100],Instance mIoU 87.92, Class mIoU 86.67, \n",
            " Class mIoUs tensor([100.0000,  73.3494], device='cuda:0')\n",
            "Train Epoch [8/100] Loss 0.016 : 100% 45/45 [00:07<00:00,  5.91it/s]\n",
            "100% 8/8 [00:00<00:00, 10.42it/s]\n",
            "\u001b[32m[12/12 09:36:54 FallenTreePart]: \u001b[0mTest Epoch [8/100],Instance mIoU 87.69, Class mIoU 86.43, \n",
            " Class mIoUs tensor([99.8263, 73.0353], device='cuda:0')\n",
            "Train Epoch [9/100] Loss 0.017 : 100% 45/45 [00:07<00:00,  5.80it/s]\n",
            "100% 8/8 [00:00<00:00,  8.36it/s]\n",
            "\u001b[32m[12/12 09:37:04 FallenTreePart]: \u001b[0mTest Epoch [9/100],Instance mIoU 88.45, Class mIoU 87.25, \n",
            " Class mIoUs tensor([100.0000,  74.5100], device='cuda:0')\n",
            "Train Epoch [10/100] Loss 0.014 : 100% 45/45 [00:07<00:00,  5.74it/s]\n",
            "100% 8/8 [00:00<00:00, 10.24it/s]\n",
            "\u001b[32m[12/12 09:37:13 FallenTreePart]: \u001b[0mTest Epoch [10/100],Instance mIoU 92.36, Class mIoU 91.57, \n",
            " Class mIoUs tensor([100.0000,  83.1346], device='cuda:0')\n",
            "\u001b[32m[12/12 09:37:13 FallenTreePart]: \u001b[0mFind a better ckpt @E10, val_ins_miou 92.36 val_cls_miou 91.57, \n",
            "cls_mious: tensor([100.0000,  83.1346], device='cuda:0')\n",
            "\u001b[32m[12/12 09:37:13 FallenTreePart]: \u001b[0mFound the best model and saved in log/shapenetpart/shapenetpart-train-custom_fallen_trees-ngpus1-seed1066-20251212-093536-S85TXDdERtSiQuih8nf377/checkpoint/shapenetpart-train-custom_fallen_trees-ngpus1-seed1066-20251212-093536-S85TXDdERtSiQuih8nf377_ckpt_best.pth\n",
            "Train Epoch [11/100] Loss 0.013 : 100% 45/45 [00:07<00:00,  5.74it/s]\n",
            "100% 8/8 [00:00<00:00, 10.07it/s]\n",
            "\u001b[32m[12/12 09:37:22 FallenTreePart]: \u001b[0mTest Epoch [11/100],Instance mIoU 85.54, Class mIoU 84.88, \n",
            " Class mIoUs tensor([91.9566, 77.8039], device='cuda:0')\n",
            "Train Epoch [12/100] Loss 0.014 : 100% 45/45 [00:07<00:00,  5.68it/s]\n",
            "100% 8/8 [00:00<00:00,  8.37it/s]\n",
            "\u001b[32m[12/12 09:37:31 FallenTreePart]: \u001b[0mTest Epoch [12/100],Instance mIoU 92.26, Class mIoU 91.46, \n",
            " Class mIoUs tensor([100.0000,  82.9268], device='cuda:0')\n",
            "Train Epoch [13/100] Loss 0.011 : 100% 45/45 [00:07<00:00,  5.69it/s]\n",
            "100% 8/8 [00:00<00:00, 10.06it/s]\n",
            "\u001b[32m[12/12 09:37:40 FallenTreePart]: \u001b[0mTest Epoch [13/100],Instance mIoU 93.55, Class mIoU 92.89, \n",
            " Class mIoUs tensor([100.0000,  85.7702], device='cuda:0')\n",
            "\u001b[32m[12/12 09:37:40 FallenTreePart]: \u001b[0mFind a better ckpt @E13, val_ins_miou 93.55 val_cls_miou 92.89, \n",
            "cls_mious: tensor([100.0000,  85.7702], device='cuda:0')\n",
            "\u001b[32m[12/12 09:37:40 FallenTreePart]: \u001b[0mFound the best model and saved in log/shapenetpart/shapenetpart-train-custom_fallen_trees-ngpus1-seed1066-20251212-093536-S85TXDdERtSiQuih8nf377/checkpoint/shapenetpart-train-custom_fallen_trees-ngpus1-seed1066-20251212-093536-S85TXDdERtSiQuih8nf377_ckpt_best.pth\n",
            "Train Epoch [14/100] Loss 0.009 : 100% 45/45 [00:08<00:00,  5.58it/s]\n",
            "100% 8/8 [00:00<00:00, 10.68it/s]\n",
            "\u001b[32m[12/12 09:37:49 FallenTreePart]: \u001b[0mTest Epoch [14/100],Instance mIoU 91.94, Class mIoU 91.10, \n",
            " Class mIoUs tensor([100.0000,  82.2091], device='cuda:0')\n",
            "Train Epoch [15/100] Loss 0.007 : 100% 45/45 [00:08<00:00,  5.57it/s]\n",
            "100% 8/8 [00:00<00:00,  8.29it/s]\n",
            "\u001b[32m[12/12 09:37:58 FallenTreePart]: \u001b[0mTest Epoch [15/100],Instance mIoU 93.58, Class mIoU 92.92, \n",
            " Class mIoUs tensor([100.0000,  85.8380], device='cuda:0')\n",
            "\u001b[32m[12/12 09:37:58 FallenTreePart]: \u001b[0mFind a better ckpt @E15, val_ins_miou 93.58 val_cls_miou 92.92, \n",
            "cls_mious: tensor([100.0000,  85.8380], device='cuda:0')\n",
            "\u001b[32m[12/12 09:37:58 FallenTreePart]: \u001b[0mFound the best model and saved in log/shapenetpart/shapenetpart-train-custom_fallen_trees-ngpus1-seed1066-20251212-093536-S85TXDdERtSiQuih8nf377/checkpoint/shapenetpart-train-custom_fallen_trees-ngpus1-seed1066-20251212-093536-S85TXDdERtSiQuih8nf377_ckpt_best.pth\n",
            "Train Epoch [16/100] Loss 0.006 : 100% 45/45 [00:07<00:00,  5.66it/s]\n",
            "100% 8/8 [00:00<00:00, 10.58it/s]\n",
            "\u001b[32m[12/12 09:38:07 FallenTreePart]: \u001b[0mTest Epoch [16/100],Instance mIoU 94.69, Class mIoU 94.15, \n",
            " Class mIoUs tensor([100.0000,  88.2921], device='cuda:0')\n",
            "\u001b[32m[12/12 09:38:07 FallenTreePart]: \u001b[0mFind a better ckpt @E16, val_ins_miou 94.69 val_cls_miou 94.15, \n",
            "cls_mious: tensor([100.0000,  88.2921], device='cuda:0')\n",
            "\u001b[32m[12/12 09:38:08 FallenTreePart]: \u001b[0mFound the best model and saved in log/shapenetpart/shapenetpart-train-custom_fallen_trees-ngpus1-seed1066-20251212-093536-S85TXDdERtSiQuih8nf377/checkpoint/shapenetpart-train-custom_fallen_trees-ngpus1-seed1066-20251212-093536-S85TXDdERtSiQuih8nf377_ckpt_best.pth\n",
            "Train Epoch [17/100] Loss 0.006 : 100% 45/45 [00:07<00:00,  5.68it/s]\n",
            "100% 8/8 [00:00<00:00, 10.53it/s]\n",
            "\u001b[32m[12/12 09:38:16 FallenTreePart]: \u001b[0mTest Epoch [17/100],Instance mIoU 94.35, Class mIoU 93.77, \n",
            " Class mIoUs tensor([100.0000,  87.5353], device='cuda:0')\n",
            "Train Epoch [18/100] Loss 0.005 : 100% 45/45 [00:07<00:00,  5.66it/s]\n",
            "100% 8/8 [00:00<00:00,  8.88it/s]\n",
            "\u001b[32m[12/12 09:38:26 FallenTreePart]: \u001b[0mTest Epoch [18/100],Instance mIoU 94.01, Class mIoU 93.39, \n",
            " Class mIoUs tensor([100.0000,  86.7775], device='cuda:0')\n",
            "Train Epoch [19/100] Loss 0.005 : 100% 45/45 [00:07<00:00,  5.73it/s]\n",
            "100% 8/8 [00:00<00:00, 10.54it/s]\n",
            "\u001b[32m[12/12 09:38:35 FallenTreePart]: \u001b[0mTest Epoch [19/100],Instance mIoU 92.87, Class mIoU 92.13, \n",
            " Class mIoUs tensor([100.0000,  84.2699], device='cuda:0')\n",
            "Train Epoch [20/100] Loss 0.005 : 100% 45/45 [00:07<00:00,  5.70it/s]\n",
            "100% 8/8 [00:00<00:00, 10.76it/s]\n",
            "\u001b[32m[12/12 09:38:44 FallenTreePart]: \u001b[0mTest Epoch [20/100],Instance mIoU 94.55, Class mIoU 93.99, \n",
            " Class mIoUs tensor([100.0000,  87.9735], device='cuda:0')\n",
            "Train Epoch [21/100] Loss 0.004 : 100% 45/45 [00:08<00:00,  5.61it/s]\n",
            "100% 8/8 [00:00<00:00,  9.13it/s]\n",
            "\u001b[32m[12/12 09:38:53 FallenTreePart]: \u001b[0mTest Epoch [21/100],Instance mIoU 94.01, Class mIoU 93.39, \n",
            " Class mIoUs tensor([100.0000,  86.7859], device='cuda:0')\n",
            "Train Epoch [22/100] Loss 0.005 : 100% 45/45 [00:07<00:00,  5.66it/s]\n",
            "100% 8/8 [00:00<00:00,  9.35it/s]\n",
            "\u001b[32m[12/12 09:39:02 FallenTreePart]: \u001b[0mTest Epoch [22/100],Instance mIoU 93.43, Class mIoU 92.75, \n",
            " Class mIoUs tensor([100.0000,  85.5091], device='cuda:0')\n",
            "Train Epoch [23/100] Loss 0.008 : 100% 45/45 [00:07<00:00,  5.65it/s]\n",
            "100% 8/8 [00:00<00:00, 10.76it/s]\n",
            "\u001b[32m[12/12 09:39:11 FallenTreePart]: \u001b[0mTest Epoch [23/100],Instance mIoU 84.72, Class mIoU 83.14, \n",
            " Class mIoUs tensor([100.0000,  66.2710], device='cuda:0')\n",
            "Train Epoch [24/100] Loss 0.010 : 100% 45/45 [00:07<00:00,  5.63it/s]\n",
            "100% 8/8 [00:00<00:00,  9.98it/s]\n",
            "\u001b[32m[12/12 09:39:20 FallenTreePart]: \u001b[0mTest Epoch [24/100],Instance mIoU 91.07, Class mIoU 90.14, \n",
            " Class mIoUs tensor([100.0000,  80.2818], device='cuda:0')\n",
            "Train Epoch [25/100] Loss 0.011 : 100% 45/45 [00:07<00:00,  5.69it/s]\n",
            "100% 8/8 [00:00<00:00, 10.64it/s]\n",
            "\u001b[32m[12/12 09:39:29 FallenTreePart]: \u001b[0mTest Epoch [25/100],Instance mIoU 86.77, Class mIoU 86.58, \n",
            " Class mIoUs tensor([88.5686, 84.5884], device='cuda:0')\n",
            "Train Epoch [26/100] Loss 0.008 : 100% 45/45 [00:07<00:00,  5.63it/s]\n",
            "100% 8/8 [00:00<00:00, 10.74it/s]\n",
            "\u001b[32m[12/12 09:39:38 FallenTreePart]: \u001b[0mTest Epoch [26/100],Instance mIoU 92.77, Class mIoU 92.02, \n",
            " Class mIoUs tensor([100.0000,  84.0462], device='cuda:0')\n",
            "Train Epoch [27/100] Loss 0.006 : 100% 45/45 [00:08<00:00,  5.62it/s]\n",
            "100% 8/8 [00:00<00:00,  9.77it/s]\n",
            "\u001b[32m[12/12 09:39:47 FallenTreePart]: \u001b[0mTest Epoch [27/100],Instance mIoU 94.75, Class mIoU 94.21, \n",
            " Class mIoUs tensor([100.0000,  88.4193], device='cuda:0')\n",
            "\u001b[32m[12/12 09:39:47 FallenTreePart]: \u001b[0mFind a better ckpt @E27, val_ins_miou 94.75 val_cls_miou 94.21, \n",
            "cls_mious: tensor([100.0000,  88.4193], device='cuda:0')\n",
            "\u001b[32m[12/12 09:39:47 FallenTreePart]: \u001b[0mFound the best model and saved in log/shapenetpart/shapenetpart-train-custom_fallen_trees-ngpus1-seed1066-20251212-093536-S85TXDdERtSiQuih8nf377/checkpoint/shapenetpart-train-custom_fallen_trees-ngpus1-seed1066-20251212-093536-S85TXDdERtSiQuih8nf377_ckpt_best.pth\n",
            "Train Epoch [28/100] Loss 0.004 : 100% 45/45 [00:07<00:00,  5.66it/s]\n",
            "100% 8/8 [00:00<00:00, 10.47it/s]\n",
            "\u001b[32m[12/12 09:39:56 FallenTreePart]: \u001b[0mTest Epoch [28/100],Instance mIoU 94.13, Class mIoU 93.52, \n",
            " Class mIoUs tensor([100.0000,  87.0387], device='cuda:0')\n",
            "Train Epoch [29/100] Loss 0.004 : 100% 45/45 [00:07<00:00,  5.69it/s]\n",
            "100% 8/8 [00:00<00:00, 10.29it/s]\n",
            "\u001b[32m[12/12 09:40:05 FallenTreePart]: \u001b[0mTest Epoch [29/100],Instance mIoU 93.52, Class mIoU 92.85, \n",
            " Class mIoUs tensor([100.0000,  85.7022], device='cuda:0')\n",
            "Train Epoch [30/100] Loss 0.003 : 100% 45/45 [00:08<00:00,  5.62it/s]\n",
            "100% 8/8 [00:00<00:00, 10.49it/s]\n",
            "\u001b[32m[12/12 09:40:14 FallenTreePart]: \u001b[0mTest Epoch [30/100],Instance mIoU 92.50, Class mIoU 91.73, \n",
            " Class mIoUs tensor([100.0000,  83.4507], device='cuda:0')\n",
            "Train Epoch [31/100] Loss 0.003 : 100% 45/45 [00:07<00:00,  5.65it/s]\n",
            "100% 8/8 [00:00<00:00, 10.79it/s]\n",
            "\u001b[32m[12/12 09:40:23 FallenTreePart]: \u001b[0mTest Epoch [31/100],Instance mIoU 94.23, Class mIoU 93.63, \n",
            " Class mIoUs tensor([100.0000,  87.2594], device='cuda:0')\n",
            "Train Epoch [32/100] Loss 0.003 : 100% 45/45 [00:07<00:00,  5.66it/s]\n",
            "100% 8/8 [00:00<00:00, 10.35it/s]\n",
            "\u001b[32m[12/12 09:40:32 FallenTreePart]: \u001b[0mTest Epoch [32/100],Instance mIoU 94.74, Class mIoU 94.20, \n",
            " Class mIoUs tensor([100.0000,  88.3965], device='cuda:0')\n",
            "Train Epoch [33/100] Loss 0.003 : 100% 45/45 [00:07<00:00,  5.63it/s]\n",
            "100% 8/8 [00:00<00:00,  9.76it/s]\n",
            "\u001b[32m[12/12 09:40:42 FallenTreePart]: \u001b[0mTest Epoch [33/100],Instance mIoU 94.71, Class mIoU 94.16, \n",
            " Class mIoUs tensor([100.0000,  88.3195], device='cuda:0')\n",
            "Train Epoch [34/100] Loss 0.003 : 100% 45/45 [00:07<00:00,  5.68it/s]\n",
            "100% 8/8 [00:00<00:00, 10.74it/s]\n",
            "\u001b[32m[12/12 09:40:50 FallenTreePart]: \u001b[0mTest Epoch [34/100],Instance mIoU 94.98, Class mIoU 94.46, \n",
            " Class mIoUs tensor([100.0000,  88.9172], device='cuda:0')\n",
            "\u001b[32m[12/12 09:40:50 FallenTreePart]: \u001b[0mFind a better ckpt @E34, val_ins_miou 94.98 val_cls_miou 94.46, \n",
            "cls_mious: tensor([100.0000,  88.9172], device='cuda:0')\n",
            "\u001b[32m[12/12 09:40:51 FallenTreePart]: \u001b[0mFound the best model and saved in log/shapenetpart/shapenetpart-train-custom_fallen_trees-ngpus1-seed1066-20251212-093536-S85TXDdERtSiQuih8nf377/checkpoint/shapenetpart-train-custom_fallen_trees-ngpus1-seed1066-20251212-093536-S85TXDdERtSiQuih8nf377_ckpt_best.pth\n",
            "Train Epoch [35/100] Loss 0.003 : 100% 45/45 [00:07<00:00,  5.67it/s]\n",
            "100% 8/8 [00:00<00:00, 10.06it/s]\n",
            "\u001b[32m[12/12 09:40:59 FallenTreePart]: \u001b[0mTest Epoch [35/100],Instance mIoU 95.83, Class mIoU 95.40, \n",
            " Class mIoUs tensor([100.0000,  90.7946], device='cuda:0')\n",
            "\u001b[32m[12/12 09:40:59 FallenTreePart]: \u001b[0mFind a better ckpt @E35, val_ins_miou 95.83 val_cls_miou 95.40, \n",
            "cls_mious: tensor([100.0000,  90.7946], device='cuda:0')\n",
            "\u001b[32m[12/12 09:41:00 FallenTreePart]: \u001b[0mFound the best model and saved in log/shapenetpart/shapenetpart-train-custom_fallen_trees-ngpus1-seed1066-20251212-093536-S85TXDdERtSiQuih8nf377/checkpoint/shapenetpart-train-custom_fallen_trees-ngpus1-seed1066-20251212-093536-S85TXDdERtSiQuih8nf377_ckpt_best.pth\n",
            "Train Epoch [36/100] Loss 0.003 : 100% 45/45 [00:08<00:00,  5.62it/s]\n",
            "100% 8/8 [00:00<00:00, 10.52it/s]\n",
            "\u001b[32m[12/12 09:41:09 FallenTreePart]: \u001b[0mTest Epoch [36/100],Instance mIoU 95.13, Class mIoU 94.63, \n",
            " Class mIoUs tensor([100.0000,  89.2549], device='cuda:0')\n",
            "Train Epoch [37/100] Loss 0.003 : 100% 45/45 [00:07<00:00,  5.66it/s]\n",
            "100% 8/8 [00:00<00:00, 10.55it/s]\n",
            "\u001b[32m[12/12 09:41:18 FallenTreePart]: \u001b[0mTest Epoch [37/100],Instance mIoU 93.92, Class mIoU 93.29, \n",
            " Class mIoUs tensor([100.0000,  86.5789], device='cuda:0')\n",
            "Train Epoch [38/100] Loss 0.003 : 100% 45/45 [00:08<00:00,  5.59it/s]\n",
            "100% 8/8 [00:00<00:00, 10.33it/s]\n",
            "\u001b[32m[12/12 09:41:27 FallenTreePart]: \u001b[0mTest Epoch [38/100],Instance mIoU 95.41, Class mIoU 94.93, \n",
            " Class mIoUs tensor([100.0000,  89.8675], device='cuda:0')\n",
            "Train Epoch [39/100] Loss 0.003 : 100% 45/45 [00:07<00:00,  5.64it/s]\n",
            "100% 8/8 [00:00<00:00, 10.31it/s]\n",
            "\u001b[32m[12/12 09:41:36 FallenTreePart]: \u001b[0mTest Epoch [39/100],Instance mIoU 95.87, Class mIoU 95.44, \n",
            " Class mIoUs tensor([100.0000,  90.8782], device='cuda:0')\n",
            "\u001b[32m[12/12 09:41:36 FallenTreePart]: \u001b[0mFind a better ckpt @E39, val_ins_miou 95.87 val_cls_miou 95.44, \n",
            "cls_mious: tensor([100.0000,  90.8782], device='cuda:0')\n",
            "\u001b[32m[12/12 09:41:36 FallenTreePart]: \u001b[0mFound the best model and saved in log/shapenetpart/shapenetpart-train-custom_fallen_trees-ngpus1-seed1066-20251212-093536-S85TXDdERtSiQuih8nf377/checkpoint/shapenetpart-train-custom_fallen_trees-ngpus1-seed1066-20251212-093536-S85TXDdERtSiQuih8nf377_ckpt_best.pth\n",
            "Train Epoch [40/100] Loss 0.003 : 100% 45/45 [00:07<00:00,  5.69it/s]\n",
            "100% 8/8 [00:00<00:00, 10.58it/s]\n",
            "\u001b[32m[12/12 09:41:45 FallenTreePart]: \u001b[0mTest Epoch [40/100],Instance mIoU 94.71, Class mIoU 94.17, \n",
            " Class mIoUs tensor([100.0000,  88.3351], device='cuda:0')\n",
            "Train Epoch [41/100] Loss 0.002 : 100% 45/45 [00:08<00:00,  5.57it/s]\n",
            "100% 8/8 [00:00<00:00, 10.37it/s]\n",
            "\u001b[32m[12/12 09:41:54 FallenTreePart]: \u001b[0mTest Epoch [41/100],Instance mIoU 95.84, Class mIoU 95.41, \n",
            " Class mIoUs tensor([100.0000,  90.8227], device='cuda:0')\n",
            "Train Epoch [42/100] Loss 0.002 : 100% 45/45 [00:07<00:00,  5.68it/s]\n",
            "100% 8/8 [00:00<00:00, 10.21it/s]\n",
            "\u001b[32m[12/12 09:42:03 FallenTreePart]: \u001b[0mTest Epoch [42/100],Instance mIoU 95.44, Class mIoU 94.97, \n",
            " Class mIoUs tensor([100.0000,  89.9406], device='cuda:0')\n",
            "Train Epoch [43/100] Loss 0.002 : 100% 45/45 [00:07<00:00,  5.66it/s]\n",
            "100% 8/8 [00:00<00:00,  9.18it/s]\n",
            "\u001b[32m[12/12 09:42:12 FallenTreePart]: \u001b[0mTest Epoch [43/100],Instance mIoU 95.10, Class mIoU 94.59, \n",
            " Class mIoUs tensor([100.0000,  89.1860], device='cuda:0')\n",
            "Train Epoch [44/100] Loss 0.002 : 100% 45/45 [00:08<00:00,  5.60it/s]\n",
            "100% 8/8 [00:00<00:00, 10.56it/s]\n",
            "\u001b[32m[12/12 09:42:21 FallenTreePart]: \u001b[0mTest Epoch [44/100],Instance mIoU 95.44, Class mIoU 94.97, \n",
            " Class mIoUs tensor([100.0000,  89.9332], device='cuda:0')\n",
            "Train Epoch [45/100] Loss 0.002 : 100% 45/45 [00:07<00:00,  5.69it/s]\n",
            "100% 8/8 [00:00<00:00, 10.33it/s]\n",
            "\u001b[32m[12/12 09:42:30 FallenTreePart]: \u001b[0mTest Epoch [45/100],Instance mIoU 95.75, Class mIoU 95.31, \n",
            " Class mIoUs tensor([100.0000,  90.6165], device='cuda:0')\n",
            "Train Epoch [46/100] Loss 0.003 : 100% 45/45 [00:07<00:00,  5.64it/s]\n",
            "100% 8/8 [00:00<00:00,  8.27it/s]\n",
            "\u001b[32m[12/12 09:42:40 FallenTreePart]: \u001b[0mTest Epoch [46/100],Instance mIoU 93.63, Class mIoU 92.98, \n",
            " Class mIoUs tensor([100.0000,  85.9507], device='cuda:0')\n",
            "Train Epoch [47/100] Loss 0.003 : 100% 45/45 [00:08<00:00,  5.60it/s]\n",
            "100% 8/8 [00:00<00:00, 10.33it/s]\n",
            "\u001b[32m[12/12 09:42:49 FallenTreePart]: \u001b[0mTest Epoch [47/100],Instance mIoU 92.49, Class mIoU 91.71, \n",
            " Class mIoUs tensor([100.0000,  83.4245], device='cuda:0')\n",
            "Train Epoch [48/100] Loss 0.003 : 100% 45/45 [00:07<00:00,  5.64it/s]\n",
            "100% 8/8 [00:00<00:00, 10.05it/s]\n",
            "\u001b[32m[12/12 09:42:58 FallenTreePart]: \u001b[0mTest Epoch [48/100],Instance mIoU 93.41, Class mIoU 92.73, \n",
            " Class mIoUs tensor([100.0000,  85.4521], device='cuda:0')\n",
            "Train Epoch [49/100] Loss 0.003 : 100% 45/45 [00:08<00:00,  5.59it/s]\n",
            "100% 8/8 [00:01<00:00,  7.78it/s]\n",
            "\u001b[32m[12/12 09:43:07 FallenTreePart]: \u001b[0mTest Epoch [49/100],Instance mIoU 93.92, Class mIoU 93.29, \n",
            " Class mIoUs tensor([100.0000,  86.5844], device='cuda:0')\n",
            "Train Epoch [50/100] Loss 0.022 : 100% 45/45 [00:08<00:00,  5.61it/s]\n",
            "100% 8/8 [00:00<00:00, 10.06it/s]\n",
            "\u001b[32m[12/12 09:43:17 FallenTreePart]: \u001b[0mTest Epoch [50/100],Instance mIoU 83.16, Class mIoU 81.42, \n",
            " Class mIoUs tensor([100.0000,  62.8414], device='cuda:0')\n",
            "Train Epoch [51/100] Loss 0.019 : 100% 45/45 [00:07<00:00,  5.65it/s]\n",
            "100% 8/8 [00:00<00:00,  9.90it/s]\n",
            "\u001b[32m[12/12 09:43:26 FallenTreePart]: \u001b[0mTest Epoch [51/100],Instance mIoU 46.22, Class mIoU 48.66, \n",
            " Class mIoUs tensor([22.5802, 74.7490], device='cuda:0')\n",
            "Train Epoch [52/100] Loss 0.013 : 100% 45/45 [00:08<00:00,  5.62it/s]\n",
            "100% 8/8 [00:01<00:00,  7.91it/s]\n",
            "\u001b[32m[12/12 09:43:35 FallenTreePart]: \u001b[0mTest Epoch [52/100],Instance mIoU 89.56, Class mIoU 88.48, \n",
            " Class mIoUs tensor([100.0000,  76.9508], device='cuda:0')\n",
            "Train Epoch [53/100] Loss 0.008 : 100% 45/45 [00:07<00:00,  5.64it/s]\n",
            "100% 8/8 [00:00<00:00, 10.45it/s]\n",
            "\u001b[32m[12/12 09:43:44 FallenTreePart]: \u001b[0mTest Epoch [53/100],Instance mIoU 94.74, Class mIoU 94.19, \n",
            " Class mIoUs tensor([100.0000,  88.3814], device='cuda:0')\n",
            "Train Epoch [54/100] Loss 0.006 : 100% 45/45 [00:07<00:00,  5.69it/s]\n",
            "100% 8/8 [00:00<00:00, 10.25it/s]\n",
            "\u001b[32m[12/12 09:43:53 FallenTreePart]: \u001b[0mTest Epoch [54/100],Instance mIoU 93.95, Class mIoU 93.32, \n",
            " Class mIoUs tensor([100.0000,  86.6451], device='cuda:0')\n",
            "Train Epoch [55/100] Loss 0.005 : 100% 45/45 [00:07<00:00,  5.67it/s]\n",
            "100% 8/8 [00:00<00:00,  8.25it/s]\n",
            "\u001b[32m[12/12 09:44:03 FallenTreePart]: \u001b[0mTest Epoch [55/100],Instance mIoU 94.38, Class mIoU 93.80, \n",
            " Class mIoUs tensor([100.0000,  87.6019], device='cuda:0')\n",
            "Train Epoch [56/100] Loss 0.004 : 100% 45/45 [00:08<00:00,  5.58it/s]\n",
            "100% 8/8 [00:00<00:00, 10.37it/s]\n",
            "\u001b[32m[12/12 09:44:12 FallenTreePart]: \u001b[0mTest Epoch [56/100],Instance mIoU 94.62, Class mIoU 94.06, \n",
            " Class mIoUs tensor([100.0000,  88.1296], device='cuda:0')\n",
            "Train Epoch [57/100] Loss 0.003 : 100% 45/45 [00:07<00:00,  5.65it/s]\n",
            "100% 8/8 [00:00<00:00, 10.56it/s]\n",
            "\u001b[32m[12/12 09:44:21 FallenTreePart]: \u001b[0mTest Epoch [57/100],Instance mIoU 94.37, Class mIoU 93.79, \n",
            " Class mIoUs tensor([100.0000,  87.5717], device='cuda:0')\n",
            "Train Epoch [58/100] Loss 0.003 : 100% 45/45 [00:07<00:00,  5.65it/s]\n",
            "100% 8/8 [00:00<00:00,  8.05it/s]\n",
            "\u001b[32m[12/12 09:44:30 FallenTreePart]: \u001b[0mTest Epoch [58/100],Instance mIoU 94.30, Class mIoU 93.71, \n",
            " Class mIoUs tensor([100.0000,  87.4112], device='cuda:0')\n",
            "Train Epoch [59/100] Loss 0.003 : 100% 45/45 [00:07<00:00,  5.64it/s]\n",
            "100% 8/8 [00:00<00:00, 10.36it/s]\n",
            "\u001b[32m[12/12 09:44:39 FallenTreePart]: \u001b[0mTest Epoch [59/100],Instance mIoU 94.16, Class mIoU 93.56, \n",
            " Class mIoUs tensor([100.0000,  87.1164], device='cuda:0')\n",
            "Train Epoch [60/100] Loss 0.003 : 100% 45/45 [00:07<00:00,  5.68it/s]\n",
            "100% 8/8 [00:00<00:00, 10.44it/s]\n",
            "\u001b[32m[12/12 09:44:48 FallenTreePart]: \u001b[0mTest Epoch [60/100],Instance mIoU 94.48, Class mIoU 93.90, \n",
            " Class mIoUs tensor([100.0000,  87.8073], device='cuda:0')\n",
            "Train Epoch [61/100] Loss 0.003 : 100% 45/45 [00:07<00:00,  5.64it/s]\n",
            "100% 8/8 [00:00<00:00,  8.96it/s]\n",
            "\u001b[32m[12/12 09:44:57 FallenTreePart]: \u001b[0mTest Epoch [61/100],Instance mIoU 94.74, Class mIoU 94.20, \n",
            " Class mIoUs tensor([100.0000,  88.3979], device='cuda:0')\n",
            "Train Epoch [62/100] Loss 0.003 : 100% 45/45 [00:07<00:00,  5.68it/s]\n",
            "100% 8/8 [00:00<00:00, 10.67it/s]\n",
            "\u001b[32m[12/12 09:45:06 FallenTreePart]: \u001b[0mTest Epoch [62/100],Instance mIoU 94.66, Class mIoU 94.11, \n",
            " Class mIoUs tensor([100.0000,  88.2221], device='cuda:0')\n",
            "Train Epoch [63/100] Loss 0.002 : 100% 45/45 [00:07<00:00,  5.65it/s]\n",
            "100% 8/8 [00:00<00:00, 10.62it/s]\n",
            "\u001b[32m[12/12 09:45:15 FallenTreePart]: \u001b[0mTest Epoch [63/100],Instance mIoU 94.95, Class mIoU 94.42, \n",
            " Class mIoUs tensor([100.0000,  88.8495], device='cuda:0')\n",
            "Train Epoch [64/100] Loss 0.002 : 100% 45/45 [00:07<00:00,  5.64it/s]\n",
            "100% 8/8 [00:00<00:00,  8.81it/s]\n",
            "\u001b[32m[12/12 09:45:25 FallenTreePart]: \u001b[0mTest Epoch [64/100],Instance mIoU 95.00, Class mIoU 94.48, \n",
            " Class mIoUs tensor([100.0000,  88.9614], device='cuda:0')\n",
            "Train Epoch [65/100] Loss 0.002 : 100% 45/45 [00:07<00:00,  5.67it/s]\n",
            "100% 8/8 [00:00<00:00, 10.74it/s]\n",
            "\u001b[32m[12/12 09:45:34 FallenTreePart]: \u001b[0mTest Epoch [65/100],Instance mIoU 94.34, Class mIoU 93.76, \n",
            " Class mIoUs tensor([100.0000,  87.5131], device='cuda:0')\n",
            "Train Epoch [66/100] Loss 0.002 : 100% 45/45 [00:07<00:00,  5.65it/s]\n",
            "100% 8/8 [00:00<00:00, 10.75it/s]\n",
            "\u001b[32m[12/12 09:45:43 FallenTreePart]: \u001b[0mTest Epoch [66/100],Instance mIoU 95.41, Class mIoU 94.93, \n",
            " Class mIoUs tensor([100.0000,  89.8602], device='cuda:0')\n",
            "Train Epoch [67/100] Loss 0.002 : 100% 45/45 [00:07<00:00,  5.65it/s]\n",
            "100% 8/8 [00:00<00:00,  8.99it/s]\n",
            "\u001b[32m[12/12 09:45:52 FallenTreePart]: \u001b[0mTest Epoch [67/100],Instance mIoU 94.82, Class mIoU 94.28, \n",
            " Class mIoUs tensor([100.0000,  88.5574], device='cuda:0')\n",
            "Train Epoch [68/100] Loss 0.002 : 100% 45/45 [00:07<00:00,  5.67it/s]\n",
            "100% 8/8 [00:00<00:00, 10.79it/s]\n",
            "\u001b[32m[12/12 09:46:01 FallenTreePart]: \u001b[0mTest Epoch [68/100],Instance mIoU 95.22, Class mIoU 94.73, \n",
            " Class mIoUs tensor([100.0000,  89.4598], device='cuda:0')\n",
            "Train Epoch [69/100] Loss 0.002 : 100% 45/45 [00:07<00:00,  5.67it/s]\n",
            "100% 8/8 [00:00<00:00, 10.82it/s]\n",
            "\u001b[32m[12/12 09:46:10 FallenTreePart]: \u001b[0mTest Epoch [69/100],Instance mIoU 95.14, Class mIoU 94.64, \n",
            " Class mIoUs tensor([100.0000,  89.2807], device='cuda:0')\n",
            "Train Epoch [70/100] Loss 0.002 : 100% 45/45 [00:08<00:00,  5.60it/s]\n",
            "100% 8/8 [00:00<00:00,  9.77it/s]\n",
            "\u001b[32m[12/12 09:46:19 FallenTreePart]: \u001b[0mTest Epoch [70/100],Instance mIoU 95.05, Class mIoU 94.54, \n",
            " Class mIoUs tensor([100.0000,  89.0743], device='cuda:0')\n",
            "Train Epoch [71/100] Loss 0.002 : 100% 45/45 [00:07<00:00,  5.65it/s]\n",
            "100% 8/8 [00:00<00:00, 10.76it/s]\n",
            "\u001b[32m[12/12 09:46:28 FallenTreePart]: \u001b[0mTest Epoch [71/100],Instance mIoU 95.23, Class mIoU 94.74, \n",
            " Class mIoUs tensor([100.0000,  89.4841], device='cuda:0')\n",
            "Train Epoch [72/100] Loss 0.002 : 100% 45/45 [00:07<00:00,  5.65it/s]\n",
            "100% 8/8 [00:00<00:00, 10.82it/s]\n",
            "\u001b[32m[12/12 09:46:37 FallenTreePart]: \u001b[0mTest Epoch [72/100],Instance mIoU 95.29, Class mIoU 94.81, \n",
            " Class mIoUs tensor([100.0000,  89.6112], device='cuda:0')\n",
            "Train Epoch [73/100] Loss 0.002 : 100% 45/45 [00:07<00:00,  5.65it/s]\n",
            "100% 8/8 [00:00<00:00, 10.55it/s]\n",
            "\u001b[32m[12/12 09:46:46 FallenTreePart]: \u001b[0mTest Epoch [73/100],Instance mIoU 95.29, Class mIoU 94.80, \n",
            " Class mIoUs tensor([100.0000,  89.6021], device='cuda:0')\n",
            "Train Epoch [74/100] Loss 0.002 : 100% 45/45 [00:07<00:00,  5.69it/s]\n",
            "100% 8/8 [00:00<00:00, 10.31it/s]\n",
            "\u001b[32m[12/12 09:46:55 FallenTreePart]: \u001b[0mTest Epoch [74/100],Instance mIoU 95.29, Class mIoU 94.80, \n",
            " Class mIoUs tensor([100.0000,  89.5974], device='cuda:0')\n",
            "Train Epoch [75/100] Loss 0.002 : 100% 45/45 [00:08<00:00,  5.62it/s]\n",
            "100% 8/8 [00:00<00:00, 10.26it/s]\n",
            "\u001b[32m[12/12 09:47:04 FallenTreePart]: \u001b[0mTest Epoch [75/100],Instance mIoU 95.28, Class mIoU 94.79, \n",
            " Class mIoUs tensor([100.0000,  89.5769], device='cuda:0')\n",
            "Train Epoch [76/100] Loss 0.002 : 100% 45/45 [00:08<00:00,  5.62it/s]\n",
            "100% 8/8 [00:00<00:00,  9.23it/s]\n",
            "\u001b[32m[12/12 09:47:13 FallenTreePart]: \u001b[0mTest Epoch [76/100],Instance mIoU 95.28, Class mIoU 94.80, \n",
            " Class mIoUs tensor([100.0000,  89.5905], device='cuda:0')\n",
            "Train Epoch [77/100] Loss 0.002 : 100% 45/45 [00:07<00:00,  5.66it/s]\n",
            "100% 8/8 [00:00<00:00, 10.47it/s]\n",
            "\u001b[32m[12/12 09:47:22 FallenTreePart]: \u001b[0mTest Epoch [77/100],Instance mIoU 95.28, Class mIoU 94.80, \n",
            " Class mIoUs tensor([100.0000,  89.5920], device='cuda:0')\n",
            "Train Epoch [78/100] Loss 0.002 : 100% 45/45 [00:08<00:00,  5.60it/s]\n",
            "100% 8/8 [00:00<00:00, 10.85it/s]\n",
            "\u001b[32m[12/12 09:47:31 FallenTreePart]: \u001b[0mTest Epoch [78/100],Instance mIoU 95.27, Class mIoU 94.78, \n",
            " Class mIoUs tensor([100.0000,  89.5653], device='cuda:0')\n",
            "Train Epoch [79/100] Loss 0.002 : 100% 45/45 [00:07<00:00,  5.67it/s]\n",
            "100% 8/8 [00:00<00:00, 10.48it/s]\n",
            "\u001b[32m[12/12 09:47:40 FallenTreePart]: \u001b[0mTest Epoch [79/100],Instance mIoU 95.27, Class mIoU 94.78, \n",
            " Class mIoUs tensor([100.0000,  89.5687], device='cuda:0')\n",
            "Train Epoch [80/100] Loss 0.002 : 100% 45/45 [00:07<00:00,  5.66it/s]\n",
            "100% 8/8 [00:00<00:00,  9.56it/s]\n",
            "\u001b[32m[12/12 09:47:50 FallenTreePart]: \u001b[0mTest Epoch [80/100],Instance mIoU 95.27, Class mIoU 94.78, \n",
            " Class mIoUs tensor([100.0000,  89.5521], device='cuda:0')\n",
            "Train Epoch [81/100] Loss 0.002 : 100% 45/45 [00:08<00:00,  5.58it/s]\n",
            "100% 8/8 [00:00<00:00, 10.17it/s]\n",
            "\u001b[32m[12/12 09:47:59 FallenTreePart]: \u001b[0mTest Epoch [81/100],Instance mIoU 95.26, Class mIoU 94.77, \n",
            " Class mIoUs tensor([100.0000,  89.5303], device='cuda:0')\n",
            "Train Epoch [82/100] Loss 0.002 : 100% 45/45 [00:07<00:00,  5.65it/s]\n",
            "100% 8/8 [00:00<00:00, 10.32it/s]\n",
            "\u001b[32m[12/12 09:48:08 FallenTreePart]: \u001b[0mTest Epoch [82/100],Instance mIoU 95.25, Class mIoU 94.76, \n",
            " Class mIoUs tensor([100.0000,  89.5148], device='cuda:0')\n",
            "Train Epoch [83/100] Loss 0.001 : 100% 45/45 [00:07<00:00,  5.64it/s]\n",
            "100% 8/8 [00:00<00:00,  8.43it/s]\n",
            "\u001b[32m[12/12 09:48:17 FallenTreePart]: \u001b[0mTest Epoch [83/100],Instance mIoU 95.25, Class mIoU 94.75, \n",
            " Class mIoUs tensor([100.0000,  89.5091], device='cuda:0')\n",
            "Train Epoch [84/100] Loss 0.001 : 100% 45/45 [00:08<00:00,  5.60it/s]\n",
            "100% 8/8 [00:00<00:00, 10.07it/s]\n",
            "\u001b[32m[12/12 09:48:26 FallenTreePart]: \u001b[0mTest Epoch [84/100],Instance mIoU 95.24, Class mIoU 94.75, \n",
            " Class mIoUs tensor([100.0000,  89.4908], device='cuda:0')\n",
            "Train Epoch [85/100] Loss 0.001 : 100% 45/45 [00:07<00:00,  5.66it/s]\n",
            "100% 8/8 [00:00<00:00, 10.67it/s]\n",
            "\u001b[32m[12/12 09:48:35 FallenTreePart]: \u001b[0mTest Epoch [85/100],Instance mIoU 95.23, Class mIoU 94.74, \n",
            " Class mIoUs tensor([100.0000,  89.4775], device='cuda:0')\n",
            "Train Epoch [86/100] Loss 0.001 : 100% 45/45 [00:07<00:00,  5.63it/s]\n",
            "100% 8/8 [00:00<00:00,  8.13it/s]\n",
            "\u001b[32m[12/12 09:48:45 FallenTreePart]: \u001b[0mTest Epoch [86/100],Instance mIoU 95.23, Class mIoU 94.74, \n",
            " Class mIoUs tensor([100.0000,  89.4808], device='cuda:0')\n",
            "Train Epoch [87/100] Loss 0.001 : 100% 45/45 [00:08<00:00,  5.61it/s]\n",
            "100% 8/8 [00:00<00:00, 10.61it/s]\n",
            "\u001b[32m[12/12 09:48:54 FallenTreePart]: \u001b[0mTest Epoch [87/100],Instance mIoU 95.24, Class mIoU 94.74, \n",
            " Class mIoUs tensor([100.0000,  89.4885], device='cuda:0')\n",
            "Train Epoch [88/100] Loss 0.001 : 100% 45/45 [00:07<00:00,  5.66it/s]\n",
            "100% 8/8 [00:00<00:00, 10.92it/s]\n",
            "\u001b[32m[12/12 09:49:03 FallenTreePart]: \u001b[0mTest Epoch [88/100],Instance mIoU 95.23, Class mIoU 94.74, \n",
            " Class mIoUs tensor([100.0000,  89.4814], device='cuda:0')\n",
            "Train Epoch [89/100] Loss 0.001 : 100% 45/45 [00:07<00:00,  5.63it/s]\n",
            "100% 8/8 [00:00<00:00,  8.20it/s]\n",
            "\u001b[32m[12/12 09:49:12 FallenTreePart]: \u001b[0mTest Epoch [89/100],Instance mIoU 95.24, Class mIoU 94.74, \n",
            " Class mIoUs tensor([100.0000,  89.4877], device='cuda:0')\n",
            "Train Epoch [90/100] Loss 0.001 : 100% 45/45 [00:08<00:00,  5.57it/s]\n",
            "100% 8/8 [00:00<00:00, 10.26it/s]\n",
            "\u001b[32m[12/12 09:49:21 FallenTreePart]: \u001b[0mTest Epoch [90/100],Instance mIoU 95.28, Class mIoU 94.79, \n",
            " Class mIoUs tensor([100.0000,  89.5781], device='cuda:0')\n",
            "Train Epoch [91/100] Loss 0.001 : 100% 45/45 [00:07<00:00,  5.65it/s]\n",
            "100% 8/8 [00:00<00:00, 10.55it/s]\n",
            "\u001b[32m[12/12 09:49:30 FallenTreePart]: \u001b[0mTest Epoch [91/100],Instance mIoU 95.28, Class mIoU 94.79, \n",
            " Class mIoUs tensor([100.0000,  89.5822], device='cuda:0')\n",
            "Train Epoch [92/100] Loss 0.001 : 100% 45/45 [00:07<00:00,  5.63it/s]\n",
            "100% 8/8 [00:00<00:00,  8.60it/s]\n",
            "\u001b[32m[12/12 09:49:40 FallenTreePart]: \u001b[0mTest Epoch [92/100],Instance mIoU 95.28, Class mIoU 94.79, \n",
            " Class mIoUs tensor([100.0000,  89.5823], device='cuda:0')\n",
            "Train Epoch [93/100] Loss 0.001 : 100% 45/45 [00:08<00:00,  5.59it/s]\n",
            "100% 8/8 [00:00<00:00, 10.81it/s]\n",
            "\u001b[32m[12/12 09:49:49 FallenTreePart]: \u001b[0mTest Epoch [93/100],Instance mIoU 95.28, Class mIoU 94.79, \n",
            " Class mIoUs tensor([100.0000,  89.5780], device='cuda:0')\n",
            "Train Epoch [94/100] Loss 0.001 : 100% 45/45 [00:07<00:00,  5.68it/s]\n",
            "100% 8/8 [00:00<00:00, 10.29it/s]\n",
            "\u001b[32m[12/12 09:49:58 FallenTreePart]: \u001b[0mTest Epoch [94/100],Instance mIoU 95.28, Class mIoU 94.79, \n",
            " Class mIoUs tensor([100.0000,  89.5803], device='cuda:0')\n",
            "Train Epoch [95/100] Loss 0.001 : 100% 45/45 [00:07<00:00,  5.65it/s]\n",
            "100% 8/8 [00:00<00:00,  8.57it/s]\n",
            "\u001b[32m[12/12 09:50:07 FallenTreePart]: \u001b[0mTest Epoch [95/100],Instance mIoU 95.28, Class mIoU 94.79, \n",
            " Class mIoUs tensor([100.0000,  89.5857], device='cuda:0')\n",
            "Train Epoch [96/100] Loss 0.001 : 100% 45/45 [00:08<00:00,  5.61it/s]\n",
            "100% 8/8 [00:00<00:00, 10.02it/s]\n",
            "\u001b[32m[12/12 09:50:16 FallenTreePart]: \u001b[0mTest Epoch [96/100],Instance mIoU 95.28, Class mIoU 94.79, \n",
            " Class mIoUs tensor([100.0000,  89.5843], device='cuda:0')\n",
            "Train Epoch [97/100] Loss 0.001 : 100% 45/45 [00:07<00:00,  5.65it/s]\n",
            "100% 8/8 [00:00<00:00, 10.46it/s]\n",
            "\u001b[32m[12/12 09:50:25 FallenTreePart]: \u001b[0mTest Epoch [97/100],Instance mIoU 95.28, Class mIoU 94.79, \n",
            " Class mIoUs tensor([100.0000,  89.5854], device='cuda:0')\n",
            "Train Epoch [98/100] Loss 0.001 : 100% 45/45 [00:07<00:00,  5.63it/s]\n",
            "100% 8/8 [00:00<00:00,  8.03it/s]\n",
            "\u001b[32m[12/12 09:50:35 FallenTreePart]: \u001b[0mTest Epoch [98/100],Instance mIoU 95.28, Class mIoU 94.79, \n",
            " Class mIoUs tensor([100.0000,  89.5766], device='cuda:0')\n",
            "Train Epoch [99/100] Loss 0.001 : 100% 45/45 [00:07<00:00,  5.67it/s]\n",
            "100% 8/8 [00:00<00:00, 10.63it/s]\n",
            "\u001b[32m[12/12 09:50:44 FallenTreePart]: \u001b[0mTest Epoch [99/100],Instance mIoU 95.28, Class mIoU 94.79, \n",
            " Class mIoUs tensor([100.0000,  89.5801], device='cuda:0')\n",
            "Train Epoch [100/100] Loss 0.001 : 100% 45/45 [00:07<00:00,  5.66it/s]\n",
            "100% 8/8 [00:00<00:00, 10.42it/s]\n",
            "\u001b[32m[12/12 09:50:53 FallenTreePart]: \u001b[0mTest Epoch [100/100],Instance mIoU 95.28, Class mIoU 94.79, \n",
            " Class mIoUs tensor([100.0000,  89.5737], device='cuda:0')\n",
            "\u001b[32m[12/12 09:50:53 FallenTreePart]: \u001b[0mBest Epoch 39,Instance mIoU 95.87, Class mIoU 95.44, \n",
            " Class mIoUs tensor([100.0000,  90.8782], device='cuda:0')\n",
            "\u001b[32m[12/12 09:50:53 FallenTreePart]: \u001b[0mSuccessful Loading the ckpt from log/shapenetpart/shapenetpart-train-custom_fallen_trees-ngpus1-seed1066-20251212-093536-S85TXDdERtSiQuih8nf377/checkpoint/shapenetpart-train-custom_fallen_trees-ngpus1-seed1066-20251212-093536-S85TXDdERtSiQuih8nf377_ckpt_best.pth\n",
            "\u001b[32m[12/12 09:50:53 FallenTreePart]: \u001b[0mckpts @ 39 epoch( {} )\n",
            "100% 8/8 [00:05<00:00,  1.45it/s]\n",
            "\u001b[32m[12/12 09:50:58 FallenTreePart]: \u001b[0mTest Epoch [100/100],Instance mIoU 95.60, Class mIoU 95.14, \n",
            " Class mIoUs tensor([100.0000,  90.2828], device='cuda:0')\n",
            "\u001b[32m[12/12 09:50:58 FallenTreePart]: \u001b[0m---Voting---\n",
            "Best Epoch 39,Voting Instance mIoU 95.60, Voting Class mIoU 95.14, \n",
            " Voting Class mIoUs tensor([100.0000,  90.2828], device='cuda:0')\n",
            "Traceback (most recent call last):\n",
            "  File \"/content/PointNeXt/examples/shapenetpart/main.py\", line 447, in <module>\n",
            "    main(0, cfg)\n",
            "  File \"/content/PointNeXt/examples/shapenetpart/main.py\", line 283, in main\n",
            "    dist.destroy_process_group()\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/torch/distributed/distributed_c10d.py\", line 1721, in destroy_process_group\n",
            "    assert pg is not None\n",
            "           ^^^^^^^^^^^^^^\n",
            "AssertionError\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Save to Drive"
      ],
      "metadata": {
        "id": "WiZVTd19mU_T"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Smart Backup using Glob\n",
        "import glob\n",
        "\n",
        "# 1. Define the pattern (The * acts as the regex)\n",
        "# We look for any folder starting with the project name inside the log dir\n",
        "search_pattern = \"/content/PointNeXt/log/shapenetpart/shapenetpart-train-custom_fallen_trees*\"\n",
        "\n",
        "print(f\" Searching for runs matching: {search_pattern}...\")\n",
        "\n",
        "# 2. Find all matching folders\n",
        "found_folders = glob.glob(search_pattern)\n",
        "\n",
        "if not found_folders:\n",
        "    print(\"\\u274c Error: No training folders found matching that pattern.\")\n",
        "else:\n",
        "    # 3. Pick the LATEST folder (in case trained multiple times)\n",
        "    latest_run_dir = max(found_folders, key=os.path.getmtime)\n",
        "    print(f\"\\u2705 Found latest run: {os.path.basename(latest_run_dir)}\")\n",
        "\n",
        "    # 4. Construct the checkpoint path\n",
        "    source_ckpt_dir = os.path.join(latest_run_dir, \"checkpoint\")\n",
        "    drive_model_dir = \"/content/drive/MyDrive/ML_Projects/PointNeXt/Models\"\n",
        "\n",
        "    # 5. Perform Backup\n",
        "    if os.path.exists(source_ckpt_dir):\n",
        "        os.makedirs(drive_model_dir, exist_ok=True)\n",
        "        files = glob.glob(os.path.join(source_ckpt_dir, \"*_best.pth\"))\n",
        "\n",
        "        if files:\n",
        "            best_model = files[0]\n",
        "            filename = os.path.basename(best_model)\n",
        "            dest_path = os.path.join(drive_model_dir, filename)\n",
        "            shutil.copy(best_model, dest_path)\n",
        "            print(f\"\\U0001F4BE Success! Model saved to: {dest_path}\")\n",
        "        else:\n",
        "            print(\"\\u26A0\\uFE0F Warning: No '_best.pth' file found.\")\n",
        "    else:\n",
        "        print(f\"\\u274c Error: Checkpoint folder missing in {latest_run_dir}\")"
      ],
      "metadata": {
        "id": "1tfOQoTPqBcL",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2457b585-d5e4-4d2f-e8bc-a162da030a35"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " Searching for runs matching: /content/PointNeXt/log/shapenetpart/shapenetpart-train-custom_fallen_trees*...\n",
            "‚úÖ Found latest run: shapenetpart-train-custom_fallen_trees-ngpus1-seed1066-20251212-093536-S85TXDdERtSiQuih8nf377\n",
            "üíæ Success! Model saved to: /content/drive/MyDrive/ML_Projects/PointNeXt/Models/shapenetpart-train-custom_fallen_trees-ngpus1-seed1066-20251212-093536-S85TXDdERtSiQuih8nf377_ckpt_best.pth\n"
          ]
        }
      ]
    }
  ]
}