{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Nobobi-Hasan/PointNeXt-PartSegmentation-FallenTrees/blob/main/PointNeXt_03_05_Testing_shapenetpart.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "sFv142IrXih-"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "import os\n",
        "import shutil\n",
        "import torch\n",
        "import glob\n",
        "import subprocess\n",
        "import sys"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "s_7fmYKOY0ek",
        "outputId": "84c75112-ef91-49a0-b2b4-df0517c6dfdb"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HyfAxPHiY1xD",
        "outputId": "1496ebc1-6129-4f2a-cd24-b16c107f641c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚úÖ Found Model: shapenetpart-train-custom_fallen_trees-ngpus1-seed7520-20251212-155340-E84zcZ3REWTnK2wbniefub_ckpt_best.pth\n",
            "Project Root: /content/drive/MyDrive/ML_Projects/PointNeXt\n"
          ]
        }
      ],
      "source": [
        "# Project Paths\n",
        "DRIVE_PROJECT_ROOT = \"/content/drive/MyDrive/ML_Projects/PointNeXt\"\n",
        "DRIVE_DATA_ZIP = os.path.join(DRIVE_PROJECT_ROOT, \"Data/processed_data.zip\")\n",
        "DRIVE_RAW_DATA_PATH = os.path.join(DRIVE_PROJECT_ROOT, \"Data/data.zip\")\n",
        "DRIVE_MODEL_DIR = os.path.join(DRIVE_PROJECT_ROOT, \"Models\")\n",
        "# raw_extract_path = \"/content/raw_data\"\n",
        "\n",
        "# Find the latest best model automatically\n",
        "model_candidates = glob.glob(os.path.join(DRIVE_MODEL_DIR, \"*_best.pth\"))\n",
        "if not model_candidates:\n",
        "    print(\"\\u274c Error: No model found in Drive!\")\n",
        "    model_path = \"\"\n",
        "else:\n",
        "    # Pick the most recently created file\n",
        "    model_path = max(model_candidates, key=os.path.getmtime)\n",
        "    print(f\"\\u2705 Found Model: {os.path.basename(model_path)}\")\n",
        "\n",
        "print(f\"Project Root: {DRIVE_PROJECT_ROOT}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "O0NDcQgVZKE8"
      },
      "source": [
        "# Re-Install Environment (Identical to Training)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "zGKsQfViY6jL"
      },
      "outputs": [],
      "source": [
        "# Copy & Unzip Processed Data\n",
        "if not os.path.exists(\"/content/processed_data\"):\n",
        "    if os.path.exists(DRIVE_DATA_ZIP):\n",
        "        print(\"Copying processed data from Drive...\")\n",
        "        shutil.copy(DRIVE_DATA_ZIP, \"/content/processed_data.zip\")\n",
        "        print(\"Unzipping...\")\n",
        "        !unzip -q -o /content/processed_data.zip -d /\n",
        "        print(\"Data Ready.\")\n",
        "    else:\n",
        "        print(f\"Error: Processed Data zip not found at {DRIVE_DATA_ZIP}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "aSZC0zfDRIK6"
      },
      "outputs": [],
      "source": [
        "# Copy & Unzip Raw Data\n",
        "if not os.path.exists(\"/content/raw_data\"):\n",
        "    if os.path.exists(DRIVE_RAW_DATA_PATH):\n",
        "        print(\"Copying raw data from Drive...\")\n",
        "        shutil.copy(DRIVE_RAW_DATA_PATH, \"/content/raw_data.zip\")\n",
        "        print(\"Unzipping...\")\n",
        "        !unzip -q /content/raw_data.zip -d /content/raw_data\n",
        "        print(\"Data Ready.\")\n",
        "    else:\n",
        "        print(f\"Error: Raw Data zip not found at {DRIVE_RAW_DATA_PATH}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "H9sLTQ64g5wQ"
      },
      "outputs": [],
      "source": [
        "# remove __MACOSX from raw_data folder\n",
        "!rm -rf /content/raw_data/__MACOSX"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fwNjdRQUaRRr",
        "outputId": "a5aec7f3-e753-4480-da14-8f4b5709ac06"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning PointNeXt...\n",
            "/content\n",
            "Cloning into 'PointNeXt'...\n",
            "remote: Enumerating objects: 723, done.\u001b[K\n",
            "remote: Counting objects: 100% (327/327), done.\u001b[K\n",
            "remote: Compressing objects: 100% (145/145), done.\u001b[K\n",
            "remote: Total 723 (delta 247), reused 182 (delta 182), pack-reused 396 (from 1)\u001b[K\n",
            "Receiving objects: 100% (723/723), 5.22 MiB | 14.34 MiB/s, done.\n",
            "Resolving deltas: 100% (428/428), done.\n"
          ]
        }
      ],
      "source": [
        "# Clone PointNeXt (Clean)\n",
        "if os.path.exists(\"/content/PointNeXt\"):\n",
        "    shutil.rmtree(\"/content/PointNeXt\")\n",
        "print(\"Cloning PointNeXt...\")\n",
        "%cd /content\n",
        "!git clone https://github.com/guochengqian/PointNeXt.git"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iqwYQnsWaYFz",
        "outputId": "fbaf4b57-524d-4715-aa1f-8d7217aa4eec"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/PointNeXt\n",
            "Downloading openpoints via HTTPS...\n",
            "Submodule 'openpoints' (https://github.com/guochengqian/openpoints.git) registered for path 'openpoints'\n",
            "Cloning into '/content/PointNeXt/openpoints'...\n",
            "Submodule path 'openpoints': checked out 'baeca5e319aa2e756d179e494469eb7f5ffd29f0'\n",
            "‚úÖ Submodule 'openpoints' downloaded successfully.\n"
          ]
        }
      ],
      "source": [
        "# For openpoints\n",
        "\n",
        "%cd /content/PointNeXt\n",
        "\n",
        "# 1. Replace SSH url with HTTPS url in .gitmodules\n",
        "!sed -i 's/git@github.com:/https:\\/\\/github.com\\//' .gitmodules\n",
        "\n",
        "# 2. Sync the new URL\n",
        "!git submodule sync\n",
        "\n",
        "# 3. Update the submodule (This will work now)\n",
        "print(\"Downloading openpoints via HTTPS...\")\n",
        "!git submodule update --init --recursive\n",
        "\n",
        "print(\"\\u2705 Submodule 'openpoints' downloaded successfully.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "91jtkjUbaSud",
        "outputId": "6b8db018-7b39-4fe0-8842-02500bac21ec"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Installing Dependencies...\n",
            "/content/PointNeXt\n",
            "Looking in indexes: https://download.pytorch.org/whl/cu121\n",
            "Requirement already satisfied: torch==2.4.0 in /usr/local/lib/python3.12/dist-packages (2.4.0+cu121)\n",
            "Requirement already satisfied: torchvision==0.19.0 in /usr/local/lib/python3.12/dist-packages (0.19.0+cu121)\n",
            "Requirement already satisfied: torchaudio==2.4.0 in /usr/local/lib/python3.12/dist-packages (2.4.0+cu121)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from torch==2.4.0) (3.20.0)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.12/dist-packages (from torch==2.4.0) (4.15.0)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.12/dist-packages (from torch==2.4.0) (1.14.0)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.12/dist-packages (from torch==2.4.0) (3.6.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from torch==2.4.0) (3.1.6)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.12/dist-packages (from torch==2.4.0) (2025.3.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (from torch==2.4.0) (75.2.0)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.1.105 in /usr/local/lib/python3.12/dist-packages (from torch==2.4.0) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.1.105 in /usr/local/lib/python3.12/dist-packages (from torch==2.4.0) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.1.105 in /usr/local/lib/python3.12/dist-packages (from torch==2.4.0) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /usr/local/lib/python3.12/dist-packages (from torch==2.4.0) (9.1.0.70)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.1.3.1 in /usr/local/lib/python3.12/dist-packages (from torch==2.4.0) (12.1.3.1)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.0.2.54 in /usr/local/lib/python3.12/dist-packages (from torch==2.4.0) (11.0.2.54)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.2.106 in /usr/local/lib/python3.12/dist-packages (from torch==2.4.0) (10.3.2.106)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.4.5.107 in /usr/local/lib/python3.12/dist-packages (from torch==2.4.0) (11.4.5.107)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.1.0.106 in /usr/local/lib/python3.12/dist-packages (from torch==2.4.0) (12.1.0.106)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.20.5 in /usr/local/lib/python3.12/dist-packages (from torch==2.4.0) (2.20.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.1.105 in /usr/local/lib/python3.12/dist-packages (from torch==2.4.0) (12.1.105)\n",
            "Requirement already satisfied: triton==3.0.0 in /usr/local/lib/python3.12/dist-packages (from torch==2.4.0) (3.0.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.12/dist-packages (from torchvision==0.19.0) (2.0.2)\n",
            "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.12/dist-packages (from torchvision==0.19.0) (11.3.0)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12 in /usr/local/lib/python3.12/dist-packages (from nvidia-cusolver-cu12==11.4.5.107->torch==2.4.0) (12.6.85)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2->torch==2.4.0) (3.0.3)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from sympy->torch==2.4.0) (1.3.0)\n",
            "Looking in links: https://data.pyg.org/whl/torch-2.4.0+cu121.html\n",
            "Requirement already satisfied: torch-scatter in /usr/local/lib/python3.12/dist-packages (2.1.2+pt24cu121)\n",
            "Requirement already satisfied: torch-sparse in /usr/local/lib/python3.12/dist-packages (0.6.18+pt24cu121)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.12/dist-packages (from torch-sparse) (1.16.3)\n",
            "Requirement already satisfied: numpy<2.6,>=1.25.2 in /usr/local/lib/python3.12/dist-packages (from scipy->torch-sparse) (2.0.2)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.12/dist-packages (from -r requirements.txt (line 1)) (1.6.1)\n",
            "Requirement already satisfied: pickleshare in /usr/local/lib/python3.12/dist-packages (from -r requirements.txt (line 2)) (0.7.5)\n",
            "Requirement already satisfied: ninja in /usr/local/lib/python3.12/dist-packages (from -r requirements.txt (line 3)) (1.13.0)\n",
            "Requirement already satisfied: gdown in /usr/local/lib/python3.12/dist-packages (from -r requirements.txt (line 4)) (5.2.0)\n",
            "Requirement already satisfied: easydict in /usr/local/lib/python3.12/dist-packages (from -r requirements.txt (line 5)) (1.13)\n",
            "Requirement already satisfied: PyYAML in /usr/local/lib/python3.12/dist-packages (from -r requirements.txt (line 6)) (6.0.3)\n",
            "Requirement already satisfied: protobuf in /usr/local/lib/python3.12/dist-packages (from -r requirements.txt (line 7)) (5.29.5)\n",
            "Requirement already satisfied: tensorboard in /usr/local/lib/python3.12/dist-packages (from -r requirements.txt (line 8)) (2.19.0)\n",
            "Requirement already satisfied: termcolor in /usr/local/lib/python3.12/dist-packages (from -r requirements.txt (line 9)) (3.2.0)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.12/dist-packages (from -r requirements.txt (line 10)) (4.67.1)\n",
            "Requirement already satisfied: multimethod in /usr/local/lib/python3.12/dist-packages (from -r requirements.txt (line 11)) (2.0.2)\n",
            "Requirement already satisfied: h5py in /usr/local/lib/python3.12/dist-packages (from -r requirements.txt (line 12)) (3.15.1)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.12/dist-packages (from -r requirements.txt (line 13)) (3.10.0)\n",
            "Requirement already satisfied: wandb in /usr/local/lib/python3.12/dist-packages (from -r requirements.txt (line 14)) (0.23.1)\n",
            "Collecting pyvista (from -r requirements.txt (line 15))\n",
            "  Using cached pyvista-0.46.4-py3-none-any.whl.metadata (15 kB)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (from -r requirements.txt (line 16)) (75.2.0)\n",
            "Requirement already satisfied: Cython in /usr/local/lib/python3.12/dist-packages (from -r requirements.txt (line 17)) (3.0.12)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.12/dist-packages (from -r requirements.txt (line 18)) (2.2.2)\n",
            "Collecting deepspeed (from -r requirements.txt (line 19))\n",
            "  Using cached deepspeed-0.18.3-py3-none-any.whl\n",
            "Requirement already satisfied: shortuuid in /usr/local/lib/python3.12/dist-packages (from -r requirements.txt (line 20)) (1.0.13)\n",
            "Collecting mkdocs-material (from -r requirements.txt (line 23))\n",
            "  Using cached mkdocs_material-9.7.0-py3-none-any.whl.metadata (19 kB)\n",
            "Collecting mkdocs-awesome-pages-plugin (from -r requirements.txt (line 24))\n",
            "  Using cached mkdocs_awesome_pages_plugin-2.10.1-py3-none-any.whl.metadata (13 kB)\n",
            "Requirement already satisfied: mdx_truly_sane_lists in /usr/local/lib/python3.12/dist-packages (from -r requirements.txt (line 25)) (1.3)\n",
            "Requirement already satisfied: numpy>=1.19.5 in /usr/local/lib/python3.12/dist-packages (from scikit-learn->-r requirements.txt (line 1)) (2.0.2)\n",
            "Requirement already satisfied: scipy>=1.6.0 in /usr/local/lib/python3.12/dist-packages (from scikit-learn->-r requirements.txt (line 1)) (1.16.3)\n",
            "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.12/dist-packages (from scikit-learn->-r requirements.txt (line 1)) (1.5.2)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.12/dist-packages (from scikit-learn->-r requirements.txt (line 1)) (3.6.0)\n",
            "Requirement already satisfied: beautifulsoup4 in /usr/local/lib/python3.12/dist-packages (from gdown->-r requirements.txt (line 4)) (4.13.5)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from gdown->-r requirements.txt (line 4)) (3.20.0)\n",
            "Requirement already satisfied: requests[socks] in /usr/local/lib/python3.12/dist-packages (from gdown->-r requirements.txt (line 4)) (2.32.4)\n",
            "Requirement already satisfied: absl-py>=0.4 in /usr/local/lib/python3.12/dist-packages (from tensorboard->-r requirements.txt (line 8)) (1.4.0)\n",
            "Requirement already satisfied: grpcio>=1.48.2 in /usr/local/lib/python3.12/dist-packages (from tensorboard->-r requirements.txt (line 8)) (1.76.0)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.12/dist-packages (from tensorboard->-r requirements.txt (line 8)) (3.10)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.12/dist-packages (from tensorboard->-r requirements.txt (line 8)) (25.0)\n",
            "Requirement already satisfied: six>1.9 in /usr/local/lib/python3.12/dist-packages (from tensorboard->-r requirements.txt (line 8)) (1.17.0)\n",
            "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /usr/local/lib/python3.12/dist-packages (from tensorboard->-r requirements.txt (line 8)) (0.7.2)\n",
            "Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.12/dist-packages (from tensorboard->-r requirements.txt (line 8)) (3.1.4)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib->-r requirements.txt (line 13)) (1.3.3)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.12/dist-packages (from matplotlib->-r requirements.txt (line 13)) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.12/dist-packages (from matplotlib->-r requirements.txt (line 13)) (4.61.0)\n",
            "Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib->-r requirements.txt (line 13)) (1.4.9)\n",
            "Requirement already satisfied: pillow>=8 in /usr/local/lib/python3.12/dist-packages (from matplotlib->-r requirements.txt (line 13)) (11.3.0)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib->-r requirements.txt (line 13)) (3.2.5)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.12/dist-packages (from matplotlib->-r requirements.txt (line 13)) (2.9.0.post0)\n",
            "Requirement already satisfied: click>=8.0.1 in /usr/local/lib/python3.12/dist-packages (from wandb->-r requirements.txt (line 14)) (8.3.1)\n",
            "Requirement already satisfied: gitpython!=3.1.29,>=1.0.0 in /usr/local/lib/python3.12/dist-packages (from wandb->-r requirements.txt (line 14)) (3.1.45)\n",
            "Requirement already satisfied: platformdirs in /usr/local/lib/python3.12/dist-packages (from wandb->-r requirements.txt (line 14)) (4.5.1)\n",
            "Requirement already satisfied: pydantic<3 in /usr/local/lib/python3.12/dist-packages (from wandb->-r requirements.txt (line 14)) (2.12.3)\n",
            "Requirement already satisfied: sentry-sdk>=2.0.0 in /usr/local/lib/python3.12/dist-packages (from wandb->-r requirements.txt (line 14)) (2.47.0)\n",
            "Requirement already satisfied: typing-extensions<5,>=4.8 in /usr/local/lib/python3.12/dist-packages (from wandb->-r requirements.txt (line 14)) (4.15.0)\n",
            "Requirement already satisfied: pooch in /usr/local/lib/python3.12/dist-packages (from pyvista->-r requirements.txt (line 15)) (1.8.2)\n",
            "Requirement already satisfied: scooby>=0.5.1 in /usr/local/lib/python3.12/dist-packages (from pyvista->-r requirements.txt (line 15)) (0.11.0)\n",
            "Collecting vtk!=9.4.0 (from pyvista->-r requirements.txt (line 15))\n",
            "  Using cached vtk-9.5.2-cp312-cp312-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (5.6 kB)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.12/dist-packages (from pandas->-r requirements.txt (line 18)) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.12/dist-packages (from pandas->-r requirements.txt (line 18)) (2025.2)\n",
            "Requirement already satisfied: einops in /usr/local/lib/python3.12/dist-packages (from deepspeed->-r requirements.txt (line 19)) (0.8.1)\n",
            "Requirement already satisfied: hjson in /usr/local/lib/python3.12/dist-packages (from deepspeed->-r requirements.txt (line 19)) (3.1.0)\n",
            "Requirement already satisfied: msgpack in /usr/local/lib/python3.12/dist-packages (from deepspeed->-r requirements.txt (line 19)) (1.1.2)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.12/dist-packages (from deepspeed->-r requirements.txt (line 19)) (5.9.5)\n",
            "Requirement already satisfied: py-cpuinfo in /usr/local/lib/python3.12/dist-packages (from deepspeed->-r requirements.txt (line 19)) (9.0.0)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.12/dist-packages (from deepspeed->-r requirements.txt (line 19)) (2.4.0+cu121)\n",
            "Requirement already satisfied: nvidia-ml-py in /usr/local/lib/python3.12/dist-packages (from deepspeed->-r requirements.txt (line 19)) (13.590.44)\n",
            "Requirement already satisfied: babel>=2.10 in /usr/local/lib/python3.12/dist-packages (from mkdocs-material->-r requirements.txt (line 23)) (2.17.0)\n",
            "Requirement already satisfied: backrefs>=5.7.post1 in /usr/local/lib/python3.12/dist-packages (from mkdocs-material->-r requirements.txt (line 23)) (6.1)\n",
            "Requirement already satisfied: colorama>=0.4 in /usr/local/lib/python3.12/dist-packages (from mkdocs-material->-r requirements.txt (line 23)) (0.4.6)\n",
            "Requirement already satisfied: jinja2>=3.1 in /usr/local/lib/python3.12/dist-packages (from mkdocs-material->-r requirements.txt (line 23)) (3.1.6)\n",
            "Requirement already satisfied: mkdocs-material-extensions>=1.3 in /usr/local/lib/python3.12/dist-packages (from mkdocs-material->-r requirements.txt (line 23)) (1.3.1)\n",
            "Collecting mkdocs>=1.6 (from mkdocs-material->-r requirements.txt (line 23))\n",
            "  Using cached mkdocs-1.6.1-py3-none-any.whl.metadata (6.0 kB)\n",
            "Requirement already satisfied: paginate>=0.5 in /usr/local/lib/python3.12/dist-packages (from mkdocs-material->-r requirements.txt (line 23)) (0.5.7)\n",
            "Requirement already satisfied: pygments>=2.16 in /usr/local/lib/python3.12/dist-packages (from mkdocs-material->-r requirements.txt (line 23)) (2.19.2)\n",
            "Requirement already satisfied: pymdown-extensions>=10.2 in /usr/local/lib/python3.12/dist-packages (from mkdocs-material->-r requirements.txt (line 23)) (10.19)\n",
            "Requirement already satisfied: natsort>=8.1.0 in /usr/local/lib/python3.12/dist-packages (from mkdocs-awesome-pages-plugin->-r requirements.txt (line 24)) (8.4.0)\n",
            "Requirement already satisfied: wcmatch>=7 in /usr/local/lib/python3.12/dist-packages (from mkdocs-awesome-pages-plugin->-r requirements.txt (line 24)) (10.1)\n",
            "Requirement already satisfied: gitdb<5,>=4.0.1 in /usr/local/lib/python3.12/dist-packages (from gitpython!=3.1.29,>=1.0.0->wandb->-r requirements.txt (line 14)) (4.0.12)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2>=3.1->mkdocs-material->-r requirements.txt (line 23)) (3.0.3)\n",
            "Requirement already satisfied: ghp-import>=1.0 in /usr/local/lib/python3.12/dist-packages (from mkdocs>=1.6->mkdocs-material->-r requirements.txt (line 23)) (2.1.0)\n",
            "Requirement already satisfied: mergedeep>=1.3.4 in /usr/local/lib/python3.12/dist-packages (from mkdocs>=1.6->mkdocs-material->-r requirements.txt (line 23)) (1.3.4)\n",
            "Requirement already satisfied: mkdocs-get-deps>=0.2.0 in /usr/local/lib/python3.12/dist-packages (from mkdocs>=1.6->mkdocs-material->-r requirements.txt (line 23)) (0.2.0)\n",
            "Requirement already satisfied: pathspec>=0.11.1 in /usr/local/lib/python3.12/dist-packages (from mkdocs>=1.6->mkdocs-material->-r requirements.txt (line 23)) (0.12.1)\n",
            "Requirement already satisfied: pyyaml-env-tag>=0.1 in /usr/local/lib/python3.12/dist-packages (from mkdocs>=1.6->mkdocs-material->-r requirements.txt (line 23)) (1.1)\n",
            "Requirement already satisfied: watchdog>=2.0 in /usr/local/lib/python3.12/dist-packages (from mkdocs>=1.6->mkdocs-material->-r requirements.txt (line 23)) (6.0.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.12/dist-packages (from pydantic<3->wandb->-r requirements.txt (line 14)) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.41.4 in /usr/local/lib/python3.12/dist-packages (from pydantic<3->wandb->-r requirements.txt (line 14)) (2.41.4)\n",
            "Requirement already satisfied: typing-inspection>=0.4.2 in /usr/local/lib/python3.12/dist-packages (from pydantic<3->wandb->-r requirements.txt (line 14)) (0.4.2)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests[socks]->gdown->-r requirements.txt (line 4)) (3.4.4)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests[socks]->gdown->-r requirements.txt (line 4)) (3.11)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests[socks]->gdown->-r requirements.txt (line 4)) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests[socks]->gdown->-r requirements.txt (line 4)) (2025.11.12)\n",
            "Requirement already satisfied: bracex>=2.1.1 in /usr/local/lib/python3.12/dist-packages (from wcmatch>=7->mkdocs-awesome-pages-plugin->-r requirements.txt (line 24)) (2.6)\n",
            "Requirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.12/dist-packages (from beautifulsoup4->gdown->-r requirements.txt (line 4)) (2.8)\n",
            "Requirement already satisfied: PySocks!=1.5.7,>=1.5.6 in /usr/local/lib/python3.12/dist-packages (from requests[socks]->gdown->-r requirements.txt (line 4)) (1.7.1)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.12/dist-packages (from torch->deepspeed->-r requirements.txt (line 19)) (1.14.0)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.12/dist-packages (from torch->deepspeed->-r requirements.txt (line 19)) (3.6.1)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.12/dist-packages (from torch->deepspeed->-r requirements.txt (line 19)) (2025.3.0)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.1.105 in /usr/local/lib/python3.12/dist-packages (from torch->deepspeed->-r requirements.txt (line 19)) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.1.105 in /usr/local/lib/python3.12/dist-packages (from torch->deepspeed->-r requirements.txt (line 19)) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.1.105 in /usr/local/lib/python3.12/dist-packages (from torch->deepspeed->-r requirements.txt (line 19)) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /usr/local/lib/python3.12/dist-packages (from torch->deepspeed->-r requirements.txt (line 19)) (9.1.0.70)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.1.3.1 in /usr/local/lib/python3.12/dist-packages (from torch->deepspeed->-r requirements.txt (line 19)) (12.1.3.1)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.0.2.54 in /usr/local/lib/python3.12/dist-packages (from torch->deepspeed->-r requirements.txt (line 19)) (11.0.2.54)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.2.106 in /usr/local/lib/python3.12/dist-packages (from torch->deepspeed->-r requirements.txt (line 19)) (10.3.2.106)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.4.5.107 in /usr/local/lib/python3.12/dist-packages (from torch->deepspeed->-r requirements.txt (line 19)) (11.4.5.107)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.1.0.106 in /usr/local/lib/python3.12/dist-packages (from torch->deepspeed->-r requirements.txt (line 19)) (12.1.0.106)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.20.5 in /usr/local/lib/python3.12/dist-packages (from torch->deepspeed->-r requirements.txt (line 19)) (2.20.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.1.105 in /usr/local/lib/python3.12/dist-packages (from torch->deepspeed->-r requirements.txt (line 19)) (12.1.105)\n",
            "Requirement already satisfied: triton==3.0.0 in /usr/local/lib/python3.12/dist-packages (from torch->deepspeed->-r requirements.txt (line 19)) (3.0.0)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12 in /usr/local/lib/python3.12/dist-packages (from nvidia-cusolver-cu12==11.4.5.107->torch->deepspeed->-r requirements.txt (line 19)) (12.6.85)\n",
            "Requirement already satisfied: smmap<6,>=3.0.1 in /usr/local/lib/python3.12/dist-packages (from gitdb<5,>=4.0.1->gitpython!=3.1.29,>=1.0.0->wandb->-r requirements.txt (line 14)) (5.0.2)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from sympy->torch->deepspeed->-r requirements.txt (line 19)) (1.3.0)\n",
            "Using cached pyvista-0.46.4-py3-none-any.whl (2.4 MB)\n",
            "Using cached mkdocs_material-9.7.0-py3-none-any.whl (9.3 MB)\n",
            "Using cached mkdocs_awesome_pages_plugin-2.10.1-py3-none-any.whl (15 kB)\n",
            "Using cached mkdocs-1.6.1-py3-none-any.whl (3.9 MB)\n",
            "Using cached vtk-9.5.2-cp312-cp312-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (112.3 MB)\n",
            "Installing collected packages: vtk, mkdocs, pyvista, mkdocs-material, mkdocs-awesome-pages-plugin, deepspeed\n",
            "Successfully installed deepspeed-0.18.3 mkdocs-1.6.1 mkdocs-awesome-pages-plugin-2.10.1 mkdocs-material-9.7.0 pyvista-0.46.4 vtk-9.5.2\n",
            "Requirement already satisfied: multimethod in /usr/local/lib/python3.12/dist-packages (2.0.2)\n",
            "Requirement already satisfied: shortuuid in /usr/local/lib/python3.12/dist-packages (1.0.13)\n",
            "Requirement already satisfied: wandb in /usr/local/lib/python3.12/dist-packages (0.23.1)\n",
            "Requirement already satisfied: deepspeed in /usr/local/lib/python3.12/dist-packages (0.18.3)\n",
            "Requirement already satisfied: hjson in /usr/local/lib/python3.12/dist-packages (3.1.0)\n",
            "Collecting ml_collections\n",
            "  Downloading ml_collections-1.1.0-py3-none-any.whl.metadata (22 kB)\n",
            "Requirement already satisfied: click>=8.0.1 in /usr/local/lib/python3.12/dist-packages (from wandb) (8.3.1)\n",
            "Requirement already satisfied: gitpython!=3.1.29,>=1.0.0 in /usr/local/lib/python3.12/dist-packages (from wandb) (3.1.45)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.12/dist-packages (from wandb) (25.0)\n",
            "Requirement already satisfied: platformdirs in /usr/local/lib/python3.12/dist-packages (from wandb) (4.5.1)\n",
            "Requirement already satisfied: protobuf!=4.21.0,!=5.28.0,<7,>=3.19.0 in /usr/local/lib/python3.12/dist-packages (from wandb) (5.29.5)\n",
            "Requirement already satisfied: pydantic<3 in /usr/local/lib/python3.12/dist-packages (from wandb) (2.12.3)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.12/dist-packages (from wandb) (6.0.3)\n",
            "Requirement already satisfied: requests<3,>=2.0.0 in /usr/local/lib/python3.12/dist-packages (from wandb) (2.32.4)\n",
            "Requirement already satisfied: sentry-sdk>=2.0.0 in /usr/local/lib/python3.12/dist-packages (from wandb) (2.47.0)\n",
            "Requirement already satisfied: typing-extensions<5,>=4.8 in /usr/local/lib/python3.12/dist-packages (from wandb) (4.15.0)\n",
            "Requirement already satisfied: einops in /usr/local/lib/python3.12/dist-packages (from deepspeed) (0.8.1)\n",
            "Requirement already satisfied: msgpack in /usr/local/lib/python3.12/dist-packages (from deepspeed) (1.1.2)\n",
            "Requirement already satisfied: ninja in /usr/local/lib/python3.12/dist-packages (from deepspeed) (1.13.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.12/dist-packages (from deepspeed) (2.0.2)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.12/dist-packages (from deepspeed) (5.9.5)\n",
            "Requirement already satisfied: py-cpuinfo in /usr/local/lib/python3.12/dist-packages (from deepspeed) (9.0.0)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.12/dist-packages (from deepspeed) (2.4.0+cu121)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.12/dist-packages (from deepspeed) (4.67.1)\n",
            "Requirement already satisfied: nvidia-ml-py in /usr/local/lib/python3.12/dist-packages (from deepspeed) (13.590.44)\n",
            "Requirement already satisfied: absl-py in /usr/local/lib/python3.12/dist-packages (from ml_collections) (1.4.0)\n",
            "Requirement already satisfied: gitdb<5,>=4.0.1 in /usr/local/lib/python3.12/dist-packages (from gitpython!=3.1.29,>=1.0.0->wandb) (4.0.12)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.12/dist-packages (from pydantic<3->wandb) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.41.4 in /usr/local/lib/python3.12/dist-packages (from pydantic<3->wandb) (2.41.4)\n",
            "Requirement already satisfied: typing-inspection>=0.4.2 in /usr/local/lib/python3.12/dist-packages (from pydantic<3->wandb) (0.4.2)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests<3,>=2.0.0->wandb) (3.4.4)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests<3,>=2.0.0->wandb) (3.11)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests<3,>=2.0.0->wandb) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests<3,>=2.0.0->wandb) (2025.11.12)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from torch->deepspeed) (3.20.0)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.12/dist-packages (from torch->deepspeed) (1.14.0)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.12/dist-packages (from torch->deepspeed) (3.6.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from torch->deepspeed) (3.1.6)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.12/dist-packages (from torch->deepspeed) (2025.3.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (from torch->deepspeed) (75.2.0)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.1.105 in /usr/local/lib/python3.12/dist-packages (from torch->deepspeed) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.1.105 in /usr/local/lib/python3.12/dist-packages (from torch->deepspeed) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.1.105 in /usr/local/lib/python3.12/dist-packages (from torch->deepspeed) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /usr/local/lib/python3.12/dist-packages (from torch->deepspeed) (9.1.0.70)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.1.3.1 in /usr/local/lib/python3.12/dist-packages (from torch->deepspeed) (12.1.3.1)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.0.2.54 in /usr/local/lib/python3.12/dist-packages (from torch->deepspeed) (11.0.2.54)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.2.106 in /usr/local/lib/python3.12/dist-packages (from torch->deepspeed) (10.3.2.106)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.4.5.107 in /usr/local/lib/python3.12/dist-packages (from torch->deepspeed) (11.4.5.107)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.1.0.106 in /usr/local/lib/python3.12/dist-packages (from torch->deepspeed) (12.1.0.106)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.20.5 in /usr/local/lib/python3.12/dist-packages (from torch->deepspeed) (2.20.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.1.105 in /usr/local/lib/python3.12/dist-packages (from torch->deepspeed) (12.1.105)\n",
            "Requirement already satisfied: triton==3.0.0 in /usr/local/lib/python3.12/dist-packages (from torch->deepspeed) (3.0.0)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12 in /usr/local/lib/python3.12/dist-packages (from nvidia-cusolver-cu12==11.4.5.107->torch->deepspeed) (12.6.85)\n",
            "Requirement already satisfied: smmap<6,>=3.0.1 in /usr/local/lib/python3.12/dist-packages (from gitdb<5,>=4.0.1->gitpython!=3.1.29,>=1.0.0->wandb) (5.0.2)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2->torch->deepspeed) (3.0.3)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from sympy->torch->deepspeed) (1.3.0)\n",
            "Downloading ml_collections-1.1.0-py3-none-any.whl (76 kB)\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m76.7/76.7 kB\u001b[0m \u001b[31m2.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: ml_collections\n",
            "Successfully installed ml_collections-1.1.0\n",
            "‚úÖ Dependencies Installed. (Skipped setup.py)\n"
          ]
        }
      ],
      "source": [
        "# Install Dependencies\n",
        "print(\"Installing Dependencies...\")\n",
        "%cd /content/PointNeXt\n",
        "\n",
        "# A. Install PyTorch 2.4.0 (Compatible)\n",
        "!pip install torch==2.4.0 torchvision==0.19.0 torchaudio==2.4.0 --index-url https://download.pytorch.org/whl/cu121\n",
        "\n",
        "# B. Install torch-scatter/sparse\n",
        "!pip install torch-scatter torch-sparse -f https://data.pyg.org/whl/torch-2.4.0+cu121.html\n",
        "\n",
        "# C. Fix requirements.txt\n",
        "!sed -i 's/==.*//g' requirements.txt\n",
        "\n",
        "# D. Install requirements\n",
        "!pip install -r requirements.txt\n",
        "\n",
        "# E. SKIP 'pip install -e .' (Because setup.py is missing) in training\n",
        "!pip install multimethod shortuuid wandb deepspeed hjson ml_collections\n",
        "# !pip install -e .\n",
        "\n",
        "print(\"\\u2705 Dependencies Installed. (Skipped setup.py)\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aRHp1wmoayhD"
      },
      "source": [
        "# Compile C++ Kernels"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RaBjABdpaxmU",
        "outputId": "ad37e9f7-73ef-4dcc-9446-37ac9bd68351"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Searching for C++ kernel setup.py...\n",
            "‚úÖ Found setup.py at: /content/PointNeXt/openpoints/cpp/pointnet2_batch\n",
            "Compiling kernels in /content/PointNeXt/openpoints/cpp/pointnet2_batch...\n",
            "Compilation Successful!\n",
            "‚úÖ Found setup.py at: /content/PointNeXt/openpoints/cpp/emd\n",
            "Compiling kernels in /content/PointNeXt/openpoints/cpp/emd...\n",
            "Compilation Successful!\n",
            "‚úÖ Found setup.py at: /content/PointNeXt/openpoints/cpp/pointops\n",
            "Compiling kernels in /content/PointNeXt/openpoints/cpp/pointops...\n",
            "Compilation Successful!\n",
            "‚úÖ Found setup.py at: /content/PointNeXt/openpoints/cpp/subsampling\n",
            "Compiling kernels in /content/PointNeXt/openpoints/cpp/subsampling...\n",
            "‚ùå Compilation Failed: Command '['/usr/bin/python3', 'setup.py', 'install']' returned non-zero exit status 1.\n",
            "‚úÖ Found setup.py at: /content/PointNeXt/openpoints/cpp/chamfer_dist\n",
            "Compiling kernels in /content/PointNeXt/openpoints/cpp/chamfer_dist...\n",
            "Compilation Successful!\n"
          ]
        }
      ],
      "source": [
        "# 1. Search for the missing setup.py\n",
        "print(\"Searching for C++ kernel setup.py...\")\n",
        "target_dir = \"/content/PointNeXt/openpoints/cpp\"\n",
        "found_setup = False\n",
        "\n",
        "for root, dirs, files in os.walk(target_dir):\n",
        "    if \"setup.py\" in files:\n",
        "        print(f\"\\u2705 Found setup.py at: {root}\")\n",
        "        found_setup = True\n",
        "\n",
        "        # 2. Force Compile\n",
        "        print(f\"Compiling kernels in {root}...\")\n",
        "        try:\n",
        "            subprocess.check_call([sys.executable, \"setup.py\", \"install\"], cwd=root)\n",
        "            print(\"Compilation Successful!\")\n",
        "        except subprocess.CalledProcessError as e:\n",
        "            print(f\"\\u274c Compilation Failed: {e}\")\n",
        "\n",
        "if not found_setup:\n",
        "    print(\"\\u274c Critical Error: Could not find setup.py anywhere in openpoints/cpp!\")\n",
        "    print(\"Did the 'git submodule update' step finish successfully?\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XgxI8XtcbJuD"
      },
      "source": [
        "# Re-Create Config File"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "UDkJwEjCbInD",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "bc7418b9-1815-4ad5-a06a-a8ecc16f8c33"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚úÖ Config Updated.\n"
          ]
        }
      ],
      "source": [
        "config_path = \"/content/PointNeXt/cfgs/shapenetpart/custom_fallen_trees.yaml\"\n",
        "\n",
        "config_content = \"\"\"\n",
        "num_classes: 4\n",
        "shape_classes: 2\n",
        "epochs: 100\n",
        "\n",
        "# Explicitly define feature keys to match the custom data loader\n",
        "feature_keys: 'pos,x'\n",
        "\n",
        "model:\n",
        "  NAME: BasePartSeg\n",
        "  encoder_args:\n",
        "    NAME: PointNextEncoder\n",
        "    blocks: [1, 1, 1, 1, 1]\n",
        "    strides: [1, 2, 2, 2, 2]\n",
        "    width: 32\n",
        "    in_channels: 7\n",
        "    sa_layers: 3\n",
        "    sa_use_res: True\n",
        "    radius: 0.1\n",
        "    radius_scaling: 2.5\n",
        "    nsample: 32\n",
        "    expansion: 4\n",
        "    aggr_args:\n",
        "      feature_type: 'dp_fj'\n",
        "    reduction: 'max'\n",
        "    group_args:\n",
        "      NAME: 'ballquery'\n",
        "      normalize_dp: True\n",
        "    conv_args:\n",
        "      order: conv-norm-act\n",
        "    act_args:\n",
        "      act: 'relu'\n",
        "    norm_args:\n",
        "      norm: 'bn'\n",
        "  decoder_args:\n",
        "    NAME: PointNextPartDecoder\n",
        "    cls_map: curvenet\n",
        "  cls_args:\n",
        "    NAME: SegHead\n",
        "    global_feat: max,avg\n",
        "    num_classes: 4\n",
        "    shape_classes: 2\n",
        "    in_channels: null\n",
        "    norm_args:\n",
        "      norm: 'bn'\n",
        "\n",
        "dataset:\n",
        "  common:\n",
        "    NAME: FallenTreePart\n",
        "    data_root: /content/processed_data\n",
        "    use_normal: False\n",
        "    use_xyz: True\n",
        "    num_points: 2048\n",
        "  train:\n",
        "    split: train\n",
        "  val:\n",
        "    split: val\n",
        "\n",
        "batch_size: 16\n",
        "dataloader:\n",
        "  num_workers: 4\n",
        "\n",
        "lr: 0.001\n",
        "min_lr: null\n",
        "optimizer:\n",
        "  NAME: adamw\n",
        "  weight_decay: 1.0e-4\n",
        "\n",
        "criterion_args:\n",
        "  NAME: Poly1FocalLoss\n",
        "\n",
        "# scheduler\n",
        "epochs: 100\n",
        "sched: multistep\n",
        "decay_epochs: [70, 90]\n",
        "decay_rate: 0.1\n",
        "warmup_epochs: 0\n",
        "\n",
        "datatransforms:\n",
        "  train: [PointsToTensor, PointCloudScaling, PointCloudCenterAndNormalize, PointCloudJitter, ChromaticDropGPU]\n",
        "  val: [PointsToTensor, PointCloudCenterAndNormalize]\n",
        "  kwargs:\n",
        "    jitter_sigma: 0.001\n",
        "    jitter_clip: 0.005\n",
        "    scale: [0.8, 1.2]\n",
        "    gravity_dim: 1\n",
        "    angle: [0, 1.0, 0]\n",
        "\n",
        "log_dir: /content/PointNeXt/log/shapenetpart/custom_trees\n",
        "\"\"\"\n",
        "\n",
        "with open(config_path, 'w') as f:\n",
        "    f.write(config_content)\n",
        "print(f\"\\u2705 Config Updated.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pYlJX3m0iKi8"
      },
      "source": [
        "# Re-Create Custom Dataset File (fallentree.py)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "V1mV_JhSirfk",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "cbeda5a8-2ce0-41cc-e61a-99c95be9b692"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "üìÇ Created folder: /content/PointNeXt/openpoints/dataset/fallentree\n",
            "‚úÖ Created: /content/PointNeXt/openpoints/dataset/fallentree/__init__.py\n"
          ]
        }
      ],
      "source": [
        "# Create the directory\n",
        "new_dataset_dir = \"/content/PointNeXt/openpoints/dataset/fallentree\"\n",
        "os.makedirs(new_dataset_dir, exist_ok=True)\n",
        "print(f\"\\U0001F4C2 Created folder: {new_dataset_dir}\")\n",
        "\n",
        "# Create the '__init__.py' to make it a package\n",
        "init_path = os.path.join(new_dataset_dir, \"__init__.py\")\n",
        "with open(init_path, 'w') as f:\n",
        "    f.write(\"from .fallentree import FallenTreePart\\n\")\n",
        "print(f\"\\u2705 Created: {init_path}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "43Aoz09hizVM",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "208e43fd-6b78-4207-bbb8-2de732350e4c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚úÖ Updated: /content/PointNeXt/openpoints/dataset/fallentree/fallentree.py.\n"
          ]
        }
      ],
      "source": [
        "# Define Path\n",
        "code_path = \"/content/PointNeXt/openpoints/dataset/fallentree/fallentree.py\"\n",
        "\n",
        "# Define Code\n",
        "dataset_code = \"\"\"\n",
        "import os\n",
        "import glob\n",
        "import json\n",
        "import logging\n",
        "import numpy as np\n",
        "import torch\n",
        "from torch.utils.data import Dataset\n",
        "from ..build import DATASETS\n",
        "\n",
        "\n",
        "# NEW QUOTA-BASED SAMPLING FUNCTION\n",
        "def quota_sample(xyz, part_labels, npoint):\n",
        "    \\\"\\\"\\\"\n",
        "    For TRAINING: Forces small classes (Roots/Pits) to have a minimum number of points.\n",
        "    This helps the model see them, even if they are tiny in the real tree.\n",
        "    \\\"\\\"\\\"\n",
        "\n",
        "    total_points = len(xyz)\n",
        "    if total_points <= npoint:\n",
        "        return np.random.choice(total_points, npoint, replace=True)\n",
        "\n",
        "    unique_parts = np.unique(part_labels)\n",
        "\n",
        "    # THE STRATEGY\n",
        "    # We want to reserve specific slots for the hard classes.\n",
        "    # Total 2048 points.\n",
        "    # If Root exists, force 256 points.\n",
        "    # If Pit exists, force 256 points.\n",
        "    # The rest from all points.\n",
        "\n",
        "    min_points_map = {\n",
        "        0: 0,    # Standing Trunk: No minimum forced\n",
        "        1: 0,    # Fallen Trunk: No minimum forced\n",
        "        2: 256,  # Root: 256 points minimum\n",
        "        3: 256   # Pit: 256 points minimum\n",
        "    }\n",
        "\n",
        "    indices_collected = []\n",
        "\n",
        "    # 1. Collect the hard classes first\n",
        "    for part in unique_parts:\n",
        "        if part in min_points_map and min_points_map[part] > 0:\n",
        "            part_indices = np.where(part_labels == part)[0]\n",
        "\n",
        "            # If the part is smaller than the quota, take it all with replacement (duplicate points)\n",
        "            req_count = min_points_map[part]\n",
        "\n",
        "            if len(part_indices) >= req_count:\n",
        "                chosen = np.random.choice(part_indices, req_count, replace=False)\n",
        "            else:\n",
        "                chosen = np.random.choice(part_indices, req_count, replace=True)\n",
        "\n",
        "            indices_collected.extend(chosen)\n",
        "\n",
        "    # Fill the rest of the 2048 points with points left\n",
        "    current_count = len(indices_collected)\n",
        "    remaining_slots = npoint - current_count\n",
        "\n",
        "    if remaining_slots > 0:\n",
        "        all_indices = np.arange(total_points)\n",
        "        fill_indices = np.random.choice(all_indices, remaining_slots, replace=False)\n",
        "        indices_collected.extend(fill_indices)\n",
        "\n",
        "    final_indices = np.array(indices_collected)\n",
        "\n",
        "    # Shuffle so the roots or pits aren't all at the beginning\n",
        "    np.random.shuffle(final_indices)\n",
        "\n",
        "    # If by some edge case we have too many, trim it\n",
        "    if len(final_indices) > npoint:\n",
        "        final_indices = final_indices[:npoint]\n",
        "\n",
        "    return final_indices\n",
        "\n",
        "\n",
        "@DATASETS.register_module()\n",
        "class FallenTreePart(Dataset):\n",
        "    classes = ['standing', 'fallen']\n",
        "    num_classes = 4\n",
        "    shape_classes = 2\n",
        "\n",
        "    # --- FIX: Add dummy key -1 for part_seg_refinement compatibility ---\n",
        "    # -1 points to all parts (0,1,2,3) so the code can find the max index\n",
        "    cls2parts = {\n",
        "        0: [0],\n",
        "        1: [1, 2, 3],\n",
        "        -1: [0, 1, 2, 3]\n",
        "    }\n",
        "\n",
        "    part_start = [0, 1]\n",
        "\n",
        "    # Pre-compute embedding\n",
        "    cls2partembed = torch.zeros(shape_classes, num_classes)\n",
        "    for i in [0, 1]: # Iterate only real classes\n",
        "        idx = cls2parts[i]\n",
        "        cls2partembed[i].scatter_(0, torch.LongTensor(idx), 1)\n",
        "\n",
        "    def __init__(self,\n",
        "                 data_root,\n",
        "                 split=None,\n",
        "                 num_points=2048,\n",
        "                 use_normal=False,\n",
        "                 use_xyz=True,\n",
        "                 **kwargs):\n",
        "        self.root = data_root\n",
        "        self.npoints = num_points\n",
        "        self.split = split\n",
        "\n",
        "        # if split == 'val': split_name = 'test'\n",
        "        # else: split_name = split\n",
        "\n",
        "        split_name = split\n",
        "\n",
        "        split_file = os.path.join(self.root, 'train_test_split', f'shuffled_{split_name}_file_list.json')\n",
        "        if not os.path.exists(split_file):\n",
        "             raise FileNotFoundError(f\"Split list not found: {split_file}\")\n",
        "\n",
        "        logging.info(f\"Loading {split} split from: {split_file}\")\n",
        "        with open(split_file, 'r') as f:\n",
        "            raw_list = json.load(f)\n",
        "\n",
        "        self.file_list = []\n",
        "        for item in raw_list:\n",
        "            clean_item = item.replace(\\\"\\\\\\\\\\\", \\\"/\\\")\n",
        "            fname = os.path.basename(clean_item)\n",
        "            candidates = [\n",
        "                clean_item,\n",
        "                os.path.join(self.root, clean_item),\n",
        "                os.path.join(self.root, '0', fname),\n",
        "                os.path.join(self.root, '1', fname)\n",
        "            ]\n",
        "            found = False\n",
        "            for path in candidates:\n",
        "                if os.path.exists(path):\n",
        "                    self.file_list.append(path)\n",
        "                    found = True\n",
        "                    break\n",
        "\n",
        "        logging.info(f\"Found {len(self.file_list)} valid files for {split} split.\")\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        file_path = self.file_list[index]\n",
        "        cls_idx = 0 if '/0/' in file_path.replace('\\\\\\\\', '/') else 1\n",
        "\n",
        "        data = np.load(file_path).astype(np.float32)\n",
        "        xyz = data[:, 0:3]\n",
        "        features = data[:, 3:7]\n",
        "        part_label = data[:, 7].astype(np.int64)\n",
        "        cls_label = np.array([cls_idx]).astype(np.int64)\n",
        "\n",
        "        if self.split == 'train':\n",
        "            choice = quota_sample(xyz, part_label, self.npoints)\n",
        "        else:\n",
        "            if len(xyz) >= self.npoints:\n",
        "                choice = np.random.choice(len(xyz), self.npoints, replace=False)\n",
        "            else:\n",
        "                choice = np.random.choice(len(xyz), self.npoints, replace=True)\n",
        "\n",
        "        xyz = xyz[choice]\n",
        "        features = features[choice]\n",
        "        part_label = part_label[choice]\n",
        "\n",
        "        return {'pos': xyz, 'x': features, 'y': part_label, 'cls': cls_label}\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.file_list)\n",
        "\"\"\"\n",
        "\n",
        "with open(code_path, 'w') as f:\n",
        "    f.write(dataset_code)\n",
        "print(f\"\\u2705 Updated: {code_path}.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "FNv7X0N4iLTE",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "de9d6b0c-ad5c-408f-9548-20d6671fc320"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Registering new dataset in main __init__.py...\n",
            "‚úÖ Dataset Class Restored.\n"
          ]
        }
      ],
      "source": [
        "# Register the new folder in the Main Library\n",
        "# We need to add \"from .fallentree import FallenTreePart\" to openpoints/dataset/__init__.py\n",
        "\n",
        "main_init = \"/content/PointNeXt/openpoints/dataset/__init__.py\"\n",
        "with open(main_init, 'r') as f:\n",
        "    content = f.read()\n",
        "\n",
        "if \"fallentree\" not in content:\n",
        "    print(\"Registering new dataset in main __init__.py...\")\n",
        "    with open(main_init, 'a') as f:\n",
        "        f.write(\"\\nfrom .fallentree import FallenTreePart\\n\")\n",
        "    print(\"\\u2705 Dataset Class Restored.\")\n",
        "else:\n",
        "    print(\"\\u2705 Already registered.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2ndBZy0wlWv7"
      },
      "source": [
        "# Inference (Generates Output)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "7IPeajVn0EMs",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3a740363-d04d-41b2-8630-5b3227471364"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Adding to path: /content/PointNeXt/openpoints/cpp/pointnet2_batch/build/lib.linux-x86_64-cpython-312\n",
            "‚úÖ SUCCESS: pointnet2_batch_cuda is importable.\n"
          ]
        }
      ],
      "source": [
        "# Force Load C++ Extension\n",
        "import sys\n",
        "import os\n",
        "\n",
        "# Add the specific build folder to Python path\n",
        "cpp_root = \"/content/PointNeXt/openpoints/cpp/pointnet2_batch\"\n",
        "build_dirs = [os.path.join(cpp_root, d) for d in os.listdir(cpp_root) if d.startswith('build')]\n",
        "\n",
        "if build_dirs:\n",
        "    # Find the lib folder inside build\n",
        "    lib_dir = os.path.join(build_dirs[0], [d for d in os.listdir(build_dirs[0]) if d.startswith('lib')][0])\n",
        "    if os.path.exists(lib_dir):\n",
        "        print(f\"Adding to path: {lib_dir}\")\n",
        "        sys.path.insert(0, lib_dir)\n",
        "    else:\n",
        "        print(\"Could not find lib folder in build.\")\n",
        "else:\n",
        "    print(\"No build folder found. Did compilation actually finish?\")\n",
        "\n",
        "# Try importing manually to check\n",
        "try:\n",
        "    import pointnet2_batch_cuda\n",
        "    print(\"\\u2705 SUCCESS: pointnet2_batch_cuda is importable.\")\n",
        "except ImportError as e:\n",
        "    print(f\"\\u274c STILL FAILING: {e}\")\n",
        "    print(\"Trying fallback: Re-running setup.py install in verbose mode...\")\n",
        "    # Last ditch effort: Run install again with verbose output to see where it goes\n",
        "    %cd /content/PointNeXt/openpoints/cpp/pointnet2_batch\n",
        "    !python setup.py install"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "q-ANP9sz0g2k",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "86157472-55b7-49a0-e591-9627cd5050d4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "üîß Patching Python Path for C++ Modules...\n",
            "   ‚úÖ  Already in path: pointnet2_batch\n",
            "   ‚úÖ Added to path: chamfer_dist -> /content/PointNeXt/openpoints/cpp/chamfer_dist/build/lib.linux-x86_64-cpython-312\n",
            "   ‚úÖ Added to path: emd -> /content/PointNeXt/openpoints/cpp/emd/build/lib.linux-x86_64-cpython-312\n",
            "   ‚úÖ Added to path: pointops -> /content/PointNeXt/openpoints/cpp/pointops/build/lib.linux-x86_64-cpython-312\n",
            "\n",
            "üîÑ Verifying Imports...\n",
            "   ‚úÖ pointnet2_batch_cuda: OK\n",
            "   ‚úÖ chamfer: OK\n",
            "   ‚ùå emd: FAILED (No module named 'emd')\n",
            "\n",
            "üöÄ Ready for Inference.\n"
          ]
        }
      ],
      "source": [
        "# Force Load ALL C++ Extensions (Universal Fix)\n",
        "import sys\n",
        "import os\n",
        "import importlib\n",
        "\n",
        "cpp_base = \"/content/PointNeXt/openpoints/cpp\"\n",
        "modules_to_fix = [\n",
        "    \"pointnet2_batch\",\n",
        "    \"chamfer_dist\",\n",
        "    \"emd\",\n",
        "    \"pointops\"\n",
        "]\n",
        "\n",
        "print(\"\\U0001F527 Patching Python Path for C++ Modules...\")\n",
        "\n",
        "for module in modules_to_fix:\n",
        "    mod_path = os.path.join(cpp_base, module)\n",
        "\n",
        "    # Find the build/lib folder\n",
        "    build_dirs = [os.path.join(mod_path, d) for d in os.listdir(mod_path) if d.startswith('build')]\n",
        "\n",
        "    if build_dirs:\n",
        "        # Find the lib folder inside build\n",
        "        lib_content = [d for d in os.listdir(build_dirs[0]) if d.startswith('lib')]\n",
        "        if lib_content:\n",
        "            lib_dir = os.path.join(build_dirs[0], lib_content[0])\n",
        "\n",
        "            if os.path.exists(lib_dir):\n",
        "                if lib_dir not in sys.path:\n",
        "                    sys.path.insert(0, lib_dir)\n",
        "                    print(f\"   \\u2705 Added to path: {module} -> {lib_dir}\")\n",
        "                else:\n",
        "                    print(f\"   \\u2705  Already in path: {module}\")\n",
        "            else:\n",
        "                print(f\"   \\u26A0\\uFE0F Lib folder exists but path is wrong: {lib_dir}\")\n",
        "        else:\n",
        "             print(f\"   \\u26A0\\uFE0F No 'lib' folder found in {build_dirs[0]}\")\n",
        "    else:\n",
        "        print(f\"   \\u274c No 'build' folder found for {module}. Did compilation fail?\")\n",
        "\n",
        "print(\"\\n\\U0001F504 Verifying Imports...\")\n",
        "try:\n",
        "    import pointnet2_batch_cuda\n",
        "    print(\"   \\u2705 pointnet2_batch_cuda: OK\")\n",
        "except ImportError as e: print(f\"   \\u274c pointnet2_batch_cuda: FAILED ({e})\")\n",
        "\n",
        "try:\n",
        "    import chamfer\n",
        "    print(\"   \\u2705 chamfer: OK\")\n",
        "except ImportError as e: print(f\"   \\u274c chamfer: FAILED ({e})\")\n",
        "\n",
        "try:\n",
        "    import emd\n",
        "    print(\"   \\u2705 emd: OK\")\n",
        "except ImportError as e: print(f\"   \\u274c emd: FAILED ({e})\")\n",
        "\n",
        "print(\"\\n\\U0001F680 Ready for Inference.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "R4pXeu144JSr",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a00127a8-fba8-4adb-a1b8-f7db7af400b4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "üìÇ Checking contents of: /content/PointNeXt/openpoints/cpp/emd/build/lib.linux-x86_64-cpython-312\n",
            "   Found files: ['emd_cuda.cpython-312-x86_64-linux-gnu.so']\n",
            "   Trying to import: 'emd_cuda'\n",
            "   ‚úÖ SUCCESS! Imported 'emd_cuda'\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "import sys\n",
        "\n",
        "# Check the filename inside the build folder\n",
        "emd_build_dir = \"/content/PointNeXt/openpoints/cpp/emd/build/lib.linux-x86_64-cpython-312\"\n",
        "\n",
        "print(f\"\\U0001F4C2 Checking contents of: {emd_build_dir}\")\n",
        "if os.path.exists(emd_build_dir):\n",
        "    files = os.listdir(emd_build_dir)\n",
        "    print(f\"   Found files: {files}\")\n",
        "\n",
        "    # Add to path if not there\n",
        "    if emd_build_dir not in sys.path:\n",
        "        sys.path.insert(0, emd_build_dir)\n",
        "\n",
        "    # Try importing based on filename\n",
        "    for f in files:\n",
        "        if f.endswith(\".so\"):\n",
        "            module_name = f.split(\".\")[0] # e.g., \"emd_cuda\" from \"emd_cuda.cpython...\"\n",
        "            print(f\"   Trying to import: '{module_name}'\")\n",
        "            try:\n",
        "                __import__(module_name)\n",
        "                print(f\"   \\u2705 SUCCESS! Imported '{module_name}'\")\n",
        "            except ImportError as e:\n",
        "                print(f\"   \\u274c Failed to import '{module_name}': {e}\")\n",
        "else:\n",
        "    print(\"\\u274c Directory not found.\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def pc_normalize(pc):\n",
        "    centroid = np.mean(pc, axis=0)\n",
        "    pc = pc - centroid\n",
        "    m = np.max(np.sqrt(np.sum(pc ** 2, axis=1)))\n",
        "    pc = pc / m\n",
        "    return pc\n",
        "\n",
        "def rgb_normalize(points):\n",
        "    min_vals = points.min(axis=0)      # shape: (,3)\n",
        "    max_vals = points.max(axis=0)      # shape: (,3)\n",
        "\n",
        "    normalized = (points - min_vals) / (max_vals - min_vals + 1e-8)  # avoid div by zero\n",
        "\n",
        "    return normalized\n",
        "\n",
        "def intensity_normalize(points):\n",
        "    min_vals = points.min()      # shape: (,3)\n",
        "    max_vals = points.max()      # shape: (,3)\n",
        "\n",
        "    normalized = (points - min_vals) / (max_vals - min_vals + 1e-8)  # avoid div by zero\n",
        "\n",
        "    return normalized"
      ],
      "metadata": {
        "id": "anSTj-d3pJCo"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import numpy as np\n",
        "import os\n",
        "import json\n",
        "from tqdm import tqdm\n",
        "from openpoints.utils import EasyConfig\n",
        "from openpoints.models import build_model_from_cfg\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.metrics import confusion_matrix\n",
        "\n",
        "# DEFINE FPS FUNCTION\n",
        "def farthest_point_sample(xyz, npoint):\n",
        "    N, C = xyz.shape\n",
        "    centroids = np.zeros((npoint,), dtype=np.int64)\n",
        "    distance = np.ones((N,)) * 1e10\n",
        "    farthest = np.random.randint(0, N)\n",
        "\n",
        "    for i in range(npoint):\n",
        "        centroids[i] = farthest\n",
        "        centroid = xyz[farthest, :]\n",
        "        dist = np.sum((xyz - centroid) ** 2, -1)\n",
        "        mask = dist < distance\n",
        "        distance[mask] = dist[mask]\n",
        "        farthest = np.argmax(distance, -1)\n",
        "    return centroids\n",
        "\n",
        "\n",
        "TEST_LIST = \"/content/processed_data/train_test_split/shuffled_test_file_list.json\"\n",
        "OUTPUT_DIR = \"/content/inference_results_12ch\"\n",
        "RAW_DIR = \"/content/raw_data\"\n",
        "NUM_POINTS = 2048\n",
        "NUM_CLASSES = 4\n",
        "\n",
        "if not os.path.exists(OUTPUT_DIR):\n",
        "    os.makedirs(OUTPUT_DIR)\n",
        "\n",
        "print(f\"\\n\\U0001F680 Starting Inference using model: {os.path.basename(model_path)}\")\n",
        "\n",
        "# 1. Load Config & Model\n",
        "cfg = EasyConfig()\n",
        "cfg.load(config_path, recursive=True)\n",
        "model = build_model_from_cfg(cfg.model).cuda()\n",
        "checkpoint = torch.load(model_path)\n",
        "model.load_state_dict(checkpoint['model'])\n",
        "model.eval()\n",
        "print(\"\\u2705 Model loaded.\")\n",
        "\n",
        "# 2. Load Test List\n",
        "with open(TEST_LIST, 'r') as f:\n",
        "    test_files = json.load(f)\n",
        "\n",
        "# 3. Metrics Init\n",
        "total_correct = 0\n",
        "total_points = 0\n",
        "instance_ious = []\n",
        "global_confusion = np.zeros((NUM_CLASSES, NUM_CLASSES))\n",
        "\n",
        "# 4. Inference Loop\n",
        "for entry in tqdm(test_files):\n",
        "    filename = os.path.basename(entry.replace(\"\\\\\", \"/\"))\n",
        "\n",
        "    # Find Raw File\n",
        "    found_path = None\n",
        "    for root, dirs, files in os.walk(RAW_DIR):\n",
        "        if filename in files:\n",
        "            found_path = os.path.join(root, filename)\n",
        "            break\n",
        "\n",
        "    if found_path:\n",
        "        # A. Load Raw Data\n",
        "        raw_data_11c = np.load(found_path).astype(np.float32)\n",
        "        N_points = raw_data_11c.shape[0]\n",
        "\n",
        "        # Get GT for metrics\n",
        "        gt_labels = raw_data_11c[:, 10].astype(np.int64)\n",
        "\n",
        "        # CLASS DETECTION\n",
        "        # Use filename, not path (safer)\n",
        "        if \"standing\" in filename:\n",
        "            cls_idx = 0\n",
        "        else:\n",
        "            cls_idx = 1\n",
        "\n",
        "        # B. NORMALIZATION\n",
        "        # Extract\n",
        "        xyz_raw = raw_data_11c[:, 0:3]\n",
        "        rgb_raw = raw_data_11c[:, 3:6]\n",
        "        int_raw = raw_data_11c[:, 6].reshape(-1, 1)\n",
        "\n",
        "        # Normalize\n",
        "        xyz_norm = pc_normalize(xyz_raw)\n",
        "        rgb_norm = rgb_normalize(rgb_raw)\n",
        "        int_norm = intensity_normalize(int_raw)\n",
        "\n",
        "        # Combine Features (RGB + I)\n",
        "        features_norm = np.hstack((rgb_norm, int_norm))\n",
        "\n",
        "        # C. Resample\n",
        "        if N_points >= NUM_POINTS:\n",
        "            # choice = np.random.choice(N_points, NUM_POINTS, replace=False)\n",
        "            choice = farthest_point_sample(xyz_norm, NUM_POINTS)\n",
        "        else:\n",
        "            # Upsample with random repeat if too small\n",
        "            choice = np.random.choice(N_points, NUM_POINTS, replace=True)\n",
        "\n",
        "        xyz_sampled = xyz_norm[choice]\n",
        "        features_sampled = features_norm[choice]\n",
        "\n",
        "        # D. Predict\n",
        "        pos_tensor = torch.from_numpy(xyz_sampled).unsqueeze(0).cuda()\n",
        "        # X expects (B, C, N) -> Transpose\n",
        "        model_input = np.hstack((xyz_sampled, features_sampled)) # XYZ+RGB+I (7 channels)\n",
        "        x_tensor = torch.from_numpy(model_input).unsqueeze(0).cuda().transpose(1, 2).contiguous()\n",
        "        cls_tensor = torch.tensor([[cls_idx]], dtype=torch.long).cuda()\n",
        "\n",
        "        with torch.no_grad():\n",
        "            data_batch = {'pos': pos_tensor, 'x': x_tensor, 'cls': cls_tensor}\n",
        "            logits = model(data_batch)\n",
        "            preds_sampled = torch.argmax(logits, dim=1).squeeze(0).cpu().numpy()\n",
        "\n",
        "        # E. Interpolate\n",
        "        knn = KNeighborsClassifier(n_neighbors=1)\n",
        "        knn.fit(xyz_sampled, preds_sampled)\n",
        "        preds_full = knn.predict(xyz_norm)\n",
        "\n",
        "        # --- METRICS ---\n",
        "        correct = np.sum(preds_full == gt_labels)\n",
        "        total_correct += correct\n",
        "        total_points += N_points\n",
        "\n",
        "        # Mask invalid classes based on object type (Standing vs Fallen)\n",
        "        # Standing (0) should ignore roots/pits (2, 3)\n",
        "        valid_parts = [0] if cls_idx == 0 else [1, 2, 3]\n",
        "\n",
        "        # Instance IoU\n",
        "        union = np.histogram(preds_full, bins=NUM_CLASSES, range=(0, NUM_CLASSES))[0] + \\\n",
        "                np.histogram(gt_labels, bins=NUM_CLASSES, range=(0, NUM_CLASSES))[0]\n",
        "        intersection = np.histogram(preds_full[preds_full == gt_labels], bins=NUM_CLASSES, range=(0, NUM_CLASSES))[0]\n",
        "        union = union - intersection\n",
        "        iou = intersection / (union + 1e-6)\n",
        "\n",
        "        # Only average IoU for parts that SHOULD exist\n",
        "        instance_ious.append(np.mean(iou[valid_parts]))\n",
        "\n",
        "        # Global Confusion\n",
        "        mask = (gt_labels >= 0) & (gt_labels < NUM_CLASSES)\n",
        "        global_confusion += confusion_matrix(gt_labels[mask], preds_full[mask], labels=np.arange(NUM_CLASSES))\n",
        "\n",
        "        # F. Save Result (N, 12)\n",
        "        preds_col = preds_full.reshape(-1, 1)\n",
        "        final_data = np.hstack((raw_data_11c, preds_col))\n",
        "\n",
        "        save_name = filename.replace(\".npy\", \"_ALL_PTS.npy\")\n",
        "        np.save(os.path.join(OUTPUT_DIR, save_name), final_data)\n",
        "\n",
        "    else:\n",
        "        print(f\"\\u26A0\\ufe0f Warning: Could not find raw file {filename}\")\n",
        "\n",
        "# --- REPORT ---\n",
        "OA = total_correct / total_points * 100\n",
        "mIoU_Instance = np.mean(instance_ious) * 100\n",
        "\n",
        "class_intersection = np.diag(global_confusion)\n",
        "class_union = np.sum(global_confusion, axis=1) + np.sum(global_confusion, axis=0) - class_intersection\n",
        "class_ious = class_intersection / (class_union + 1e-6) * 100\n",
        "mIoU_Class = np.mean(class_ious)\n",
        "\n",
        "print(\"\\n\" + \"=\"*40)\n",
        "print(f\"Overall Accuracy (OA): {OA:.2f}%\")\n",
        "print(f\"Instance mIoU:       {mIoU_Instance:.2f}%\")\n",
        "print(f\"Class mIoU:          {mIoU_Class:.2f}%\")\n",
        "print(\"=\"*30)\n",
        "\n",
        "print(\"PER-PART BREAKDOWN:\")\n",
        "part_names = [\"Standing Trunk (0)\", \"Fallen Trunk (1)\", \"Root Ball (2)\", \"Pit (3)\"]\n",
        "for i, score in enumerate(class_ious):\n",
        "    print(f\"{part_names[i]:<20}: {score:.2f}%\")\n",
        "print(\"=\"*40)\n",
        "\n",
        "# Zip\n",
        "!zip -q -r inference_results_12ch.zip {OUTPUT_DIR}"
      ],
      "metadata": {
        "id": "CyJLE13lUF56",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5ee964fc-bdbb-49c3-f8ed-bd9ffcf2b990"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/content/PointNeXt/openpoints/models/backbone/pointnetv2.py:79: SyntaxWarning: invalid escape sequence '\\s'\n",
            "  new_features: (B, \\sum_k(mlps[k][-1], npoint)) tensor of the new_features descriptors\n",
            "/content/PointNeXt/openpoints/models/layers/group.py:79: FutureWarning: `torch.cuda.amp.custom_fwd(args...)` is deprecated. Please use `torch.amp.custom_fwd(args..., device_type='cuda')` instead.\n",
            "  @torch.cuda.amp.custom_fwd(cast_inputs=torch.float32)\n",
            "/content/PointNeXt/openpoints/models/layers/upsampling.py:46: FutureWarning: `torch.cuda.amp.custom_fwd(args...)` is deprecated. Please use `torch.amp.custom_fwd(args..., device_type='cuda')` instead.\n",
            "  @torch.cuda.amp.custom_fwd(cast_inputs=torch.float32)\n",
            "/content/PointNeXt/openpoints/models/backbone/pointvit_inv.py:24: FutureWarning: `torch.cuda.amp.custom_fwd(args...)` is deprecated. Please use `torch.amp.custom_fwd(args..., device_type='cuda')` instead.\n",
            "  @custom_fwd\n",
            "/content/PointNeXt/openpoints/models/backbone/pointvit_inv.py:59: FutureWarning: `torch.cuda.amp.custom_bwd(args...)` is deprecated. Please use `torch.amp.custom_bwd(args..., device_type='cuda')` instead.\n",
            "  @custom_bwd\n",
            "/content/PointNeXt/openpoints/models/classification/cls_base.py:99: SyntaxWarning: invalid escape sequence '\\e'\n",
            "  $\\eg$ cls_feat='max,avg' means use the concatenateion of maxpooled and avgpooled features.\n",
            "WARNING:root:kwargs: {'shape_classes': 2} are not used in SegHead\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "üöÄ Starting Inference using model: shapenetpart-train-custom_fallen_trees-ngpus1-seed7520-20251212-155340-E84zcZ3REWTnK2wbniefub_ckpt_best.pth\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipython-input-750867153.py:43: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "  checkpoint = torch.load(model_path)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚úÖ Model loaded.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r  0%|          | 0/213 [00:00<?, ?it/s]/content/PointNeXt/openpoints/models/layers/subsample.py:93: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at ../torch/csrc/tensor/python_tensor.cpp:78.)\n",
            "  output = torch.cuda.IntTensor(B, npoint)\n",
            "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 213/213 [00:33<00:00,  6.35it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "========================================\n",
            "Overall Accuracy (OA): 98.00%\n",
            "Instance mIoU:       74.89%\n",
            "Class mIoU:          86.97%\n",
            "==============================\n",
            "PER-PART BREAKDOWN:\n",
            "Standing Trunk (0)  : 99.71%\n",
            "Fallen Trunk (1)    : 97.17%\n",
            "Root Ball (2)       : 75.46%\n",
            "Pit (3)             : 75.56%\n",
            "========================================\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "p2ZJ93bllR7T"
      },
      "source": [
        "# Download Result"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "id": "kYPtaXgHiOUE",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 17
        },
        "outputId": "c229fffd-d5d1-4ba2-b137-359434788be1"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "download(\"download_86fa76d1-27e8-4957-85f4-ea1e850ace87\", \"inference_results_12ch.zip\", 14619180)"
            ]
          },
          "metadata": {}
        }
      ],
      "source": [
        "from google.colab import files\n",
        "files.download('inference_results_12ch.zip')"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Test Using Official Code"
      ],
      "metadata": {
        "id": "D1KBoT3m6BMQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Patch main.py to fix Tensor Key Error\n",
        "import os\n",
        "\n",
        "target_file = \"/content/PointNeXt/examples/shapenetpart/main.py\"\n",
        "print(f\"Patching {target_file} to fix Tensor Key Error...\")\n",
        "\n",
        "with open(target_file, 'r') as f:\n",
        "    code = f.read()\n",
        "\n",
        "# The problematic line: parts = cls2parts[cls[shape_idx]]\n",
        "# We change it to: parts = cls2parts[int(cls[shape_idx])]\n",
        "\n",
        "bad_line = \"parts = cls2parts[cls[shape_idx]]\"\n",
        "good_line = \"parts = cls2parts[int(cls[shape_idx])]\"\n",
        "\n",
        "if bad_line in code:\n",
        "    code = code.replace(bad_line, good_line)\n",
        "    print(\"\\u2705 Patched: Cast tensor to int for dictionary lookup.\")\n",
        "else:\n",
        "    print(\"\\u26A0\\uFE0F Warning: Could not find exact line. Check if file changed.\")\n",
        "\n",
        "with open(target_file, 'w') as f:\n",
        "    f.write(code)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Vi-5gTfT7Qfv",
        "outputId": "13e7388d-fe00-405a-e79b-f1cdebf4ae68"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Patching /content/PointNeXt/examples/shapenetpart/main.py to fix Tensor Key Error...\n",
            "‚úÖ Patched: Cast tensor to int for dictionary lookup.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import glob\n",
        "\n",
        "# 1. Find the Model in Drive\n",
        "drive_model_dir = \"/content/drive/MyDrive/ML_Projects/PointNeXt/Models\"\n",
        "model_candidates = glob.glob(os.path.join(drive_model_dir, \"*_best.pth\"))\n",
        "\n",
        "if model_candidates:\n",
        "    # Pick the latest one\n",
        "    MODEL_PATH = max(model_candidates, key=os.path.getmtime)\n",
        "    print(f\"\\u2705 Found Model: {os.path.basename(MODEL_PATH)}\")\n",
        "else:\n",
        "    raise FileNotFoundError(\"\\u274c No model found in Drive! Did you run the backup cell?\")\n",
        "\n",
        "# 2. Define Config Path\n",
        "config_path = \"/content/PointNeXt/cfgs/shapenetpart/custom_fallen_trees.yaml\"\n",
        "\n",
        "# 3. Run Official Evaluation\n",
        "!PYTHONPATH=. python examples/shapenetpart/main.py --cfg {config_path} mode=test pretrained_path={MODEL_PATH} dataset.val.split=test"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "86SsJLP06Fag",
        "outputId": "47970fde-74ae-4fbc-ee11-220ced44ea46"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚úÖ Found Model: shapenetpart-train-custom_fallen_trees-ngpus1-seed7520-20251212-155340-E84zcZ3REWTnK2wbniefub_ckpt_best.pth\n",
            "2025-12-12 16:54:09.997696: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
            "E0000 00:00:1765558450.296571    6722 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "E0000 00:00:1765558450.377996    6722 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "W0000 00:00:1765558450.994820    6722 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1765558450.994879    6722 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1765558450.994884    6722 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1765558450.994889    6722 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "2025-12-12 16:54:11.055390: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
            "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "launch mp with 1 GPUs, current rank: 0\n",
            "\u001b[32m[12/12 16:54:24 FallenTreePart]: \u001b[0mdist_url: tcp://localhost:8888\n",
            "dist_backend: nccl\n",
            "multiprocessing_distributed: False\n",
            "ngpus_per_node: 1\n",
            "world_size: 1\n",
            "launcher: mp\n",
            "local_rank: 0\n",
            "use_gpu: True\n",
            "seed: 9269\n",
            "epoch: 0\n",
            "epochs: 100\n",
            "ignore_index: None\n",
            "val_fn: validate\n",
            "deterministic: False\n",
            "sync_bn: False\n",
            "criterion_args:\n",
            "  NAME: Poly1FocalLoss\n",
            "use_mask: False\n",
            "grad_norm_clip: 1\n",
            "layer_decay: 0\n",
            "step_per_update: 1\n",
            "start_epoch: 1\n",
            "sched_on_epoch: True\n",
            "wandb:\n",
            "  use_wandb: False\n",
            "  project: PointNext-ShapeNetPart\n",
            "  tags: ['test']\n",
            "  name: shapenetpart-train-custom_fallen_trees-ngpus1-seed7520-20251212-155340-E84zcZ3REWTnK2wbniefub_ckpt_best.pth_20251212-165424-4wJyZpQtQcMhr4MEYnirZL\n",
            "use_amp: False\n",
            "use_voting: False\n",
            "val_freq: 1\n",
            "resume: False\n",
            "test: False\n",
            "finetune: False\n",
            "mode: test\n",
            "logname: None\n",
            "load_path: None\n",
            "print_freq: 10\n",
            "save_freq: -1\n",
            "root_dir: log/shapenetpart\n",
            "pretrained_path: /content/drive/MyDrive/ML_Projects/PointNeXt/Models/shapenetpart-train-custom_fallen_trees-ngpus1-seed7520-20251212-155340-E84zcZ3REWTnK2wbniefub_ckpt_best.pth\n",
            "datatransforms:\n",
            "  train: ['PointsToTensor', 'PointCloudScaling', 'PointCloudCenterAndNormalize', 'PointCloudJitter', 'ChromaticDropGPU']\n",
            "  val: ['PointsToTensor', 'PointCloudCenterAndNormalize']\n",
            "  vote: ['PointCloudScaling']\n",
            "  kwargs:\n",
            "    jitter_sigma: 0.001\n",
            "    jitter_clip: 0.005\n",
            "    scale: [0.8, 1.2]\n",
            "    gravity_dim: 1\n",
            "    angle: [0, 1.0, 0]\n",
            "feature_keys: pos,x\n",
            "dataset:\n",
            "  common:\n",
            "    NAME: FallenTreePart\n",
            "    data_root: /content/processed_data\n",
            "    use_normal: False\n",
            "    num_points: 2048\n",
            "    use_xyz: True\n",
            "  train:\n",
            "    split: train\n",
            "  val:\n",
            "    split: test\n",
            "    presample: True\n",
            "num_classes: 4\n",
            "shape_classes: 2\n",
            "num_points: 2048\n",
            "normal_channel: True\n",
            "batch_size: 16\n",
            "dataloader:\n",
            "  num_workers: 4\n",
            "num_votes: 10\n",
            "refine: True\n",
            "lr: 0.001\n",
            "min_lr: None\n",
            "optimizer:\n",
            "  NAME: adamw\n",
            "  weight_decay: 0.0001\n",
            "sched: multistep\n",
            "decay_epochs: [70, 90]\n",
            "decay_rate: 0.1\n",
            "warmup_epochs: 0\n",
            "model:\n",
            "  NAME: BasePartSeg\n",
            "  encoder_args:\n",
            "    NAME: PointNextEncoder\n",
            "    blocks: [1, 1, 1, 1, 1]\n",
            "    strides: [1, 2, 2, 2, 2]\n",
            "    width: 32\n",
            "    in_channels: 7\n",
            "    sa_layers: 3\n",
            "    sa_use_res: True\n",
            "    radius: 0.1\n",
            "    radius_scaling: 2.5\n",
            "    nsample: 32\n",
            "    expansion: 4\n",
            "    aggr_args:\n",
            "      feature_type: dp_fj\n",
            "    reduction: max\n",
            "    group_args:\n",
            "      NAME: ballquery\n",
            "      normalize_dp: True\n",
            "    conv_args:\n",
            "      order: conv-norm-act\n",
            "    act_args:\n",
            "      act: relu\n",
            "    norm_args:\n",
            "      norm: bn\n",
            "  decoder_args:\n",
            "    NAME: PointNextPartDecoder\n",
            "    cls_map: curvenet\n",
            "  cls_args:\n",
            "    NAME: SegHead\n",
            "    global_feat: max,avg\n",
            "    num_classes: 4\n",
            "    shape_classes: 2\n",
            "    in_channels: None\n",
            "    norm_args:\n",
            "      norm: bn\n",
            "log_dir: log/shapenetpart/shapenetpart-train-custom_fallen_trees-ngpus1-seed7520-20251212-155340-E84zcZ3REWTnK2wbniefub_ckpt_best.pth_20251212-165424-4wJyZpQtQcMhr4MEYnirZL\n",
            "rank: 0\n",
            "distributed: False\n",
            "mp: False\n",
            "task_name: shapenetpart\n",
            "cfg_basename: custom_fallen_trees\n",
            "opts: mode=test-dataset.val.split=test\n",
            "is_training: False\n",
            "run_name: shapenetpart-train-custom_fallen_trees-ngpus1-seed7520-20251212-155340-E84zcZ3REWTnK2wbniefub_ckpt_best.pth_20251212-165424-4wJyZpQtQcMhr4MEYnirZL\n",
            "run_dir: log/shapenetpart/shapenetpart-train-custom_fallen_trees-ngpus1-seed7520-20251212-155340-E84zcZ3REWTnK2wbniefub_ckpt_best.pth_20251212-165424-4wJyZpQtQcMhr4MEYnirZL\n",
            "ckpt_dir: log/shapenetpart/shapenetpart-train-custom_fallen_trees-ngpus1-seed7520-20251212-155340-E84zcZ3REWTnK2wbniefub_ckpt_best.pth_20251212-165424-4wJyZpQtQcMhr4MEYnirZL/checkpoint\n",
            "code_dir: log/shapenetpart/shapenetpart-train-custom_fallen_trees-ngpus1-seed7520-20251212-155340-E84zcZ3REWTnK2wbniefub_ckpt_best.pth_20251212-165424-4wJyZpQtQcMhr4MEYnirZL/code\n",
            "log_path: log/shapenetpart/shapenetpart-train-custom_fallen_trees-ngpus1-seed7520-20251212-155340-E84zcZ3REWTnK2wbniefub_ckpt_best.pth_20251212-165424-4wJyZpQtQcMhr4MEYnirZL/shapenetpart-train-custom_fallen_trees-ngpus1-seed7520-20251212-155340-E84zcZ3REWTnK2wbniefub_ckpt_best.pth_20251212-165424-4wJyZpQtQcMhr4MEYnirZL.log\n",
            "cfg_path: log/shapenetpart/shapenetpart-train-custom_fallen_trees-ngpus1-seed7520-20251212-155340-E84zcZ3REWTnK2wbniefub_ckpt_best.pth_20251212-165424-4wJyZpQtQcMhr4MEYnirZL/cfg.yaml\n",
            "\u001b[32m[12/12 16:54:24 FallenTreePart]: \u001b[0mLoading test split from: /content/processed_data/train_test_split/shuffled_test_file_list.json\n",
            "\u001b[32m[12/12 16:54:24 FallenTreePart]: \u001b[0mFound 213 valid files for test split.\n",
            "\u001b[32m[12/12 16:54:24 FallenTreePart]: \u001b[0mlength of validation dataset: 213\n",
            "\u001b[32m[12/12 16:54:24 FallenTreePart]: \u001b[0mnumber of classes of the dataset: 4\n",
            "\u001b[32m[12/12 16:54:24 FallenTreePart]: \u001b[0mradius: [[0.1], [0.1], [0.25], [0.625], [1.5625]],\n",
            " nsample: [[32], [32], [32], [32], [32]]\n",
            "\u001b[32m[12/12 16:54:24 FallenTreePart]: \u001b[0mNAME: ballquery\n",
            "normalize_dp: True\n",
            "radius: 0.1\n",
            "nsample: 32\n",
            "\u001b[32m[12/12 16:54:24 FallenTreePart]: \u001b[0mNAME: ballquery\n",
            "normalize_dp: True\n",
            "radius: 0.25\n",
            "nsample: 32\n",
            "\u001b[32m[12/12 16:54:24 FallenTreePart]: \u001b[0mNAME: ballquery\n",
            "normalize_dp: True\n",
            "radius: 0.625\n",
            "nsample: 32\n",
            "\u001b[32m[12/12 16:54:24 FallenTreePart]: \u001b[0mNAME: ballquery\n",
            "normalize_dp: True\n",
            "radius: 1.5625\n",
            "nsample: 32\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m[12/12 16:54:24 FallenTreePart]: \u001b[0mkwargs: {'shape_classes': 2} are not used in SegHead\n",
            "\u001b[32m[12/12 16:54:24 FallenTreePart]: \u001b[0mBasePartSeg(\n",
            "  (encoder): PointNextEncoder(\n",
            "    (encoder): Sequential(\n",
            "      (0): Sequential(\n",
            "        (0): SetAbstraction(\n",
            "          (convs): Sequential(\n",
            "            (0): Sequential(\n",
            "              (0): Conv1d(7, 32, kernel_size=(1,), stride=(1,))\n",
            "            )\n",
            "          )\n",
            "        )\n",
            "      )\n",
            "      (1): Sequential(\n",
            "        (0): SetAbstraction(\n",
            "          (skipconv): Sequential(\n",
            "            (0): Conv1d(32, 64, kernel_size=(1,), stride=(1,))\n",
            "          )\n",
            "          (act): ReLU(inplace=True)\n",
            "          (convs): Sequential(\n",
            "            (0): Sequential(\n",
            "              (0): Conv2d(35, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "              (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "              (2): ReLU(inplace=True)\n",
            "            )\n",
            "            (1): Sequential(\n",
            "              (0): Conv2d(32, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "              (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "              (2): ReLU(inplace=True)\n",
            "            )\n",
            "            (2): Sequential(\n",
            "              (0): Conv2d(32, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "              (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "            )\n",
            "          )\n",
            "          (grouper): QueryAndGroup()\n",
            "        )\n",
            "      )\n",
            "      (2): Sequential(\n",
            "        (0): SetAbstraction(\n",
            "          (skipconv): Sequential(\n",
            "            (0): Conv1d(64, 128, kernel_size=(1,), stride=(1,))\n",
            "          )\n",
            "          (act): ReLU(inplace=True)\n",
            "          (convs): Sequential(\n",
            "            (0): Sequential(\n",
            "              (0): Conv2d(67, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "              (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "              (2): ReLU(inplace=True)\n",
            "            )\n",
            "            (1): Sequential(\n",
            "              (0): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "              (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "              (2): ReLU(inplace=True)\n",
            "            )\n",
            "            (2): Sequential(\n",
            "              (0): Conv2d(64, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "              (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "            )\n",
            "          )\n",
            "          (grouper): QueryAndGroup()\n",
            "        )\n",
            "      )\n",
            "      (3): Sequential(\n",
            "        (0): SetAbstraction(\n",
            "          (skipconv): Sequential(\n",
            "            (0): Conv1d(128, 256, kernel_size=(1,), stride=(1,))\n",
            "          )\n",
            "          (act): ReLU(inplace=True)\n",
            "          (convs): Sequential(\n",
            "            (0): Sequential(\n",
            "              (0): Conv2d(131, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "              (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "              (2): ReLU(inplace=True)\n",
            "            )\n",
            "            (1): Sequential(\n",
            "              (0): Conv2d(128, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "              (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "              (2): ReLU(inplace=True)\n",
            "            )\n",
            "            (2): Sequential(\n",
            "              (0): Conv2d(128, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "              (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "            )\n",
            "          )\n",
            "          (grouper): QueryAndGroup()\n",
            "        )\n",
            "      )\n",
            "      (4): Sequential(\n",
            "        (0): SetAbstraction(\n",
            "          (skipconv): Sequential(\n",
            "            (0): Conv1d(256, 512, kernel_size=(1,), stride=(1,))\n",
            "          )\n",
            "          (act): ReLU(inplace=True)\n",
            "          (convs): Sequential(\n",
            "            (0): Sequential(\n",
            "              (0): Conv2d(259, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "              (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "              (2): ReLU(inplace=True)\n",
            "            )\n",
            "            (1): Sequential(\n",
            "              (0): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "              (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "              (2): ReLU(inplace=True)\n",
            "            )\n",
            "            (2): Sequential(\n",
            "              (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "              (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "            )\n",
            "          )\n",
            "          (grouper): QueryAndGroup()\n",
            "        )\n",
            "      )\n",
            "    )\n",
            "  )\n",
            "  (decoder): PointNextPartDecoder(\n",
            "    (global_conv2): Sequential(\n",
            "      (0): Sequential(\n",
            "        (0): Conv1d(512, 128, kernel_size=(1,), stride=(1,))\n",
            "        (1): ReLU(inplace=True)\n",
            "      )\n",
            "    )\n",
            "    (global_conv1): Sequential(\n",
            "      (0): Sequential(\n",
            "        (0): Conv1d(256, 64, kernel_size=(1,), stride=(1,))\n",
            "        (1): ReLU(inplace=True)\n",
            "      )\n",
            "    )\n",
            "    (decoder): Sequential(\n",
            "      (0): Sequential(\n",
            "        (0): FeaturePropogation(\n",
            "          (convs): Sequential(\n",
            "            (0): Sequential(\n",
            "              (0): Conv1d(304, 32, kernel_size=(1,), stride=(1,), bias=False)\n",
            "              (1): BatchNorm1d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "              (2): ReLU(inplace=True)\n",
            "            )\n",
            "            (1): Sequential(\n",
            "              (0): Conv1d(32, 32, kernel_size=(1,), stride=(1,), bias=False)\n",
            "              (1): BatchNorm1d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "              (2): ReLU(inplace=True)\n",
            "            )\n",
            "          )\n",
            "        )\n",
            "      )\n",
            "      (1): Sequential(\n",
            "        (0): FeaturePropogation(\n",
            "          (convs): Sequential(\n",
            "            (0): Sequential(\n",
            "              (0): Conv1d(192, 64, kernel_size=(1,), stride=(1,), bias=False)\n",
            "              (1): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "              (2): ReLU(inplace=True)\n",
            "            )\n",
            "            (1): Sequential(\n",
            "              (0): Conv1d(64, 64, kernel_size=(1,), stride=(1,), bias=False)\n",
            "              (1): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "              (2): ReLU(inplace=True)\n",
            "            )\n",
            "          )\n",
            "        )\n",
            "      )\n",
            "      (2): Sequential(\n",
            "        (0): FeaturePropogation(\n",
            "          (convs): Sequential(\n",
            "            (0): Sequential(\n",
            "              (0): Conv1d(384, 128, kernel_size=(1,), stride=(1,), bias=False)\n",
            "              (1): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "              (2): ReLU(inplace=True)\n",
            "            )\n",
            "            (1): Sequential(\n",
            "              (0): Conv1d(128, 128, kernel_size=(1,), stride=(1,), bias=False)\n",
            "              (1): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "              (2): ReLU(inplace=True)\n",
            "            )\n",
            "          )\n",
            "        )\n",
            "      )\n",
            "      (3): Sequential(\n",
            "        (0): FeaturePropogation(\n",
            "          (convs): Sequential(\n",
            "            (0): Sequential(\n",
            "              (0): Conv1d(768, 256, kernel_size=(1,), stride=(1,), bias=False)\n",
            "              (1): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "              (2): ReLU(inplace=True)\n",
            "            )\n",
            "            (1): Sequential(\n",
            "              (0): Conv1d(256, 256, kernel_size=(1,), stride=(1,), bias=False)\n",
            "              (1): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "              (2): ReLU(inplace=True)\n",
            "            )\n",
            "          )\n",
            "        )\n",
            "      )\n",
            "    )\n",
            "  )\n",
            "  (head): SegHead(\n",
            "    (head): Sequential(\n",
            "      (0): Sequential(\n",
            "        (0): Conv1d(96, 96, kernel_size=(1,), stride=(1,), bias=False)\n",
            "        (1): BatchNorm1d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (2): ReLU(inplace=True)\n",
            "      )\n",
            "      (1): Dropout(p=0.5, inplace=False)\n",
            "      (2): Sequential(\n",
            "        (0): Conv1d(96, 4, kernel_size=(1,), stride=(1,))\n",
            "      )\n",
            "    )\n",
            "  )\n",
            ")\n",
            "\u001b[32m[12/12 16:54:24 FallenTreePart]: \u001b[0mNumber of params: 0.9774 M\n",
            "\u001b[32m[12/12 16:54:24 FallenTreePart]: \u001b[0mParam groups = {\n",
            "  \"decay\": {\n",
            "    \"weight_decay\": 0.0001,\n",
            "    \"params\": [\n",
            "      \"encoder.encoder.0.0.convs.0.0.weight\",\n",
            "      \"encoder.encoder.1.0.skipconv.0.weight\",\n",
            "      \"encoder.encoder.1.0.convs.0.0.weight\",\n",
            "      \"encoder.encoder.1.0.convs.1.0.weight\",\n",
            "      \"encoder.encoder.1.0.convs.2.0.weight\",\n",
            "      \"encoder.encoder.2.0.skipconv.0.weight\",\n",
            "      \"encoder.encoder.2.0.convs.0.0.weight\",\n",
            "      \"encoder.encoder.2.0.convs.1.0.weight\",\n",
            "      \"encoder.encoder.2.0.convs.2.0.weight\",\n",
            "      \"encoder.encoder.3.0.skipconv.0.weight\",\n",
            "      \"encoder.encoder.3.0.convs.0.0.weight\",\n",
            "      \"encoder.encoder.3.0.convs.1.0.weight\",\n",
            "      \"encoder.encoder.3.0.convs.2.0.weight\",\n",
            "      \"encoder.encoder.4.0.skipconv.0.weight\",\n",
            "      \"encoder.encoder.4.0.convs.0.0.weight\",\n",
            "      \"encoder.encoder.4.0.convs.1.0.weight\",\n",
            "      \"encoder.encoder.4.0.convs.2.0.weight\",\n",
            "      \"decoder.global_conv2.0.0.weight\",\n",
            "      \"decoder.global_conv1.0.0.weight\",\n",
            "      \"decoder.decoder.0.0.convs.0.0.weight\",\n",
            "      \"decoder.decoder.0.0.convs.1.0.weight\",\n",
            "      \"decoder.decoder.1.0.convs.0.0.weight\",\n",
            "      \"decoder.decoder.1.0.convs.1.0.weight\",\n",
            "      \"decoder.decoder.2.0.convs.0.0.weight\",\n",
            "      \"decoder.decoder.2.0.convs.1.0.weight\",\n",
            "      \"decoder.decoder.3.0.convs.0.0.weight\",\n",
            "      \"decoder.decoder.3.0.convs.1.0.weight\",\n",
            "      \"head.head.0.0.weight\",\n",
            "      \"head.head.2.0.weight\"\n",
            "    ],\n",
            "    \"lr_scale\": 1.0\n",
            "  },\n",
            "  \"no_decay\": {\n",
            "    \"weight_decay\": 0.0,\n",
            "    \"params\": [\n",
            "      \"encoder.encoder.0.0.convs.0.0.bias\",\n",
            "      \"encoder.encoder.1.0.skipconv.0.bias\",\n",
            "      \"encoder.encoder.1.0.convs.0.1.weight\",\n",
            "      \"encoder.encoder.1.0.convs.0.1.bias\",\n",
            "      \"encoder.encoder.1.0.convs.1.1.weight\",\n",
            "      \"encoder.encoder.1.0.convs.1.1.bias\",\n",
            "      \"encoder.encoder.1.0.convs.2.1.weight\",\n",
            "      \"encoder.encoder.1.0.convs.2.1.bias\",\n",
            "      \"encoder.encoder.2.0.skipconv.0.bias\",\n",
            "      \"encoder.encoder.2.0.convs.0.1.weight\",\n",
            "      \"encoder.encoder.2.0.convs.0.1.bias\",\n",
            "      \"encoder.encoder.2.0.convs.1.1.weight\",\n",
            "      \"encoder.encoder.2.0.convs.1.1.bias\",\n",
            "      \"encoder.encoder.2.0.convs.2.1.weight\",\n",
            "      \"encoder.encoder.2.0.convs.2.1.bias\",\n",
            "      \"encoder.encoder.3.0.skipconv.0.bias\",\n",
            "      \"encoder.encoder.3.0.convs.0.1.weight\",\n",
            "      \"encoder.encoder.3.0.convs.0.1.bias\",\n",
            "      \"encoder.encoder.3.0.convs.1.1.weight\",\n",
            "      \"encoder.encoder.3.0.convs.1.1.bias\",\n",
            "      \"encoder.encoder.3.0.convs.2.1.weight\",\n",
            "      \"encoder.encoder.3.0.convs.2.1.bias\",\n",
            "      \"encoder.encoder.4.0.skipconv.0.bias\",\n",
            "      \"encoder.encoder.4.0.convs.0.1.weight\",\n",
            "      \"encoder.encoder.4.0.convs.0.1.bias\",\n",
            "      \"encoder.encoder.4.0.convs.1.1.weight\",\n",
            "      \"encoder.encoder.4.0.convs.1.1.bias\",\n",
            "      \"encoder.encoder.4.0.convs.2.1.weight\",\n",
            "      \"encoder.encoder.4.0.convs.2.1.bias\",\n",
            "      \"decoder.global_conv2.0.0.bias\",\n",
            "      \"decoder.global_conv1.0.0.bias\",\n",
            "      \"decoder.decoder.0.0.convs.0.1.weight\",\n",
            "      \"decoder.decoder.0.0.convs.0.1.bias\",\n",
            "      \"decoder.decoder.0.0.convs.1.1.weight\",\n",
            "      \"decoder.decoder.0.0.convs.1.1.bias\",\n",
            "      \"decoder.decoder.1.0.convs.0.1.weight\",\n",
            "      \"decoder.decoder.1.0.convs.0.1.bias\",\n",
            "      \"decoder.decoder.1.0.convs.1.1.weight\",\n",
            "      \"decoder.decoder.1.0.convs.1.1.bias\",\n",
            "      \"decoder.decoder.2.0.convs.0.1.weight\",\n",
            "      \"decoder.decoder.2.0.convs.0.1.bias\",\n",
            "      \"decoder.decoder.2.0.convs.1.1.weight\",\n",
            "      \"decoder.decoder.2.0.convs.1.1.bias\",\n",
            "      \"decoder.decoder.3.0.convs.0.1.weight\",\n",
            "      \"decoder.decoder.3.0.convs.0.1.bias\",\n",
            "      \"decoder.decoder.3.0.convs.1.1.weight\",\n",
            "      \"decoder.decoder.3.0.convs.1.1.bias\",\n",
            "      \"head.head.0.1.weight\",\n",
            "      \"head.head.0.1.bias\",\n",
            "      \"head.head.2.0.bias\"\n",
            "    ],\n",
            "    \"lr_scale\": 1.0\n",
            "  }\n",
            "}\n",
            "\u001b[32m[12/12 16:54:25 FallenTreePart]: \u001b[0mSuccessful Loading the ckpt from /content/drive/MyDrive/ML_Projects/PointNeXt/Models/shapenetpart-train-custom_fallen_trees-ngpus1-seed7520-20251212-155340-E84zcZ3REWTnK2wbniefub_ckpt_best.pth\n",
            "\u001b[32m[12/12 16:54:25 FallenTreePart]: \u001b[0mckpts @ 65 epoch( {} )\n",
            "100% 14/14 [00:12<00:00,  1.09it/s]\n",
            "\u001b[32m[12/12 16:54:38 FallenTreePart]: \u001b[0mTest Epoch [0/100],Instance mIoU 96.25, Class mIoU 96.16, \n",
            " Class mIoUs tensor([100.0000,  92.3204], device='cuda:0')\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "hZsxiJssAqEv"
      },
      "execution_count": 23,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}