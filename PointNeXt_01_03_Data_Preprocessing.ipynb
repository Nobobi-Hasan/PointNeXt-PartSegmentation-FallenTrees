{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Nobobi-Hasan/PointNeXt-PartSegmentation-FallenTrees/blob/main/PointNeXt_01_03_Data_Preprocessing.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Mount Drive & Define Paths"
      ],
      "metadata": {
        "id": "eqU1M4p1IK2t"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "import os\n",
        "import numpy as np\n",
        "import shutil\n",
        "import json\n",
        "from tqdm import tqdm"
      ],
      "metadata": {
        "id": "0Pr-VHIQMTn-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Mount Google Drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "wDJ1GpesNQ4l",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4cf37532-7e25-492f-82ee-b2180828e388"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Path to the project root in Drive\n",
        "DRIVE_PROJECT_ROOT = \"/content/drive/MyDrive/ML_Projects/PointNeXt\"\n",
        "\n",
        "# Define project's Data directory\n",
        "DRIVE_DATA_DIR = os.path.join(DRIVE_PROJECT_ROOT, \"Data\")\n",
        "\n",
        "# Input/Output paths\n",
        "DRIVE_ZIP_PATH = os.path.join(DRIVE_DATA_DIR, \"data.zip\")\n",
        "LOCAL_DATA_DIR = \"/content/data/data\"\n",
        "LOCAL_PROCESSED_DIR = \"/content/processed_data\"\n",
        "FINAL_ZIP_NAME = \"processed_data.zip\"\n",
        "FINAL_DRIVE_PATH = os.path.join(DRIVE_DATA_DIR, FINAL_ZIP_NAME)\n",
        "\n",
        "print(f\"Project Root: {DRIVE_PROJECT_ROOT}\")\n",
        "print(f\"Data Source: {DRIVE_ZIP_PATH}\")"
      ],
      "metadata": {
        "id": "zmTOASC8NOfm",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "adc63610-07a1-4f17-fc71-9c01fbe42959"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Project Root: /content/drive/MyDrive/ML_Projects/PointNeXt\n",
            "Data Source: /content/drive/MyDrive/ML_Projects/PointNeXt/Data/data.zip\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Copy & Unzip Data"
      ],
      "metadata": {
        "id": "9ZuoahsaNoDO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "if not os.path.exists(DRIVE_ZIP_PATH):\n",
        "    print(f\"Error: Could not find 'data.zip' at {DRIVE_ZIP_PATH}\")\n",
        "    print(\"Please upload data.zip to the 'Data' folder in Drive first.\")\n",
        "else:\n",
        "    # Copy to colab local disk\n",
        "    if not os.path.exists(\"/content/data.zip\"):\n",
        "        print(\"Copying data.zip from Drive to Colab local disk...\")\n",
        "        shutil.copy(DRIVE_ZIP_PATH, \"/content/data.zip\")\n",
        "\n",
        "    # Unzip\n",
        "    if not os.path.exists(LOCAL_DATA_DIR):\n",
        "        print(\"Unzipping data...\")\n",
        "        !unzip -q /content/data.zip -d /content/data\n",
        "        print(\"Unzip Complete.\")\n",
        "    else:\n",
        "        print(\"Data already unzipped.\")"
      ],
      "metadata": {
        "id": "ZBKpdofWNg_8",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ab22a22c-fc19-4b2e-a558-bb914b5ac314"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Copying data.zip from Drive to Colab local disk...\n",
            "Unzipping data...\n",
            "Unzip Complete.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Check raw data shape\n",
        "raw_dir_0 = \"/content/data/data/0/numpy\"\n",
        "raw_dir_1 = \"/content/data/data/1/numpy\"\n",
        "\n",
        "target_file = None\n",
        "if os.path.exists(raw_dir_0) and len(os.listdir(raw_dir_0)) > 0:\n",
        "    target_file = os.path.join(raw_dir_0, os.listdir(raw_dir_0)[0])\n",
        "elif os.path.exists(raw_dir_1) and len(os.listdir(raw_dir_1)) > 0:\n",
        "    target_file = os.path.join(raw_dir_1, os.listdir(raw_dir_1)[0])\n",
        "\n",
        "if target_file:\n",
        "    raw_data = np.load(target_file)\n",
        "    print(f\"Raw File: {os.path.basename(target_file)}\")\n",
        "    print(f\"Raw Shape: {raw_data.shape}\")\n",
        "else:\n",
        "    print(\"No raw files found. Check /content/data path.\")"
      ],
      "metadata": {
        "id": "VUbxbp5kS15V",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b3b4e6af-da00-4f30-a9c9-52046930844e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Raw File: standing_trees_30.npy\n",
            "Raw Shape: (21921, 11)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Define Normalization Functions"
      ],
      "metadata": {
        "id": "rfvMdK9qO6OW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def pc_normalize(pc):\n",
        "    centroid = np.mean(pc, axis=0)\n",
        "    pc = pc - centroid\n",
        "    m = np.max(np.sqrt(np.sum(pc ** 2, axis=1)))\n",
        "    pc = pc / m\n",
        "    return pc\n",
        "\n",
        "def rgb_normalize(points):\n",
        "    min_vals = points.min(axis=0)      # shape: (,3)\n",
        "    max_vals = points.max(axis=0)      # shape: (,3)\n",
        "\n",
        "    normalized = (points - min_vals) / (max_vals - min_vals + 1e-8)  # avoid div by zero\n",
        "\n",
        "    return normalized\n",
        "\n",
        "def intensity_normalize(points):\n",
        "    min_vals = points.min()      # shape: (,3)\n",
        "    max_vals = points.max()      # shape: (,3)\n",
        "\n",
        "    normalized = (points - min_vals) / (max_vals - min_vals + 1e-8)  # avoid div by zero\n",
        "\n",
        "    return normalized"
      ],
      "metadata": {
        "id": "qqWayK06ai0_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Process, Normalize & Split"
      ],
      "metadata": {
        "id": "gzosWCxdPDBF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\" Using 15% for val \"\"\"\n",
        "\n",
        "import random\n",
        "\n",
        "def process_dataset():\n",
        "    # Define split file paths\n",
        "    train_json_path = os.path.join(LOCAL_DATA_DIR, \"train_test_split/shuffled_train_file_list.json\")\n",
        "    test_json_path = os.path.join(LOCAL_DATA_DIR, \"train_test_split/shuffled_test_file_list.json\")\n",
        "\n",
        "    if not os.path.exists(train_json_path):\n",
        "        print(f\"Error: Could not find train_test_split folder inside {LOCAL_DATA_DIR}\")\n",
        "        return\n",
        "    else:\n",
        "        LOCAL_DATA_ROOT = LOCAL_DATA_DIR\n",
        "\n",
        "    # Create Official Folder Hierarchy\n",
        "    class_0_dir = os.path.join(LOCAL_PROCESSED_DIR, \"0\")\n",
        "    class_1_dir = os.path.join(LOCAL_PROCESSED_DIR, \"1\")\n",
        "    os.makedirs(class_0_dir, exist_ok=True)\n",
        "    os.makedirs(class_1_dir, exist_ok=True)\n",
        "\n",
        "    # Destination for Split Files\n",
        "    dest_split_dir = os.path.join(LOCAL_PROCESSED_DIR, \"train_test_split\")\n",
        "    os.makedirs(dest_split_dir, exist_ok=True)\n",
        "\n",
        "    # SPLIT LOGIC\n",
        "    print(\"Loading and splitting data...\")\n",
        "    with open(train_json_path, 'r') as f:\n",
        "        full_train = json.load(f)\n",
        "\n",
        "    with open(test_json_path, 'r') as f:\n",
        "        full_test = json.load(f)\n",
        "\n",
        "    # Shuffle and Split (85% Train / 15% Val)\n",
        "    random.seed(42) # Ensure reproducibility\n",
        "    random.shuffle(full_train)\n",
        "    split_idx = int(len(full_train) * 0.85)\n",
        "\n",
        "    train_list = full_train[:split_idx]\n",
        "    val_list = full_train[split_idx:]\n",
        "\n",
        "    print(f\"Stats -> Train: {len(train_list)}, Val: {len(val_list)}, Test: {len(full_test)}\")\n",
        "\n",
        "    # Process 3 splits instead of 2\n",
        "    splits_to_process = [(\"train\", train_list), (\"val\", val_list), (\"test\", full_test)]\n",
        "\n",
        "    for split_name, file_list in splits_to_process:\n",
        "        print(f\"\\nProcessing {split_name} list...\")\n",
        "\n",
        "        new_json_list = []\n",
        "\n",
        "        for filename in tqdm(file_list):\n",
        "            clean_filename = os.path.basename(filename.replace(\"\\\\\", \"/\"))\n",
        "\n",
        "            # Find Source File\n",
        "            path_0 = os.path.join(LOCAL_DATA_ROOT, \"0\", \"numpy\", clean_filename)\n",
        "            path_1 = os.path.join(LOCAL_DATA_ROOT, \"1\", \"numpy\", clean_filename)\n",
        "\n",
        "            full_path = None\n",
        "            dest_folder = None\n",
        "            path_prefix = \"\"\n",
        "\n",
        "            if os.path.exists(path_0):\n",
        "                full_path = path_0\n",
        "                dest_folder = class_0_dir\n",
        "                path_prefix = \"0/numpy/\" # Dummy string for the parser\n",
        "            elif os.path.exists(path_1):\n",
        "                full_path = path_1\n",
        "                dest_folder = class_1_dir\n",
        "                path_prefix = \"1/numpy/\"\n",
        "\n",
        "            if full_path and dest_folder:\n",
        "                # Load & Normalize\n",
        "                data = np.load(full_path).astype(np.float32)\n",
        "                xyz = pc_normalize(data[:, 0:3])\n",
        "                rgb = rgb_normalize(data[:, 3:6])\n",
        "                intensity = intensity_normalize(data[:, 6]).reshape(-1, 1)\n",
        "                labels = data[:, 10].astype(np.int64).reshape(-1, 1)\n",
        "\n",
        "                processed_data = np.hstack((xyz, rgb, intensity, labels))\n",
        "\n",
        "                # Save File\n",
        "                np.save(os.path.join(dest_folder, clean_filename), processed_data)\n",
        "\n",
        "                # Add path to list\n",
        "                new_json_list.append(f\"{path_prefix}{clean_filename}\")\n",
        "            else:\n",
        "                print(f\"Warning: File {clean_filename} not found.\")\n",
        "\n",
        "        # Save the JSON list\n",
        "        dest_json_name = f\"shuffled_{split_name}_file_list.json\"\n",
        "        with open(os.path.join(dest_split_dir, dest_json_name), 'w') as f:\n",
        "            json.dump(new_json_list, f)\n",
        "\n",
        "    # Create Dummy Metadata\n",
        "    with open(os.path.join(LOCAL_PROCESSED_DIR, \"synsetoffset2category.txt\"), 'w') as f:\n",
        "        f.write(\"Standing_Trees\\t0\\nFallen_Trees\\t1\\n\")\n",
        "\n",
        "    print(f\"\\n\\u2705 Processing Complete! 85/15 Train/Val split created.\")\n",
        "\n",
        "# Run it\n",
        "process_dataset()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bA7Sue793JLw",
        "outputId": "9946cf76-6bf5-47df-cfd5-b5538258bd27"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading and splitting data...\n",
            "Stats -> Train: 721, Val: 128, Test: 213\n",
            "\n",
            "Processing train list...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 721/721 [00:01<00:00, 705.32it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Processing val list...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 128/128 [00:00<00:00, 796.11it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Processing test list...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 213/213 [00:00<00:00, 696.81it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "✅ Processing Complete! 85/15 Train/Val split created.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# \"\"\"for directly copying splitting files from RAW to processed data\"\"\"\n",
        "\n",
        "# print(\"Copying split files to processed directory...\")\n",
        "\n",
        "# raw_split_dir = os.path.join(LOCAL_DATA_DIR, \"train_test_split\")\n",
        "\n",
        "# # Destination: Inside the processed data folder\n",
        "# dest_split_dir = os.path.join(LOCAL_PROCESSED_DIR, \"train_test_split\")\n",
        "\n",
        "# if raw_split_dir:\n",
        "#     if os.path.exists(dest_split_dir):\n",
        "#         shutil.rmtree(dest_split_dir) # Remove old copy if exists\n",
        "#     shutil.copytree(raw_split_dir, dest_split_dir)\n",
        "#     print(f\"Success! Copied split files from '{raw_split_dir}' to '{dest_split_dir}'\")\n",
        "# else:\n",
        "#     print(\"Error: Could not find 'train_test_split' folder in raw data.\")"
      ],
      "metadata": {
        "id": "RN4zeS3Afr1_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Zip & Upload to Drive"
      ],
      "metadata": {
        "id": "I_31WOneQFlF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Zipping processed data...\")\n",
        "# Zip the folder\n",
        "!zip -r -q /content/processed_data.zip /content/processed_data\n",
        "\n",
        "print(f\"Uploading to Drive: {FINAL_DRIVE_PATH}...\")\n",
        "shutil.copy(\"/content/processed_data.zip\", FINAL_DRIVE_PATH)\n",
        "\n",
        "print(f\"\\nDONE! '{FINAL_ZIP_NAME}' is safely stored in Google Drive inside the '{DRIVE_DATA_DIR}' folder.\")"
      ],
      "metadata": {
        "id": "m3YZLic5P6uF",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ab918ba8-af60-41e8-9676-dc8ae1458eb1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Zipping processed data...\n",
            "Uploading to Drive: /content/drive/MyDrive/ML_Projects/PointNeXt/Data/processed_data.zip...\n",
            "\n",
            "DONE! 'processed_data.zip' is safely stored in Google Drive inside the '/content/drive/MyDrive/ML_Projects/PointNeXt/Data' folder.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "3Y6dWOQNEcGY"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}