{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Nobobi-Hasan/PointNeXt-PartSegmentation-FallenTrees/blob/main/PointNeXt_02_05_Training_shapenetpart.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "import os\n",
        "import shutil\n",
        "import torch\n",
        "import subprocess\n",
        "import sys"
      ],
      "metadata": {
        "id": "YAhNWDtNOK3P"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Mount Google Drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "yhFy7Up_OC2x",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "97890d95-4c06-4b23-d60f-0a2a5af4d059"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Path to the project root in Drive\n",
        "DRIVE_PROJECT_ROOT = \"/content/drive/MyDrive/ML_Projects/PointNeXt\"\n",
        "\n",
        "# Subfolders\n",
        "DRIVE_DATA_DIR = os.path.join(DRIVE_PROJECT_ROOT, \"Data\")\n",
        "DRIVE_MODELS_DIR = os.path.join(DRIVE_PROJECT_ROOT, \"Models\")\n",
        "\n",
        "# Input paths\n",
        "DRIVE_ZIP_PATH = os.path.join(DRIVE_DATA_DIR, \"processed_data.zip\")\n",
        "LOCAL_DATA_DIR = \"/content/processed_data\"\n",
        "\n",
        "print(f\"Project Root: {DRIVE_PROJECT_ROOT}\")"
      ],
      "metadata": {
        "id": "m16azjY-OFhX",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ef104c6f-0972-436c-8aa0-f6be1006f114"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Project Root: /content/drive/MyDrive/ML_Projects/PointNeXt\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "dEOzQvBtN1sV"
      },
      "outputs": [],
      "source": [
        "# Copy processed data from Drive\n",
        "if not os.path.exists(\"/content/processed_data\"):\n",
        "    if os.path.exists(DRIVE_ZIP_PATH):\n",
        "        print(\"Copying processed_data.zip from Drive...\")\n",
        "        shutil.copy(DRIVE_ZIP_PATH, \"/content/processed_data.zip\")\n",
        "        print(\"Unzipping...\")\n",
        "        !unzip -q -o /content/processed_data.zip -d /\n",
        "        print(\"Data Ready at /content/processed_data\")\n",
        "    else:\n",
        "        print(f\"Error: Could not find processed_data.zip at {DRIVE_ZIP_PATH}\")\n",
        "\n",
        "# Clone PointNeXt\n",
        "if not os.path.exists(\"/content/PointNeXt\"):\n",
        "    print(\"Cloning PointNeXt...\")\n",
        "    %cd /content\n",
        "    !git clone https://github.com/guochengqian/PointNeXt.git"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## For openpoints"
      ],
      "metadata": {
        "id": "uWxJD5qWrUgR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# For openpoints\n",
        "\n",
        "%cd /content/PointNeXt\n",
        "\n",
        "# 1. Replace SSH url with HTTPS url in .gitmodules\n",
        "!sed -i 's/git@github.com:/https:\\/\\/github.com\\//' .gitmodules\n",
        "\n",
        "# 2. Sync the new URL\n",
        "!git submodule sync\n",
        "\n",
        "# 3. Update the submodule (This will work now)\n",
        "print(\"Downloading openpoints via HTTPS...\")\n",
        "!git submodule update --init --recursive\n",
        "\n",
        "print(\"\\u2705 Submodule 'openpoints' downloaded successfully.\")"
      ],
      "metadata": {
        "id": "xvcHjNHdVR_m",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d19ed159-5737-42f2-d692-dfdea78ae252"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/PointNeXt\n",
            "Synchronizing submodule url for 'openpoints'\n",
            "Downloading openpoints via HTTPS...\n",
            "‚úÖ Submodule 'openpoints' downloaded successfully.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Config File"
      ],
      "metadata": {
        "id": "1QKZI0MtrZch"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "config_path = \"/content/PointNeXt/cfgs/shapenetpart/custom_fallen_trees.yaml\"\n",
        "\n",
        "config_content = \"\"\"\n",
        "num_classes: 4\n",
        "shape_classes: 2\n",
        "epochs: 100\n",
        "\n",
        "# Explicitly define feature keys to match the custom data loader\n",
        "feature_keys: 'pos,x'\n",
        "\n",
        "model:\n",
        "  NAME: BasePartSeg\n",
        "  encoder_args:\n",
        "    NAME: PointNextEncoder\n",
        "    blocks: [1, 1, 1, 1, 1]\n",
        "    strides: [1, 2, 2, 2, 2]\n",
        "    width: 32\n",
        "    in_channels: 7\n",
        "    sa_layers: 3\n",
        "    sa_use_res: True\n",
        "    radius: 0.1\n",
        "    radius_scaling: 2.5\n",
        "    nsample: 32\n",
        "    expansion: 4\n",
        "    aggr_args:\n",
        "      feature_type: 'dp_fj'\n",
        "    reduction: 'max'\n",
        "    group_args:\n",
        "      NAME: 'ballquery'\n",
        "      normalize_dp: True\n",
        "    conv_args:\n",
        "      order: conv-norm-act\n",
        "    act_args:\n",
        "      act: 'relu'\n",
        "    norm_args:\n",
        "      norm: 'bn'\n",
        "  decoder_args:\n",
        "    NAME: PointNextPartDecoder\n",
        "    cls_map: curvenet\n",
        "  cls_args:\n",
        "    NAME: SegHead\n",
        "    global_feat: max,avg\n",
        "    num_classes: 4\n",
        "    shape_classes: 2\n",
        "    in_channels: null\n",
        "    norm_args:\n",
        "      norm: 'bn'\n",
        "\n",
        "dataset:\n",
        "  common:\n",
        "    NAME: FallenTreePart\n",
        "    data_root: /content/processed_data\n",
        "    use_normal: False\n",
        "    use_xyz: True\n",
        "    num_points: 2048\n",
        "  train:\n",
        "    split: train\n",
        "  val:\n",
        "    split: val\n",
        "\n",
        "batch_size: 16\n",
        "dataloader:\n",
        "  num_workers: 4\n",
        "\n",
        "lr: 0.001\n",
        "min_lr: null\n",
        "optimizer:\n",
        "  NAME: adamw\n",
        "  weight_decay: 1.0e-4\n",
        "\n",
        "criterion_args:\n",
        "  NAME: Poly1FocalLoss\n",
        "\n",
        "# scheduler\n",
        "epochs: 100\n",
        "sched: multistep\n",
        "decay_epochs: [70, 90]\n",
        "decay_rate: 0.1\n",
        "warmup_epochs: 0\n",
        "\n",
        "datatransforms:\n",
        "  train: [PointsToTensor, PointCloudScaling, PointCloudCenterAndNormalize, PointCloudJitter, ChromaticDropGPU]\n",
        "  val: [PointsToTensor, PointCloudCenterAndNormalize]\n",
        "  kwargs:\n",
        "    jitter_sigma: 0.001\n",
        "    jitter_clip: 0.005\n",
        "    scale: [0.8, 1.2]\n",
        "    gravity_dim: 1\n",
        "    angle: [0, 1.0, 0]\n",
        "\n",
        "log_dir: /content/PointNeXt/log/shapenetpart/custom_trees\n",
        "\"\"\"\n",
        "\n",
        "with open(config_path, 'w') as f:\n",
        "    f.write(config_content)\n",
        "print(f\"\\u2705 Config Updated.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "G2bUEH8j4F8I",
        "outputId": "02088c93-9c38-40e5-ce3e-2b0f39b95a0b"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚úÖ Config Updated.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Dataset Handle"
      ],
      "metadata": {
        "id": "B_HEgPDlrdMo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Create the directory\n",
        "new_dataset_dir = \"/content/PointNeXt/openpoints/dataset/fallentree\"\n",
        "os.makedirs(new_dataset_dir, exist_ok=True)\n",
        "print(f\"\\U0001F4BE Created folder: {new_dataset_dir}\")\n",
        "\n",
        "# Create the '__init__.py' to make it a package\n",
        "init_path = os.path.join(new_dataset_dir, \"__init__.py\")\n",
        "with open(init_path, 'w') as f:\n",
        "    f.write(\"from .fallentree import FallenTreePart\\n\")\n",
        "print(f\"\\u2705 Created: {init_path}\")\n",
        "\n",
        "# Create the 'fallentree.py' (The Custom Loader)\n",
        "code_path = os.path.join(new_dataset_dir, \"fallentree.py\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dYNH_fgPgvC8",
        "outputId": "fd98baf8-e97b-49a0-f19d-ab7d4db96e2d"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "üíæ Created folder: /content/PointNeXt/openpoints/dataset/fallentree\n",
            "‚úÖ Created: /content/PointNeXt/openpoints/dataset/fallentree/__init__.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Define Path\n",
        "code_path = \"/content/PointNeXt/openpoints/dataset/fallentree/fallentree.py\"\n",
        "\n",
        "# Define Code\n",
        "dataset_code = \"\"\"\n",
        "import os\n",
        "import glob\n",
        "import json\n",
        "import logging\n",
        "import numpy as np\n",
        "import torch\n",
        "from torch.utils.data import Dataset\n",
        "from ..build import DATASETS\n",
        "\n",
        "\n",
        "# NEW QUOTA-BASED SAMPLING FUNCTION\n",
        "def quota_sample(xyz, part_labels, npoint):\n",
        "    \\\"\\\"\\\"\n",
        "    For TRAINING: Forces small classes (Roots/Pits) to have a minimum number of points.\n",
        "    This helps the model see them, even if they are tiny in the real tree.\n",
        "    \\\"\\\"\\\"\n",
        "\n",
        "    total_points = len(xyz)\n",
        "    if total_points <= npoint:\n",
        "        return np.random.choice(total_points, npoint, replace=True)\n",
        "\n",
        "    unique_parts = np.unique(part_labels)\n",
        "\n",
        "    # THE STRATEGY\n",
        "    # We want to reserve specific slots for the hard classes.\n",
        "    # Total 2048 points.\n",
        "    # If Root exists, force 256 points.\n",
        "    # If Pit exists, force 256 points.\n",
        "    # The rest from all points.\n",
        "\n",
        "    min_points_map = {\n",
        "        0: 0,    # Standing Trunk: No minimum forced\n",
        "        1: 0,    # Fallen Trunk: No minimum forced\n",
        "        2: 256,  # Root: 256 points minimum\n",
        "        3: 256   # Pit: 256 points minimum\n",
        "    }\n",
        "\n",
        "    indices_collected = []\n",
        "\n",
        "    # 1. Collect the hard classes first\n",
        "    for part in unique_parts:\n",
        "        if part in min_points_map and min_points_map[part] > 0:\n",
        "            part_indices = np.where(part_labels == part)[0]\n",
        "\n",
        "            # If the part is smaller than the quota, take it all with replacement (duplicate points)\n",
        "            req_count = min_points_map[part]\n",
        "\n",
        "            if len(part_indices) >= req_count:\n",
        "                chosen = np.random.choice(part_indices, req_count, replace=False)\n",
        "            else:\n",
        "                chosen = np.random.choice(part_indices, req_count, replace=True)\n",
        "\n",
        "            indices_collected.extend(chosen)\n",
        "\n",
        "    # Fill the rest of the 2048 points with points left\n",
        "    current_count = len(indices_collected)\n",
        "    remaining_slots = npoint - current_count\n",
        "\n",
        "    if remaining_slots > 0:\n",
        "        all_indices = np.arange(total_points)\n",
        "        fill_indices = np.random.choice(all_indices, remaining_slots, replace=False)\n",
        "        indices_collected.extend(fill_indices)\n",
        "\n",
        "    final_indices = np.array(indices_collected)\n",
        "\n",
        "    # Shuffle so the roots or pits aren't all at the beginning\n",
        "    np.random.shuffle(final_indices)\n",
        "\n",
        "    # If by some edge case we have too many, trim it\n",
        "    if len(final_indices) > npoint:\n",
        "        final_indices = final_indices[:npoint]\n",
        "\n",
        "    return final_indices\n",
        "\n",
        "\n",
        "@DATASETS.register_module()\n",
        "class FallenTreePart(Dataset):\n",
        "    classes = ['standing', 'fallen']\n",
        "    num_classes = 4\n",
        "    shape_classes = 2\n",
        "\n",
        "    # --- FIX: Add dummy key -1 for part_seg_refinement compatibility ---\n",
        "    # -1 points to all parts (0,1,2,3) so the code can find the max index\n",
        "    cls2parts = {\n",
        "        0: [0],\n",
        "        1: [1, 2, 3],\n",
        "        -1: [0, 1, 2, 3]\n",
        "    }\n",
        "\n",
        "    part_start = [0, 1]\n",
        "\n",
        "    # Pre-compute embedding\n",
        "    cls2partembed = torch.zeros(shape_classes, num_classes)\n",
        "    for i in [0, 1]: # Iterate only real classes\n",
        "        idx = cls2parts[i]\n",
        "        cls2partembed[i].scatter_(0, torch.LongTensor(idx), 1)\n",
        "\n",
        "    def __init__(self,\n",
        "                 data_root,\n",
        "                 split=None,\n",
        "                 num_points=2048,\n",
        "                 use_normal=False,\n",
        "                 use_xyz=True,\n",
        "                 **kwargs):\n",
        "        self.root = data_root\n",
        "        self.npoints = num_points\n",
        "        self.split = split\n",
        "\n",
        "        # if split == 'val': split_name = 'test'\n",
        "        # else: split_name = split\n",
        "\n",
        "        split_name = split\n",
        "\n",
        "        split_file = os.path.join(self.root, 'train_test_split', f'shuffled_{split_name}_file_list.json')\n",
        "        if not os.path.exists(split_file):\n",
        "             raise FileNotFoundError(f\"Split list not found: {split_file}\")\n",
        "\n",
        "        logging.info(f\"Loading {split} split from: {split_file}\")\n",
        "        with open(split_file, 'r') as f:\n",
        "            raw_list = json.load(f)\n",
        "\n",
        "        self.file_list = []\n",
        "        for item in raw_list:\n",
        "            clean_item = item.replace(\\\"\\\\\\\\\\\", \\\"/\\\")\n",
        "            fname = os.path.basename(clean_item)\n",
        "            candidates = [\n",
        "                clean_item,\n",
        "                os.path.join(self.root, clean_item),\n",
        "                os.path.join(self.root, '0', fname),\n",
        "                os.path.join(self.root, '1', fname)\n",
        "            ]\n",
        "            found = False\n",
        "            for path in candidates:\n",
        "                if os.path.exists(path):\n",
        "                    self.file_list.append(path)\n",
        "                    found = True\n",
        "                    break\n",
        "\n",
        "        logging.info(f\"Found {len(self.file_list)} valid files for {split} split.\")\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        file_path = self.file_list[index]\n",
        "        cls_idx = 0 if '/0/' in file_path.replace('\\\\\\\\', '/') else 1\n",
        "\n",
        "        data = np.load(file_path).astype(np.float32)\n",
        "        xyz = data[:, 0:3]\n",
        "        features = data[:, 3:7]\n",
        "        part_label = data[:, 7].astype(np.int64)\n",
        "        cls_label = np.array([cls_idx]).astype(np.int64)\n",
        "\n",
        "        if self.split == 'train':\n",
        "            choice = quota_sample(xyz, part_label, self.npoints)\n",
        "        else:\n",
        "            if len(xyz) >= self.npoints:\n",
        "                choice = np.random.choice(len(xyz), self.npoints, replace=False)\n",
        "            else:\n",
        "                choice = np.random.choice(len(xyz), self.npoints, replace=True)\n",
        "\n",
        "        xyz = xyz[choice]\n",
        "        features = features[choice]\n",
        "        part_label = part_label[choice]\n",
        "\n",
        "        return {'pos': xyz, 'x': features, 'y': part_label, 'cls': cls_label}\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.file_list)\n",
        "\"\"\"\n",
        "\n",
        "with open(code_path, 'w') as f:\n",
        "    f.write(dataset_code)\n",
        "print(f\"\\u2705 Updated: {code_path}.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7o_C0iiSgg7N",
        "outputId": "569df28a-70ac-4bf4-9a73-571d09c63754"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚úÖ Updated: /content/PointNeXt/openpoints/dataset/fallentree/fallentree.py.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Register the new folder in the Main Library\n",
        "# We need to add \"from .fallentree import FallenTreePart\" to openpoints/dataset/__init__.py\n",
        "\n",
        "main_init = \"/content/PointNeXt/openpoints/dataset/__init__.py\"\n",
        "with open(main_init, 'r') as f:\n",
        "    content = f.read()\n",
        "\n",
        "if \"fallentree\" not in content:\n",
        "    print(\"Registering new dataset in main __init__.py...\")\n",
        "    with open(main_init, 'a') as f:\n",
        "        f.write(\"\\nfrom .fallentree import FallenTreePart\\n\")\n",
        "    print(\"\\u2705 Registration Complete.\")\n",
        "else:\n",
        "    print(\"\\u2705 Already registered.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Md659eXhgnNb",
        "outputId": "3b37e71d-a595-4eff-cb4f-cfc0b8350300"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚úÖ Already registered.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Install Dependencies"
      ],
      "metadata": {
        "id": "XmqBEuRjriVQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Install Dependencies\n",
        "print(\"Installing Dependencies...\")\n",
        "%cd /content/PointNeXt\n",
        "\n",
        "# A. Install PyTorch 2.4.0 (Compatible)\n",
        "!pip install torch==2.4.0 torchvision==0.19.0 torchaudio==2.4.0 --index-url https://download.pytorch.org/whl/cu121\n",
        "\n",
        "# B. Install torch-scatter/sparse\n",
        "!pip install torch-scatter torch-sparse -f https://data.pyg.org/whl/torch-2.4.0+cu121.html\n",
        "\n",
        "# C. Fix requirements.txt\n",
        "!sed -i 's/==.*//g' requirements.txt\n",
        "\n",
        "# D. Install requirements\n",
        "!pip install -r requirements.txt\n",
        "\n",
        "# E. SKIP 'pip install -e .' (Because setup.py is missing)\n",
        "print(\"\\u2705 Dependencies Installed. (Skipped setup.py)\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PPA2HJBBq0ep",
        "outputId": "5aa921cb-9695-48ef-f52b-0f9091e1e490"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Installing Dependencies...\n",
            "/content/PointNeXt\n",
            "\u001b[33mDEPRECATION: Loading egg at /usr/local/lib/python3.12/dist-packages/pointnet2_cuda-0.0.0-py3.12-linux-x86_64.egg is deprecated. pip 24.3 will enforce this behaviour change. A possible replacement is to use pip for package installation. Discussion can be found at https://github.com/pypa/pip/issues/12330\u001b[0m\u001b[33m\n",
            "\u001b[0m\u001b[33mDEPRECATION: Loading egg at /usr/local/lib/python3.12/dist-packages/emd_ext-0.0.0-py3.12-linux-x86_64.egg is deprecated. pip 24.3 will enforce this behaviour change. A possible replacement is to use pip for package installation. Discussion can be found at https://github.com/pypa/pip/issues/12330\u001b[0m\u001b[33m\n",
            "\u001b[0mLooking in indexes: https://download.pytorch.org/whl/cu121\n",
            "Requirement already satisfied: torch==2.4.0 in /usr/local/lib/python3.12/dist-packages (2.4.0+cu121)\n",
            "Requirement already satisfied: torchvision==0.19.0 in /usr/local/lib/python3.12/dist-packages (0.19.0+cu121)\n",
            "Requirement already satisfied: torchaudio==2.4.0 in /usr/local/lib/python3.12/dist-packages (2.4.0+cu121)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from torch==2.4.0) (3.20.0)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.12/dist-packages (from torch==2.4.0) (4.15.0)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.12/dist-packages (from torch==2.4.0) (1.14.0)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.12/dist-packages (from torch==2.4.0) (3.6.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from torch==2.4.0) (3.1.6)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.12/dist-packages (from torch==2.4.0) (2025.3.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (from torch==2.4.0) (75.2.0)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.1.105 in /usr/local/lib/python3.12/dist-packages (from torch==2.4.0) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.1.105 in /usr/local/lib/python3.12/dist-packages (from torch==2.4.0) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.1.105 in /usr/local/lib/python3.12/dist-packages (from torch==2.4.0) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /usr/local/lib/python3.12/dist-packages (from torch==2.4.0) (9.1.0.70)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.1.3.1 in /usr/local/lib/python3.12/dist-packages (from torch==2.4.0) (12.1.3.1)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.0.2.54 in /usr/local/lib/python3.12/dist-packages (from torch==2.4.0) (11.0.2.54)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.2.106 in /usr/local/lib/python3.12/dist-packages (from torch==2.4.0) (10.3.2.106)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.4.5.107 in /usr/local/lib/python3.12/dist-packages (from torch==2.4.0) (11.4.5.107)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.1.0.106 in /usr/local/lib/python3.12/dist-packages (from torch==2.4.0) (12.1.0.106)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.20.5 in /usr/local/lib/python3.12/dist-packages (from torch==2.4.0) (2.20.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.1.105 in /usr/local/lib/python3.12/dist-packages (from torch==2.4.0) (12.1.105)\n",
            "Requirement already satisfied: triton==3.0.0 in /usr/local/lib/python3.12/dist-packages (from torch==2.4.0) (3.0.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.12/dist-packages (from torchvision==0.19.0) (2.0.2)\n",
            "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.12/dist-packages (from torchvision==0.19.0) (11.3.0)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12 in /usr/local/lib/python3.12/dist-packages (from nvidia-cusolver-cu12==11.4.5.107->torch==2.4.0) (12.6.85)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2->torch==2.4.0) (3.0.3)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from sympy->torch==2.4.0) (1.3.0)\n",
            "\u001b[33mDEPRECATION: Loading egg at /usr/local/lib/python3.12/dist-packages/pointnet2_cuda-0.0.0-py3.12-linux-x86_64.egg is deprecated. pip 24.3 will enforce this behaviour change. A possible replacement is to use pip for package installation. Discussion can be found at https://github.com/pypa/pip/issues/12330\u001b[0m\u001b[33m\n",
            "\u001b[0m\u001b[33mDEPRECATION: Loading egg at /usr/local/lib/python3.12/dist-packages/emd_ext-0.0.0-py3.12-linux-x86_64.egg is deprecated. pip 24.3 will enforce this behaviour change. A possible replacement is to use pip for package installation. Discussion can be found at https://github.com/pypa/pip/issues/12330\u001b[0m\u001b[33m\n",
            "\u001b[0mLooking in links: https://data.pyg.org/whl/torch-2.4.0+cu121.html\n",
            "Requirement already satisfied: torch-scatter in /usr/local/lib/python3.12/dist-packages (2.1.2+pt24cu121)\n",
            "Requirement already satisfied: torch-sparse in /usr/local/lib/python3.12/dist-packages (0.6.18+pt24cu121)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.12/dist-packages (from torch-sparse) (1.16.3)\n",
            "Requirement already satisfied: numpy<2.6,>=1.25.2 in /usr/local/lib/python3.12/dist-packages (from scipy->torch-sparse) (2.0.2)\n",
            "\u001b[33mDEPRECATION: Loading egg at /usr/local/lib/python3.12/dist-packages/pointnet2_cuda-0.0.0-py3.12-linux-x86_64.egg is deprecated. pip 24.3 will enforce this behaviour change. A possible replacement is to use pip for package installation. Discussion can be found at https://github.com/pypa/pip/issues/12330\u001b[0m\u001b[33m\n",
            "\u001b[0m\u001b[33mDEPRECATION: Loading egg at /usr/local/lib/python3.12/dist-packages/emd_ext-0.0.0-py3.12-linux-x86_64.egg is deprecated. pip 24.3 will enforce this behaviour change. A possible replacement is to use pip for package installation. Discussion can be found at https://github.com/pypa/pip/issues/12330\u001b[0m\u001b[33m\n",
            "\u001b[0mRequirement already satisfied: scikit-learn in /usr/local/lib/python3.12/dist-packages (from -r requirements.txt (line 1)) (1.6.1)\n",
            "Requirement already satisfied: pickleshare in /usr/local/lib/python3.12/dist-packages (from -r requirements.txt (line 2)) (0.7.5)\n",
            "Requirement already satisfied: ninja in /usr/local/lib/python3.12/dist-packages (from -r requirements.txt (line 3)) (1.13.0)\n",
            "Requirement already satisfied: gdown in /usr/local/lib/python3.12/dist-packages (from -r requirements.txt (line 4)) (5.2.0)\n",
            "Requirement already satisfied: easydict in /usr/local/lib/python3.12/dist-packages (from -r requirements.txt (line 5)) (1.13)\n",
            "Requirement already satisfied: PyYAML in /usr/local/lib/python3.12/dist-packages (from -r requirements.txt (line 6)) (6.0.3)\n",
            "Requirement already satisfied: protobuf in /usr/local/lib/python3.12/dist-packages (from -r requirements.txt (line 7)) (5.29.5)\n",
            "Requirement already satisfied: tensorboard in /usr/local/lib/python3.12/dist-packages (from -r requirements.txt (line 8)) (2.19.0)\n",
            "Requirement already satisfied: termcolor in /usr/local/lib/python3.12/dist-packages (from -r requirements.txt (line 9)) (3.2.0)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.12/dist-packages (from -r requirements.txt (line 10)) (4.67.1)\n",
            "Requirement already satisfied: multimethod in /usr/local/lib/python3.12/dist-packages (from -r requirements.txt (line 11)) (2.0.2)\n",
            "Requirement already satisfied: h5py in /usr/local/lib/python3.12/dist-packages (from -r requirements.txt (line 12)) (3.15.1)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.12/dist-packages (from -r requirements.txt (line 13)) (3.10.0)\n",
            "Requirement already satisfied: wandb in /usr/local/lib/python3.12/dist-packages (from -r requirements.txt (line 14)) (0.23.1)\n",
            "Requirement already satisfied: pyvista in /usr/local/lib/python3.12/dist-packages (from -r requirements.txt (line 15)) (0.46.4)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (from -r requirements.txt (line 16)) (75.2.0)\n",
            "Requirement already satisfied: Cython in /usr/local/lib/python3.12/dist-packages (from -r requirements.txt (line 17)) (3.0.12)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.12/dist-packages (from -r requirements.txt (line 18)) (2.2.2)\n",
            "Requirement already satisfied: deepspeed in /usr/local/lib/python3.12/dist-packages (from -r requirements.txt (line 19)) (0.18.3)\n",
            "Requirement already satisfied: shortuuid in /usr/local/lib/python3.12/dist-packages (from -r requirements.txt (line 20)) (1.0.13)\n",
            "Requirement already satisfied: mkdocs-material in /usr/local/lib/python3.12/dist-packages (from -r requirements.txt (line 23)) (9.7.0)\n",
            "Requirement already satisfied: mkdocs-awesome-pages-plugin in /usr/local/lib/python3.12/dist-packages (from -r requirements.txt (line 24)) (2.10.1)\n",
            "Requirement already satisfied: mdx_truly_sane_lists in /usr/local/lib/python3.12/dist-packages (from -r requirements.txt (line 25)) (1.3)\n",
            "Requirement already satisfied: numpy>=1.19.5 in /usr/local/lib/python3.12/dist-packages (from scikit-learn->-r requirements.txt (line 1)) (2.0.2)\n",
            "Requirement already satisfied: scipy>=1.6.0 in /usr/local/lib/python3.12/dist-packages (from scikit-learn->-r requirements.txt (line 1)) (1.16.3)\n",
            "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.12/dist-packages (from scikit-learn->-r requirements.txt (line 1)) (1.5.2)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.12/dist-packages (from scikit-learn->-r requirements.txt (line 1)) (3.6.0)\n",
            "Requirement already satisfied: beautifulsoup4 in /usr/local/lib/python3.12/dist-packages (from gdown->-r requirements.txt (line 4)) (4.13.5)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from gdown->-r requirements.txt (line 4)) (3.20.0)\n",
            "Requirement already satisfied: requests[socks] in /usr/local/lib/python3.12/dist-packages (from gdown->-r requirements.txt (line 4)) (2.32.4)\n",
            "Requirement already satisfied: absl-py>=0.4 in /usr/local/lib/python3.12/dist-packages (from tensorboard->-r requirements.txt (line 8)) (1.4.0)\n",
            "Requirement already satisfied: grpcio>=1.48.2 in /usr/local/lib/python3.12/dist-packages (from tensorboard->-r requirements.txt (line 8)) (1.76.0)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.12/dist-packages (from tensorboard->-r requirements.txt (line 8)) (3.10)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.12/dist-packages (from tensorboard->-r requirements.txt (line 8)) (25.0)\n",
            "Requirement already satisfied: six>1.9 in /usr/local/lib/python3.12/dist-packages (from tensorboard->-r requirements.txt (line 8)) (1.17.0)\n",
            "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /usr/local/lib/python3.12/dist-packages (from tensorboard->-r requirements.txt (line 8)) (0.7.2)\n",
            "Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.12/dist-packages (from tensorboard->-r requirements.txt (line 8)) (3.1.4)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib->-r requirements.txt (line 13)) (1.3.3)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.12/dist-packages (from matplotlib->-r requirements.txt (line 13)) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.12/dist-packages (from matplotlib->-r requirements.txt (line 13)) (4.61.0)\n",
            "Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib->-r requirements.txt (line 13)) (1.4.9)\n",
            "Requirement already satisfied: pillow>=8 in /usr/local/lib/python3.12/dist-packages (from matplotlib->-r requirements.txt (line 13)) (11.3.0)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib->-r requirements.txt (line 13)) (3.2.5)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.12/dist-packages (from matplotlib->-r requirements.txt (line 13)) (2.9.0.post0)\n",
            "Requirement already satisfied: click>=8.0.1 in /usr/local/lib/python3.12/dist-packages (from wandb->-r requirements.txt (line 14)) (8.3.1)\n",
            "Requirement already satisfied: gitpython!=3.1.29,>=1.0.0 in /usr/local/lib/python3.12/dist-packages (from wandb->-r requirements.txt (line 14)) (3.1.45)\n",
            "Requirement already satisfied: platformdirs in /usr/local/lib/python3.12/dist-packages (from wandb->-r requirements.txt (line 14)) (4.5.1)\n",
            "Requirement already satisfied: pydantic<3 in /usr/local/lib/python3.12/dist-packages (from wandb->-r requirements.txt (line 14)) (2.12.3)\n",
            "Requirement already satisfied: sentry-sdk>=2.0.0 in /usr/local/lib/python3.12/dist-packages (from wandb->-r requirements.txt (line 14)) (2.47.0)\n",
            "Requirement already satisfied: typing-extensions<5,>=4.8 in /usr/local/lib/python3.12/dist-packages (from wandb->-r requirements.txt (line 14)) (4.15.0)\n",
            "Requirement already satisfied: pooch in /usr/local/lib/python3.12/dist-packages (from pyvista->-r requirements.txt (line 15)) (1.8.2)\n",
            "Requirement already satisfied: scooby>=0.5.1 in /usr/local/lib/python3.12/dist-packages (from pyvista->-r requirements.txt (line 15)) (0.11.0)\n",
            "Requirement already satisfied: vtk!=9.4.0 in /usr/local/lib/python3.12/dist-packages (from pyvista->-r requirements.txt (line 15)) (9.5.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.12/dist-packages (from pandas->-r requirements.txt (line 18)) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.12/dist-packages (from pandas->-r requirements.txt (line 18)) (2025.2)\n",
            "Requirement already satisfied: einops in /usr/local/lib/python3.12/dist-packages (from deepspeed->-r requirements.txt (line 19)) (0.8.1)\n",
            "Requirement already satisfied: hjson in /usr/local/lib/python3.12/dist-packages (from deepspeed->-r requirements.txt (line 19)) (3.1.0)\n",
            "Requirement already satisfied: msgpack in /usr/local/lib/python3.12/dist-packages (from deepspeed->-r requirements.txt (line 19)) (1.1.2)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.12/dist-packages (from deepspeed->-r requirements.txt (line 19)) (5.9.5)\n",
            "Requirement already satisfied: py-cpuinfo in /usr/local/lib/python3.12/dist-packages (from deepspeed->-r requirements.txt (line 19)) (9.0.0)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.12/dist-packages (from deepspeed->-r requirements.txt (line 19)) (2.4.0+cu121)\n",
            "Requirement already satisfied: nvidia-ml-py in /usr/local/lib/python3.12/dist-packages (from deepspeed->-r requirements.txt (line 19)) (13.590.44)\n",
            "Requirement already satisfied: babel>=2.10 in /usr/local/lib/python3.12/dist-packages (from mkdocs-material->-r requirements.txt (line 23)) (2.17.0)\n",
            "Requirement already satisfied: backrefs>=5.7.post1 in /usr/local/lib/python3.12/dist-packages (from mkdocs-material->-r requirements.txt (line 23)) (6.1)\n",
            "Requirement already satisfied: colorama>=0.4 in /usr/local/lib/python3.12/dist-packages (from mkdocs-material->-r requirements.txt (line 23)) (0.4.6)\n",
            "Requirement already satisfied: jinja2>=3.1 in /usr/local/lib/python3.12/dist-packages (from mkdocs-material->-r requirements.txt (line 23)) (3.1.6)\n",
            "Requirement already satisfied: mkdocs-material-extensions>=1.3 in /usr/local/lib/python3.12/dist-packages (from mkdocs-material->-r requirements.txt (line 23)) (1.3.1)\n",
            "Requirement already satisfied: mkdocs>=1.6 in /usr/local/lib/python3.12/dist-packages (from mkdocs-material->-r requirements.txt (line 23)) (1.6.1)\n",
            "Requirement already satisfied: paginate>=0.5 in /usr/local/lib/python3.12/dist-packages (from mkdocs-material->-r requirements.txt (line 23)) (0.5.7)\n",
            "Requirement already satisfied: pygments>=2.16 in /usr/local/lib/python3.12/dist-packages (from mkdocs-material->-r requirements.txt (line 23)) (2.19.2)\n",
            "Requirement already satisfied: pymdown-extensions>=10.2 in /usr/local/lib/python3.12/dist-packages (from mkdocs-material->-r requirements.txt (line 23)) (10.19)\n",
            "Requirement already satisfied: natsort>=8.1.0 in /usr/local/lib/python3.12/dist-packages (from mkdocs-awesome-pages-plugin->-r requirements.txt (line 24)) (8.4.0)\n",
            "Requirement already satisfied: wcmatch>=7 in /usr/local/lib/python3.12/dist-packages (from mkdocs-awesome-pages-plugin->-r requirements.txt (line 24)) (10.1)\n",
            "Requirement already satisfied: gitdb<5,>=4.0.1 in /usr/local/lib/python3.12/dist-packages (from gitpython!=3.1.29,>=1.0.0->wandb->-r requirements.txt (line 14)) (4.0.12)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2>=3.1->mkdocs-material->-r requirements.txt (line 23)) (3.0.3)\n",
            "Requirement already satisfied: ghp-import>=1.0 in /usr/local/lib/python3.12/dist-packages (from mkdocs>=1.6->mkdocs-material->-r requirements.txt (line 23)) (2.1.0)\n",
            "Requirement already satisfied: mergedeep>=1.3.4 in /usr/local/lib/python3.12/dist-packages (from mkdocs>=1.6->mkdocs-material->-r requirements.txt (line 23)) (1.3.4)\n",
            "Requirement already satisfied: mkdocs-get-deps>=0.2.0 in /usr/local/lib/python3.12/dist-packages (from mkdocs>=1.6->mkdocs-material->-r requirements.txt (line 23)) (0.2.0)\n",
            "Requirement already satisfied: pathspec>=0.11.1 in /usr/local/lib/python3.12/dist-packages (from mkdocs>=1.6->mkdocs-material->-r requirements.txt (line 23)) (0.12.1)\n",
            "Requirement already satisfied: pyyaml-env-tag>=0.1 in /usr/local/lib/python3.12/dist-packages (from mkdocs>=1.6->mkdocs-material->-r requirements.txt (line 23)) (1.1)\n",
            "Requirement already satisfied: watchdog>=2.0 in /usr/local/lib/python3.12/dist-packages (from mkdocs>=1.6->mkdocs-material->-r requirements.txt (line 23)) (6.0.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.12/dist-packages (from pydantic<3->wandb->-r requirements.txt (line 14)) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.41.4 in /usr/local/lib/python3.12/dist-packages (from pydantic<3->wandb->-r requirements.txt (line 14)) (2.41.4)\n",
            "Requirement already satisfied: typing-inspection>=0.4.2 in /usr/local/lib/python3.12/dist-packages (from pydantic<3->wandb->-r requirements.txt (line 14)) (0.4.2)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests[socks]->gdown->-r requirements.txt (line 4)) (3.4.4)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests[socks]->gdown->-r requirements.txt (line 4)) (3.11)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests[socks]->gdown->-r requirements.txt (line 4)) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests[socks]->gdown->-r requirements.txt (line 4)) (2025.11.12)\n",
            "Requirement already satisfied: bracex>=2.1.1 in /usr/local/lib/python3.12/dist-packages (from wcmatch>=7->mkdocs-awesome-pages-plugin->-r requirements.txt (line 24)) (2.6)\n",
            "Requirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.12/dist-packages (from beautifulsoup4->gdown->-r requirements.txt (line 4)) (2.8)\n",
            "Requirement already satisfied: PySocks!=1.5.7,>=1.5.6 in /usr/local/lib/python3.12/dist-packages (from requests[socks]->gdown->-r requirements.txt (line 4)) (1.7.1)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.12/dist-packages (from torch->deepspeed->-r requirements.txt (line 19)) (1.14.0)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.12/dist-packages (from torch->deepspeed->-r requirements.txt (line 19)) (3.6.1)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.12/dist-packages (from torch->deepspeed->-r requirements.txt (line 19)) (2025.3.0)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.1.105 in /usr/local/lib/python3.12/dist-packages (from torch->deepspeed->-r requirements.txt (line 19)) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.1.105 in /usr/local/lib/python3.12/dist-packages (from torch->deepspeed->-r requirements.txt (line 19)) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.1.105 in /usr/local/lib/python3.12/dist-packages (from torch->deepspeed->-r requirements.txt (line 19)) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /usr/local/lib/python3.12/dist-packages (from torch->deepspeed->-r requirements.txt (line 19)) (9.1.0.70)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.1.3.1 in /usr/local/lib/python3.12/dist-packages (from torch->deepspeed->-r requirements.txt (line 19)) (12.1.3.1)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.0.2.54 in /usr/local/lib/python3.12/dist-packages (from torch->deepspeed->-r requirements.txt (line 19)) (11.0.2.54)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.2.106 in /usr/local/lib/python3.12/dist-packages (from torch->deepspeed->-r requirements.txt (line 19)) (10.3.2.106)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.4.5.107 in /usr/local/lib/python3.12/dist-packages (from torch->deepspeed->-r requirements.txt (line 19)) (11.4.5.107)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.1.0.106 in /usr/local/lib/python3.12/dist-packages (from torch->deepspeed->-r requirements.txt (line 19)) (12.1.0.106)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.20.5 in /usr/local/lib/python3.12/dist-packages (from torch->deepspeed->-r requirements.txt (line 19)) (2.20.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.1.105 in /usr/local/lib/python3.12/dist-packages (from torch->deepspeed->-r requirements.txt (line 19)) (12.1.105)\n",
            "Requirement already satisfied: triton==3.0.0 in /usr/local/lib/python3.12/dist-packages (from torch->deepspeed->-r requirements.txt (line 19)) (3.0.0)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12 in /usr/local/lib/python3.12/dist-packages (from nvidia-cusolver-cu12==11.4.5.107->torch->deepspeed->-r requirements.txt (line 19)) (12.6.85)\n",
            "Requirement already satisfied: smmap<6,>=3.0.1 in /usr/local/lib/python3.12/dist-packages (from gitdb<5,>=4.0.1->gitpython!=3.1.29,>=1.0.0->wandb->-r requirements.txt (line 14)) (5.0.2)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from sympy->torch->deepspeed->-r requirements.txt (line 19)) (1.3.0)\n",
            "‚úÖ Dependencies Installed. (Skipped setup.py)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 1. Search for the missing setup.py\n",
        "print(\"Searching for C++ kernel setup.py...\")\n",
        "target_dir = \"/content/PointNeXt/openpoints/cpp\"\n",
        "found_setup = False\n",
        "\n",
        "for root, dirs, files in os.walk(target_dir):\n",
        "    if \"setup.py\" in files:\n",
        "        print(f\"\\u2705 Found setup.py at: {root}\")\n",
        "        found_setup = True\n",
        "\n",
        "        # 2. Force Compile\n",
        "        print(f\"Compiling kernels in {root}...\")\n",
        "        try:\n",
        "            subprocess.check_call([sys.executable, \"setup.py\", \"install\"], cwd=root)\n",
        "            print(\"Compilation Successful!\")\n",
        "        except subprocess.CalledProcessError as e:\n",
        "            print(f\"\\u274c Compilation Failed: {e}\")\n",
        "\n",
        "if not found_setup:\n",
        "    print(\"\\u274c Critical Error: Could not find setup.py anywhere in openpoints/cpp!\")\n",
        "    print(\"Did the 'git submodule update' step finish successfully?\")"
      ],
      "metadata": {
        "id": "0FjSoHiNPWvH",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f3176efb-e9e3-44fc-99d1-a2e94148f7c6"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Searching for C++ kernel setup.py...\n",
            "‚úÖ Found setup.py at: /content/PointNeXt/openpoints/cpp/pointnet2_batch\n",
            "Compiling kernels in /content/PointNeXt/openpoints/cpp/pointnet2_batch...\n",
            "Compilation Successful!\n",
            "‚úÖ Found setup.py at: /content/PointNeXt/openpoints/cpp/emd\n",
            "Compiling kernels in /content/PointNeXt/openpoints/cpp/emd...\n",
            "Compilation Successful!\n",
            "‚úÖ Found setup.py at: /content/PointNeXt/openpoints/cpp/pointops\n",
            "Compiling kernels in /content/PointNeXt/openpoints/cpp/pointops...\n",
            "Compilation Successful!\n",
            "‚úÖ Found setup.py at: /content/PointNeXt/openpoints/cpp/subsampling\n",
            "Compiling kernels in /content/PointNeXt/openpoints/cpp/subsampling...\n",
            "‚ùå Compilation Failed: Command '['/usr/bin/python3', 'setup.py', 'install']' returned non-zero exit status 1.\n",
            "‚úÖ Found setup.py at: /content/PointNeXt/openpoints/cpp/chamfer_dist\n",
            "Compiling kernels in /content/PointNeXt/openpoints/cpp/chamfer_dist...\n",
            "Compilation Successful!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Fix Missing Validation List\n",
        "\n",
        "split_dir = \"/content/processed_data/train_test_split\"\n",
        "test_file = os.path.join(split_dir, \"shuffled_test_file_list.json\")\n",
        "val_file = os.path.join(split_dir, \"shuffled_val_file_list.json\")\n",
        "\n",
        "print(\"Checking validation list...\")\n",
        "\n",
        "if os.path.exists(test_file):\n",
        "    if not os.path.exists(val_file):\n",
        "        print(\"Creating dummy validation list (copy of test list)...\")\n",
        "        shutil.copy(test_file, val_file)\n",
        "        print(f\"\\u2705 Created: {val_file}\")\n",
        "    else:\n",
        "        print(\"\\u2705 Validation list already exists.\")\n",
        "else:\n",
        "    print(\"\\u274c Error: Test list not found! Did the previous copy step work?\")"
      ],
      "metadata": {
        "id": "5CMDNVe9o1HH",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e2386a83-7695-4889-f7d3-7498fae9c314"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Checking validation list...\n",
            "‚úÖ Validation list already exists.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Train On shapenetpart"
      ],
      "metadata": {
        "id": "veAsjknFPqV0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Patch main.py to fix Tensor Key Error\n",
        "import os\n",
        "\n",
        "target_file = \"/content/PointNeXt/examples/shapenetpart/main.py\"\n",
        "print(f\"Patching {target_file} to fix Tensor Key Error...\")\n",
        "\n",
        "with open(target_file, 'r') as f:\n",
        "    code = f.read()\n",
        "\n",
        "bad_line = \"parts = cls2parts[cls[shape_idx]]\"\n",
        "good_line = \"parts = cls2parts[int(cls[shape_idx])]\"\n",
        "\n",
        "if bad_line in code:\n",
        "    code = code.replace(bad_line, good_line)\n",
        "    print(\"\\u2705 Patched: Cast tensor to int for dictionary lookup.\")\n",
        "else:\n",
        "    print(\"\\u26A0\\uFE0F Warning: Could not find exact line. Check if file changed.\")\n",
        "\n",
        "with open(target_file, 'w') as f:\n",
        "    f.write(code)"
      ],
      "metadata": {
        "id": "sNkJ6uQWPwRs",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "bf4f58f4-8797-4c62-c710-bcaccd146759"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Patching /content/PointNeXt/examples/shapenetpart/main.py to fix Tensor Key Error...\n",
            "‚úÖ Patched: Cast tensor to int for dictionary lookup.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# NOW Run Training\n",
        "print(\"\\nStarting Training...\")\n",
        "%cd /content/PointNeXt\n",
        "!PYTHONPATH=. python examples/shapenetpart/main.py --cfg cfgs/shapenetpart/custom_fallen_trees.yaml mode=train"
      ],
      "metadata": {
        "id": "yTC1v-MNyjtd",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2c27e8da-e180-42f5-c718-197e9a54aa26"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Starting Training...\n",
            "/content/PointNeXt\n",
            "2025-12-12 15:53:26.788935: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
            "E0000 00:00:1765554807.025388    8116 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "E0000 00:00:1765554807.090820    8116 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "W0000 00:00:1765554807.572241    8116 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1765554807.572284    8116 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1765554807.572289    8116 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1765554807.572293    8116 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "2025-12-12 15:53:27.616311: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
            "To enable the following instructions: AVX2 AVX512F FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "/content/PointNeXt/openpoints/models/backbone/pointnetv2.py:79: SyntaxWarning: invalid escape sequence '\\s'\n",
            "  new_features: (B, \\sum_k(mlps[k][-1], npoint)) tensor of the new_features descriptors\n",
            "/content/PointNeXt/openpoints/models/classification/cls_base.py:99: SyntaxWarning: invalid escape sequence '\\e'\n",
            "  $\\eg$ cls_feat='max,avg' means use the concatenateion of maxpooled and avgpooled features.\n",
            "launch mp with 1 GPUs, current rank: 0\n",
            "\u001b[32m[12/12 15:53:40 FallenTreePart]: \u001b[0mdist_url: tcp://localhost:8888\n",
            "dist_backend: nccl\n",
            "multiprocessing_distributed: False\n",
            "ngpus_per_node: 1\n",
            "world_size: 1\n",
            "launcher: mp\n",
            "local_rank: 0\n",
            "use_gpu: True\n",
            "seed: 7520\n",
            "epoch: 0\n",
            "epochs: 100\n",
            "ignore_index: None\n",
            "val_fn: validate\n",
            "deterministic: False\n",
            "sync_bn: False\n",
            "criterion_args:\n",
            "  NAME: Poly1FocalLoss\n",
            "use_mask: False\n",
            "grad_norm_clip: 1\n",
            "layer_decay: 0\n",
            "step_per_update: 1\n",
            "start_epoch: 1\n",
            "sched_on_epoch: True\n",
            "wandb:\n",
            "  use_wandb: False\n",
            "  project: PointNext-ShapeNetPart\n",
            "  tags: ['shapenetpart', 'train', 'custom_fallen_trees', 'ngpus1', 'seed7520']\n",
            "  name: shapenetpart-train-custom_fallen_trees-ngpus1-seed7520-20251212-155340-E84zcZ3REWTnK2wbniefub\n",
            "use_amp: False\n",
            "use_voting: False\n",
            "val_freq: 1\n",
            "resume: False\n",
            "test: False\n",
            "finetune: False\n",
            "mode: train\n",
            "logname: None\n",
            "load_path: None\n",
            "print_freq: 10\n",
            "save_freq: -1\n",
            "root_dir: log/shapenetpart\n",
            "pretrained_path: None\n",
            "datatransforms:\n",
            "  train: ['PointsToTensor', 'PointCloudScaling', 'PointCloudCenterAndNormalize', 'PointCloudJitter', 'ChromaticDropGPU']\n",
            "  val: ['PointsToTensor', 'PointCloudCenterAndNormalize']\n",
            "  vote: ['PointCloudScaling']\n",
            "  kwargs:\n",
            "    jitter_sigma: 0.001\n",
            "    jitter_clip: 0.005\n",
            "    scale: [0.8, 1.2]\n",
            "    gravity_dim: 1\n",
            "    angle: [0, 1.0, 0]\n",
            "feature_keys: pos,x\n",
            "dataset:\n",
            "  common:\n",
            "    NAME: FallenTreePart\n",
            "    data_root: /content/processed_data\n",
            "    use_normal: False\n",
            "    num_points: 2048\n",
            "    use_xyz: True\n",
            "  train:\n",
            "    split: train\n",
            "  val:\n",
            "    split: val\n",
            "    presample: True\n",
            "num_classes: 4\n",
            "shape_classes: 2\n",
            "num_points: 2048\n",
            "normal_channel: True\n",
            "batch_size: 16\n",
            "dataloader:\n",
            "  num_workers: 4\n",
            "num_votes: 10\n",
            "refine: True\n",
            "lr: 0.001\n",
            "min_lr: None\n",
            "optimizer:\n",
            "  NAME: adamw\n",
            "  weight_decay: 0.0001\n",
            "sched: multistep\n",
            "decay_epochs: [70, 90]\n",
            "decay_rate: 0.1\n",
            "warmup_epochs: 0\n",
            "model:\n",
            "  NAME: BasePartSeg\n",
            "  encoder_args:\n",
            "    NAME: PointNextEncoder\n",
            "    blocks: [1, 1, 1, 1, 1]\n",
            "    strides: [1, 2, 2, 2, 2]\n",
            "    width: 32\n",
            "    in_channels: 7\n",
            "    sa_layers: 3\n",
            "    sa_use_res: True\n",
            "    radius: 0.1\n",
            "    radius_scaling: 2.5\n",
            "    nsample: 32\n",
            "    expansion: 4\n",
            "    aggr_args:\n",
            "      feature_type: dp_fj\n",
            "    reduction: max\n",
            "    group_args:\n",
            "      NAME: ballquery\n",
            "      normalize_dp: True\n",
            "    conv_args:\n",
            "      order: conv-norm-act\n",
            "    act_args:\n",
            "      act: relu\n",
            "    norm_args:\n",
            "      norm: bn\n",
            "  decoder_args:\n",
            "    NAME: PointNextPartDecoder\n",
            "    cls_map: curvenet\n",
            "  cls_args:\n",
            "    NAME: SegHead\n",
            "    global_feat: max,avg\n",
            "    num_classes: 4\n",
            "    shape_classes: 2\n",
            "    in_channels: None\n",
            "    norm_args:\n",
            "      norm: bn\n",
            "log_dir: log/shapenetpart/shapenetpart-train-custom_fallen_trees-ngpus1-seed7520-20251212-155340-E84zcZ3REWTnK2wbniefub\n",
            "rank: 0\n",
            "distributed: False\n",
            "mp: False\n",
            "task_name: shapenetpart\n",
            "cfg_basename: custom_fallen_trees\n",
            "opts: mode=train\n",
            "is_training: True\n",
            "run_name: shapenetpart-train-custom_fallen_trees-ngpus1-seed7520-20251212-155340-E84zcZ3REWTnK2wbniefub\n",
            "run_dir: log/shapenetpart/shapenetpart-train-custom_fallen_trees-ngpus1-seed7520-20251212-155340-E84zcZ3REWTnK2wbniefub\n",
            "exp_dir: log/shapenetpart/shapenetpart-train-custom_fallen_trees-ngpus1-seed7520-20251212-155340-E84zcZ3REWTnK2wbniefub\n",
            "ckpt_dir: log/shapenetpart/shapenetpart-train-custom_fallen_trees-ngpus1-seed7520-20251212-155340-E84zcZ3REWTnK2wbniefub/checkpoint\n",
            "log_path: log/shapenetpart/shapenetpart-train-custom_fallen_trees-ngpus1-seed7520-20251212-155340-E84zcZ3REWTnK2wbniefub/shapenetpart-train-custom_fallen_trees-ngpus1-seed7520-20251212-155340-E84zcZ3REWTnK2wbniefub.log\n",
            "cfg_path: log/shapenetpart/shapenetpart-train-custom_fallen_trees-ngpus1-seed7520-20251212-155340-E84zcZ3REWTnK2wbniefub/cfg.yaml\n",
            "\u001b[32m[12/12 15:53:40 FallenTreePart]: \u001b[0mLoading val split from: /content/processed_data/train_test_split/shuffled_val_file_list.json\n",
            "\u001b[32m[12/12 15:53:40 FallenTreePart]: \u001b[0mFound 128 valid files for val split.\n",
            "\u001b[32m[12/12 15:53:40 FallenTreePart]: \u001b[0mlength of validation dataset: 128\n",
            "\u001b[32m[12/12 15:53:40 FallenTreePart]: \u001b[0mnumber of classes of the dataset: 4\n",
            "\u001b[32m[12/12 15:53:41 FallenTreePart]: \u001b[0mradius: [[0.1], [0.1], [0.25], [0.625], [1.5625]],\n",
            " nsample: [[32], [32], [32], [32], [32]]\n",
            "\u001b[32m[12/12 15:53:41 FallenTreePart]: \u001b[0mNAME: ballquery\n",
            "normalize_dp: True\n",
            "radius: 0.1\n",
            "nsample: 32\n",
            "\u001b[32m[12/12 15:53:41 FallenTreePart]: \u001b[0mNAME: ballquery\n",
            "normalize_dp: True\n",
            "radius: 0.25\n",
            "nsample: 32\n",
            "\u001b[32m[12/12 15:53:41 FallenTreePart]: \u001b[0mNAME: ballquery\n",
            "normalize_dp: True\n",
            "radius: 0.625\n",
            "nsample: 32\n",
            "\u001b[32m[12/12 15:53:41 FallenTreePart]: \u001b[0mNAME: ballquery\n",
            "normalize_dp: True\n",
            "radius: 1.5625\n",
            "nsample: 32\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m[12/12 15:53:41 FallenTreePart]: \u001b[0mkwargs: {'shape_classes': 2} are not used in SegHead\n",
            "\u001b[32m[12/12 15:53:41 FallenTreePart]: \u001b[0mBasePartSeg(\n",
            "  (encoder): PointNextEncoder(\n",
            "    (encoder): Sequential(\n",
            "      (0): Sequential(\n",
            "        (0): SetAbstraction(\n",
            "          (convs): Sequential(\n",
            "            (0): Sequential(\n",
            "              (0): Conv1d(7, 32, kernel_size=(1,), stride=(1,))\n",
            "            )\n",
            "          )\n",
            "        )\n",
            "      )\n",
            "      (1): Sequential(\n",
            "        (0): SetAbstraction(\n",
            "          (skipconv): Sequential(\n",
            "            (0): Conv1d(32, 64, kernel_size=(1,), stride=(1,))\n",
            "          )\n",
            "          (act): ReLU(inplace=True)\n",
            "          (convs): Sequential(\n",
            "            (0): Sequential(\n",
            "              (0): Conv2d(35, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "              (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "              (2): ReLU(inplace=True)\n",
            "            )\n",
            "            (1): Sequential(\n",
            "              (0): Conv2d(32, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "              (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "              (2): ReLU(inplace=True)\n",
            "            )\n",
            "            (2): Sequential(\n",
            "              (0): Conv2d(32, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "              (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "            )\n",
            "          )\n",
            "          (grouper): QueryAndGroup()\n",
            "        )\n",
            "      )\n",
            "      (2): Sequential(\n",
            "        (0): SetAbstraction(\n",
            "          (skipconv): Sequential(\n",
            "            (0): Conv1d(64, 128, kernel_size=(1,), stride=(1,))\n",
            "          )\n",
            "          (act): ReLU(inplace=True)\n",
            "          (convs): Sequential(\n",
            "            (0): Sequential(\n",
            "              (0): Conv2d(67, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "              (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "              (2): ReLU(inplace=True)\n",
            "            )\n",
            "            (1): Sequential(\n",
            "              (0): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "              (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "              (2): ReLU(inplace=True)\n",
            "            )\n",
            "            (2): Sequential(\n",
            "              (0): Conv2d(64, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "              (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "            )\n",
            "          )\n",
            "          (grouper): QueryAndGroup()\n",
            "        )\n",
            "      )\n",
            "      (3): Sequential(\n",
            "        (0): SetAbstraction(\n",
            "          (skipconv): Sequential(\n",
            "            (0): Conv1d(128, 256, kernel_size=(1,), stride=(1,))\n",
            "          )\n",
            "          (act): ReLU(inplace=True)\n",
            "          (convs): Sequential(\n",
            "            (0): Sequential(\n",
            "              (0): Conv2d(131, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "              (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "              (2): ReLU(inplace=True)\n",
            "            )\n",
            "            (1): Sequential(\n",
            "              (0): Conv2d(128, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "              (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "              (2): ReLU(inplace=True)\n",
            "            )\n",
            "            (2): Sequential(\n",
            "              (0): Conv2d(128, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "              (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "            )\n",
            "          )\n",
            "          (grouper): QueryAndGroup()\n",
            "        )\n",
            "      )\n",
            "      (4): Sequential(\n",
            "        (0): SetAbstraction(\n",
            "          (skipconv): Sequential(\n",
            "            (0): Conv1d(256, 512, kernel_size=(1,), stride=(1,))\n",
            "          )\n",
            "          (act): ReLU(inplace=True)\n",
            "          (convs): Sequential(\n",
            "            (0): Sequential(\n",
            "              (0): Conv2d(259, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "              (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "              (2): ReLU(inplace=True)\n",
            "            )\n",
            "            (1): Sequential(\n",
            "              (0): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "              (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "              (2): ReLU(inplace=True)\n",
            "            )\n",
            "            (2): Sequential(\n",
            "              (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "              (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "            )\n",
            "          )\n",
            "          (grouper): QueryAndGroup()\n",
            "        )\n",
            "      )\n",
            "    )\n",
            "  )\n",
            "  (decoder): PointNextPartDecoder(\n",
            "    (global_conv2): Sequential(\n",
            "      (0): Sequential(\n",
            "        (0): Conv1d(512, 128, kernel_size=(1,), stride=(1,))\n",
            "        (1): ReLU(inplace=True)\n",
            "      )\n",
            "    )\n",
            "    (global_conv1): Sequential(\n",
            "      (0): Sequential(\n",
            "        (0): Conv1d(256, 64, kernel_size=(1,), stride=(1,))\n",
            "        (1): ReLU(inplace=True)\n",
            "      )\n",
            "    )\n",
            "    (decoder): Sequential(\n",
            "      (0): Sequential(\n",
            "        (0): FeaturePropogation(\n",
            "          (convs): Sequential(\n",
            "            (0): Sequential(\n",
            "              (0): Conv1d(304, 32, kernel_size=(1,), stride=(1,), bias=False)\n",
            "              (1): BatchNorm1d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "              (2): ReLU(inplace=True)\n",
            "            )\n",
            "            (1): Sequential(\n",
            "              (0): Conv1d(32, 32, kernel_size=(1,), stride=(1,), bias=False)\n",
            "              (1): BatchNorm1d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "              (2): ReLU(inplace=True)\n",
            "            )\n",
            "          )\n",
            "        )\n",
            "      )\n",
            "      (1): Sequential(\n",
            "        (0): FeaturePropogation(\n",
            "          (convs): Sequential(\n",
            "            (0): Sequential(\n",
            "              (0): Conv1d(192, 64, kernel_size=(1,), stride=(1,), bias=False)\n",
            "              (1): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "              (2): ReLU(inplace=True)\n",
            "            )\n",
            "            (1): Sequential(\n",
            "              (0): Conv1d(64, 64, kernel_size=(1,), stride=(1,), bias=False)\n",
            "              (1): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "              (2): ReLU(inplace=True)\n",
            "            )\n",
            "          )\n",
            "        )\n",
            "      )\n",
            "      (2): Sequential(\n",
            "        (0): FeaturePropogation(\n",
            "          (convs): Sequential(\n",
            "            (0): Sequential(\n",
            "              (0): Conv1d(384, 128, kernel_size=(1,), stride=(1,), bias=False)\n",
            "              (1): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "              (2): ReLU(inplace=True)\n",
            "            )\n",
            "            (1): Sequential(\n",
            "              (0): Conv1d(128, 128, kernel_size=(1,), stride=(1,), bias=False)\n",
            "              (1): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "              (2): ReLU(inplace=True)\n",
            "            )\n",
            "          )\n",
            "        )\n",
            "      )\n",
            "      (3): Sequential(\n",
            "        (0): FeaturePropogation(\n",
            "          (convs): Sequential(\n",
            "            (0): Sequential(\n",
            "              (0): Conv1d(768, 256, kernel_size=(1,), stride=(1,), bias=False)\n",
            "              (1): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "              (2): ReLU(inplace=True)\n",
            "            )\n",
            "            (1): Sequential(\n",
            "              (0): Conv1d(256, 256, kernel_size=(1,), stride=(1,), bias=False)\n",
            "              (1): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "              (2): ReLU(inplace=True)\n",
            "            )\n",
            "          )\n",
            "        )\n",
            "      )\n",
            "    )\n",
            "  )\n",
            "  (head): SegHead(\n",
            "    (head): Sequential(\n",
            "      (0): Sequential(\n",
            "        (0): Conv1d(96, 96, kernel_size=(1,), stride=(1,), bias=False)\n",
            "        (1): BatchNorm1d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (2): ReLU(inplace=True)\n",
            "      )\n",
            "      (1): Dropout(p=0.5, inplace=False)\n",
            "      (2): Sequential(\n",
            "        (0): Conv1d(96, 4, kernel_size=(1,), stride=(1,))\n",
            "      )\n",
            "    )\n",
            "  )\n",
            ")\n",
            "\u001b[32m[12/12 15:53:41 FallenTreePart]: \u001b[0mNumber of params: 0.9774 M\n",
            "\u001b[32m[12/12 15:53:41 FallenTreePart]: \u001b[0mParam groups = {\n",
            "  \"decay\": {\n",
            "    \"weight_decay\": 0.0001,\n",
            "    \"params\": [\n",
            "      \"encoder.encoder.0.0.convs.0.0.weight\",\n",
            "      \"encoder.encoder.1.0.skipconv.0.weight\",\n",
            "      \"encoder.encoder.1.0.convs.0.0.weight\",\n",
            "      \"encoder.encoder.1.0.convs.1.0.weight\",\n",
            "      \"encoder.encoder.1.0.convs.2.0.weight\",\n",
            "      \"encoder.encoder.2.0.skipconv.0.weight\",\n",
            "      \"encoder.encoder.2.0.convs.0.0.weight\",\n",
            "      \"encoder.encoder.2.0.convs.1.0.weight\",\n",
            "      \"encoder.encoder.2.0.convs.2.0.weight\",\n",
            "      \"encoder.encoder.3.0.skipconv.0.weight\",\n",
            "      \"encoder.encoder.3.0.convs.0.0.weight\",\n",
            "      \"encoder.encoder.3.0.convs.1.0.weight\",\n",
            "      \"encoder.encoder.3.0.convs.2.0.weight\",\n",
            "      \"encoder.encoder.4.0.skipconv.0.weight\",\n",
            "      \"encoder.encoder.4.0.convs.0.0.weight\",\n",
            "      \"encoder.encoder.4.0.convs.1.0.weight\",\n",
            "      \"encoder.encoder.4.0.convs.2.0.weight\",\n",
            "      \"decoder.global_conv2.0.0.weight\",\n",
            "      \"decoder.global_conv1.0.0.weight\",\n",
            "      \"decoder.decoder.0.0.convs.0.0.weight\",\n",
            "      \"decoder.decoder.0.0.convs.1.0.weight\",\n",
            "      \"decoder.decoder.1.0.convs.0.0.weight\",\n",
            "      \"decoder.decoder.1.0.convs.1.0.weight\",\n",
            "      \"decoder.decoder.2.0.convs.0.0.weight\",\n",
            "      \"decoder.decoder.2.0.convs.1.0.weight\",\n",
            "      \"decoder.decoder.3.0.convs.0.0.weight\",\n",
            "      \"decoder.decoder.3.0.convs.1.0.weight\",\n",
            "      \"head.head.0.0.weight\",\n",
            "      \"head.head.2.0.weight\"\n",
            "    ],\n",
            "    \"lr_scale\": 1.0\n",
            "  },\n",
            "  \"no_decay\": {\n",
            "    \"weight_decay\": 0.0,\n",
            "    \"params\": [\n",
            "      \"encoder.encoder.0.0.convs.0.0.bias\",\n",
            "      \"encoder.encoder.1.0.skipconv.0.bias\",\n",
            "      \"encoder.encoder.1.0.convs.0.1.weight\",\n",
            "      \"encoder.encoder.1.0.convs.0.1.bias\",\n",
            "      \"encoder.encoder.1.0.convs.1.1.weight\",\n",
            "      \"encoder.encoder.1.0.convs.1.1.bias\",\n",
            "      \"encoder.encoder.1.0.convs.2.1.weight\",\n",
            "      \"encoder.encoder.1.0.convs.2.1.bias\",\n",
            "      \"encoder.encoder.2.0.skipconv.0.bias\",\n",
            "      \"encoder.encoder.2.0.convs.0.1.weight\",\n",
            "      \"encoder.encoder.2.0.convs.0.1.bias\",\n",
            "      \"encoder.encoder.2.0.convs.1.1.weight\",\n",
            "      \"encoder.encoder.2.0.convs.1.1.bias\",\n",
            "      \"encoder.encoder.2.0.convs.2.1.weight\",\n",
            "      \"encoder.encoder.2.0.convs.2.1.bias\",\n",
            "      \"encoder.encoder.3.0.skipconv.0.bias\",\n",
            "      \"encoder.encoder.3.0.convs.0.1.weight\",\n",
            "      \"encoder.encoder.3.0.convs.0.1.bias\",\n",
            "      \"encoder.encoder.3.0.convs.1.1.weight\",\n",
            "      \"encoder.encoder.3.0.convs.1.1.bias\",\n",
            "      \"encoder.encoder.3.0.convs.2.1.weight\",\n",
            "      \"encoder.encoder.3.0.convs.2.1.bias\",\n",
            "      \"encoder.encoder.4.0.skipconv.0.bias\",\n",
            "      \"encoder.encoder.4.0.convs.0.1.weight\",\n",
            "      \"encoder.encoder.4.0.convs.0.1.bias\",\n",
            "      \"encoder.encoder.4.0.convs.1.1.weight\",\n",
            "      \"encoder.encoder.4.0.convs.1.1.bias\",\n",
            "      \"encoder.encoder.4.0.convs.2.1.weight\",\n",
            "      \"encoder.encoder.4.0.convs.2.1.bias\",\n",
            "      \"decoder.global_conv2.0.0.bias\",\n",
            "      \"decoder.global_conv1.0.0.bias\",\n",
            "      \"decoder.decoder.0.0.convs.0.1.weight\",\n",
            "      \"decoder.decoder.0.0.convs.0.1.bias\",\n",
            "      \"decoder.decoder.0.0.convs.1.1.weight\",\n",
            "      \"decoder.decoder.0.0.convs.1.1.bias\",\n",
            "      \"decoder.decoder.1.0.convs.0.1.weight\",\n",
            "      \"decoder.decoder.1.0.convs.0.1.bias\",\n",
            "      \"decoder.decoder.1.0.convs.1.1.weight\",\n",
            "      \"decoder.decoder.1.0.convs.1.1.bias\",\n",
            "      \"decoder.decoder.2.0.convs.0.1.weight\",\n",
            "      \"decoder.decoder.2.0.convs.0.1.bias\",\n",
            "      \"decoder.decoder.2.0.convs.1.1.weight\",\n",
            "      \"decoder.decoder.2.0.convs.1.1.bias\",\n",
            "      \"decoder.decoder.3.0.convs.0.1.weight\",\n",
            "      \"decoder.decoder.3.0.convs.0.1.bias\",\n",
            "      \"decoder.decoder.3.0.convs.1.1.weight\",\n",
            "      \"decoder.decoder.3.0.convs.1.1.bias\",\n",
            "      \"head.head.0.1.weight\",\n",
            "      \"head.head.0.1.bias\",\n",
            "      \"head.head.2.0.bias\"\n",
            "    ],\n",
            "    \"lr_scale\": 1.0\n",
            "  }\n",
            "}\n",
            "\u001b[32m[12/12 15:53:41 FallenTreePart]: \u001b[0mTraining from scratch\n",
            "\u001b[32m[12/12 15:53:41 FallenTreePart]: \u001b[0mLoading train split from: /content/processed_data/train_test_split/shuffled_train_file_list.json\n",
            "\u001b[32m[12/12 15:53:41 FallenTreePart]: \u001b[0mFound 721 valid files for train split.\n",
            "\u001b[32m[12/12 15:53:41 FallenTreePart]: \u001b[0mlength of training dataset: 721\n",
            "Train Epoch [1/100] Loss 0.086 : 100% 45/45 [00:16<00:00,  2.77it/s]\n",
            "100% 8/8 [00:00<00:00,  9.39it/s]\n",
            "\u001b[32m[12/12 15:53:58 FallenTreePart]: \u001b[0mTest Epoch [1/100],Instance mIoU 86.07, Class mIoU 84.78, \n",
            " Class mIoUs tensor([98.5714, 70.9824], device='cuda:0')\n",
            "\u001b[32m[12/12 15:53:58 FallenTreePart]: \u001b[0mFind a better ckpt @E1, val_ins_miou 86.07 val_cls_miou 84.78, \n",
            "cls_mious: tensor([98.5714, 70.9824], device='cuda:0')\n",
            "\u001b[32m[12/12 15:53:58 FallenTreePart]: \u001b[0mFound the best model and saved in log/shapenetpart/shapenetpart-train-custom_fallen_trees-ngpus1-seed7520-20251212-155340-E84zcZ3REWTnK2wbniefub/checkpoint/shapenetpart-train-custom_fallen_trees-ngpus1-seed7520-20251212-155340-E84zcZ3REWTnK2wbniefub_ckpt_best.pth\n",
            "Train Epoch [2/100] Loss 0.041 : 100% 45/45 [00:07<00:00,  6.15it/s]\n",
            "100% 8/8 [00:00<00:00, 10.73it/s]\n",
            "\u001b[32m[12/12 15:54:07 FallenTreePart]: \u001b[0mTest Epoch [2/100],Instance mIoU 85.04, Class mIoU 83.49, \n",
            " Class mIoUs tensor([100.0000,  66.9783], device='cuda:0')\n",
            "Train Epoch [3/100] Loss 0.032 : 100% 45/45 [00:07<00:00,  6.14it/s]\n",
            "100% 8/8 [00:00<00:00, 10.67it/s]\n",
            "\u001b[32m[12/12 15:54:15 FallenTreePart]: \u001b[0mTest Epoch [3/100],Instance mIoU 88.41, Class mIoU 87.22, \n",
            " Class mIoUs tensor([100.0000,  74.4330], device='cuda:0')\n",
            "\u001b[32m[12/12 15:54:15 FallenTreePart]: \u001b[0mFind a better ckpt @E3, val_ins_miou 88.41 val_cls_miou 87.22, \n",
            "cls_mious: tensor([100.0000,  74.4330], device='cuda:0')\n",
            "\u001b[32m[12/12 15:54:15 FallenTreePart]: \u001b[0mFound the best model and saved in log/shapenetpart/shapenetpart-train-custom_fallen_trees-ngpus1-seed7520-20251212-155340-E84zcZ3REWTnK2wbniefub/checkpoint/shapenetpart-train-custom_fallen_trees-ngpus1-seed7520-20251212-155340-E84zcZ3REWTnK2wbniefub_ckpt_best.pth\n",
            "Train Epoch [4/100] Loss 0.024 : 100% 45/45 [00:07<00:00,  6.00it/s]\n",
            "100% 8/8 [00:00<00:00, 11.86it/s]\n",
            "\u001b[32m[12/12 15:54:24 FallenTreePart]: \u001b[0mTest Epoch [4/100],Instance mIoU 90.66, Class mIoU 89.69, \n",
            " Class mIoUs tensor([100.0000,  79.3769], device='cuda:0')\n",
            "\u001b[32m[12/12 15:54:24 FallenTreePart]: \u001b[0mFind a better ckpt @E4, val_ins_miou 90.66 val_cls_miou 89.69, \n",
            "cls_mious: tensor([100.0000,  79.3769], device='cuda:0')\n",
            "\u001b[32m[12/12 15:54:24 FallenTreePart]: \u001b[0mFound the best model and saved in log/shapenetpart/shapenetpart-train-custom_fallen_trees-ngpus1-seed7520-20251212-155340-E84zcZ3REWTnK2wbniefub/checkpoint/shapenetpart-train-custom_fallen_trees-ngpus1-seed7520-20251212-155340-E84zcZ3REWTnK2wbniefub_ckpt_best.pth\n",
            "Train Epoch [5/100] Loss 0.022 : 100% 45/45 [00:07<00:00,  6.07it/s]\n",
            "100% 8/8 [00:00<00:00,  9.72it/s]\n",
            "\u001b[32m[12/12 15:54:32 FallenTreePart]: \u001b[0mTest Epoch [5/100],Instance mIoU 87.56, Class mIoU 86.28, \n",
            " Class mIoUs tensor([99.9658, 72.5965], device='cuda:0')\n",
            "Train Epoch [6/100] Loss 0.019 : 100% 45/45 [00:07<00:00,  5.91it/s]\n",
            "100% 8/8 [00:00<00:00, 10.95it/s]\n",
            "\u001b[32m[12/12 15:54:41 FallenTreePart]: \u001b[0mTest Epoch [6/100],Instance mIoU 84.38, Class mIoU 82.78, \n",
            " Class mIoUs tensor([99.8249, 65.7425], device='cuda:0')\n",
            "Train Epoch [7/100] Loss 0.020 : 100% 45/45 [00:07<00:00,  6.01it/s]\n",
            "100% 8/8 [00:00<00:00, 11.23it/s]\n",
            "\u001b[32m[12/12 15:54:49 FallenTreePart]: \u001b[0mTest Epoch [7/100],Instance mIoU 92.29, Class mIoU 91.49, \n",
            " Class mIoUs tensor([100.0000,  82.9863], device='cuda:0')\n",
            "\u001b[32m[12/12 15:54:49 FallenTreePart]: \u001b[0mFind a better ckpt @E7, val_ins_miou 92.29 val_cls_miou 91.49, \n",
            "cls_mious: tensor([100.0000,  82.9863], device='cuda:0')\n",
            "\u001b[32m[12/12 15:54:49 FallenTreePart]: \u001b[0mFound the best model and saved in log/shapenetpart/shapenetpart-train-custom_fallen_trees-ngpus1-seed7520-20251212-155340-E84zcZ3REWTnK2wbniefub/checkpoint/shapenetpart-train-custom_fallen_trees-ngpus1-seed7520-20251212-155340-E84zcZ3REWTnK2wbniefub_ckpt_best.pth\n",
            "Train Epoch [8/100] Loss 0.014 : 100% 45/45 [00:07<00:00,  5.96it/s]\n",
            "100% 8/8 [00:00<00:00,  8.56it/s]\n",
            "\u001b[32m[12/12 15:54:58 FallenTreePart]: \u001b[0mTest Epoch [8/100],Instance mIoU 94.09, Class mIoU 93.48, \n",
            " Class mIoUs tensor([100.0000,  86.9662], device='cuda:0')\n",
            "\u001b[32m[12/12 15:54:58 FallenTreePart]: \u001b[0mFind a better ckpt @E8, val_ins_miou 94.09 val_cls_miou 93.48, \n",
            "cls_mious: tensor([100.0000,  86.9662], device='cuda:0')\n",
            "\u001b[32m[12/12 15:54:58 FallenTreePart]: \u001b[0mFound the best model and saved in log/shapenetpart/shapenetpart-train-custom_fallen_trees-ngpus1-seed7520-20251212-155340-E84zcZ3REWTnK2wbniefub/checkpoint/shapenetpart-train-custom_fallen_trees-ngpus1-seed7520-20251212-155340-E84zcZ3REWTnK2wbniefub_ckpt_best.pth\n",
            "Train Epoch [9/100] Loss 0.013 : 100% 45/45 [00:07<00:00,  6.03it/s]\n",
            "100% 8/8 [00:00<00:00, 11.61it/s]\n",
            "\u001b[32m[12/12 15:55:07 FallenTreePart]: \u001b[0mTest Epoch [9/100],Instance mIoU 90.42, Class mIoU 89.57, \n",
            " Class mIoUs tensor([98.5714, 80.5759], device='cuda:0')\n",
            "Train Epoch [10/100] Loss 0.011 : 100% 45/45 [00:07<00:00,  5.96it/s]\n",
            "100% 8/8 [00:00<00:00, 11.15it/s]\n",
            "\u001b[32m[12/12 15:55:15 FallenTreePart]: \u001b[0mTest Epoch [10/100],Instance mIoU 90.97, Class mIoU 90.04, \n",
            " Class mIoUs tensor([100.0000,  80.0763], device='cuda:0')\n",
            "Train Epoch [11/100] Loss 0.013 : 100% 45/45 [00:07<00:00,  5.91it/s]\n",
            "100% 8/8 [00:00<00:00,  9.18it/s]\n",
            "\u001b[32m[12/12 15:55:24 FallenTreePart]: \u001b[0mTest Epoch [11/100],Instance mIoU 90.37, Class mIoU 89.97, \n",
            " Class mIoUs tensor([94.2857, 85.6515], device='cuda:0')\n",
            "Train Epoch [12/100] Loss 0.013 : 100% 45/45 [00:07<00:00,  5.82it/s]\n",
            "100% 8/8 [00:00<00:00, 11.15it/s]\n",
            "\u001b[32m[12/12 15:55:33 FallenTreePart]: \u001b[0mTest Epoch [12/100],Instance mIoU 90.30, Class mIoU 89.45, \n",
            " Class mIoUs tensor([98.5714, 80.3268], device='cuda:0')\n",
            "Train Epoch [13/100] Loss 0.011 : 100% 45/45 [00:07<00:00,  5.82it/s]\n",
            "100% 8/8 [00:00<00:00, 11.26it/s]\n",
            "\u001b[32m[12/12 15:55:42 FallenTreePart]: \u001b[0mTest Epoch [13/100],Instance mIoU 93.87, Class mIoU 93.38, \n",
            " Class mIoUs tensor([98.5714, 88.1966], device='cuda:0')\n",
            "Train Epoch [14/100] Loss 0.008 : 100% 45/45 [00:07<00:00,  5.77it/s]\n",
            "100% 8/8 [00:00<00:00,  9.08it/s]\n",
            "\u001b[32m[12/12 15:55:51 FallenTreePart]: \u001b[0mTest Epoch [14/100],Instance mIoU 91.71, Class mIoU 91.00, \n",
            " Class mIoUs tensor([98.5714, 83.4293], device='cuda:0')\n",
            "Train Epoch [15/100] Loss 0.006 : 100% 45/45 [00:07<00:00,  5.71it/s]\n",
            "100% 8/8 [00:00<00:00, 11.12it/s]\n",
            "\u001b[32m[12/12 15:56:00 FallenTreePart]: \u001b[0mTest Epoch [15/100],Instance mIoU 94.51, Class mIoU 93.94, \n",
            " Class mIoUs tensor([100.0000,  87.8890], device='cuda:0')\n",
            "\u001b[32m[12/12 15:56:00 FallenTreePart]: \u001b[0mFind a better ckpt @E15, val_ins_miou 94.51 val_cls_miou 93.94, \n",
            "cls_mious: tensor([100.0000,  87.8890], device='cuda:0')\n",
            "\u001b[32m[12/12 15:56:00 FallenTreePart]: \u001b[0mFound the best model and saved in log/shapenetpart/shapenetpart-train-custom_fallen_trees-ngpus1-seed7520-20251212-155340-E84zcZ3REWTnK2wbniefub/checkpoint/shapenetpart-train-custom_fallen_trees-ngpus1-seed7520-20251212-155340-E84zcZ3REWTnK2wbniefub_ckpt_best.pth\n",
            "Train Epoch [16/100] Loss 0.006 : 100% 45/45 [00:07<00:00,  5.75it/s]\n",
            "100% 8/8 [00:00<00:00, 10.78it/s]\n",
            "\u001b[32m[12/12 15:56:09 FallenTreePart]: \u001b[0mTest Epoch [16/100],Instance mIoU 93.94, Class mIoU 93.31, \n",
            " Class mIoUs tensor([100.0000,  86.6236], device='cuda:0')\n",
            "Train Epoch [17/100] Loss 0.006 : 100% 45/45 [00:07<00:00,  5.67it/s]\n",
            "100% 8/8 [00:00<00:00,  8.74it/s]\n",
            "\u001b[32m[12/12 15:56:18 FallenTreePart]: \u001b[0mTest Epoch [17/100],Instance mIoU 92.32, Class mIoU 91.53, \n",
            " Class mIoUs tensor([100.0000,  83.0504], device='cuda:0')\n",
            "Train Epoch [18/100] Loss 0.006 : 100% 45/45 [00:08<00:00,  5.60it/s]\n",
            "100% 8/8 [00:00<00:00, 11.44it/s]\n",
            "\u001b[32m[12/12 15:56:27 FallenTreePart]: \u001b[0mTest Epoch [18/100],Instance mIoU 92.26, Class mIoU 91.60, \n",
            " Class mIoUs tensor([98.5714, 84.6341], device='cuda:0')\n",
            "Train Epoch [19/100] Loss 0.005 : 100% 45/45 [00:07<00:00,  5.67it/s]\n",
            "100% 8/8 [00:00<00:00, 10.90it/s]\n",
            "\u001b[32m[12/12 15:56:36 FallenTreePart]: \u001b[0mTest Epoch [19/100],Instance mIoU 94.11, Class mIoU 93.50, \n",
            " Class mIoUs tensor([100.0000,  87.0036], device='cuda:0')\n",
            "Train Epoch [20/100] Loss 0.005 : 100% 45/45 [00:08<00:00,  5.60it/s]\n",
            "100% 8/8 [00:00<00:00,  9.29it/s]\n",
            "\u001b[32m[12/12 15:56:45 FallenTreePart]: \u001b[0mTest Epoch [20/100],Instance mIoU 93.36, Class mIoU 92.68, \n",
            " Class mIoUs tensor([100.0000,  85.3508], device='cuda:0')\n",
            "Train Epoch [21/100] Loss 0.005 : 100% 45/45 [00:07<00:00,  5.74it/s]\n",
            "100% 8/8 [00:00<00:00, 11.32it/s]\n",
            "\u001b[32m[12/12 15:56:54 FallenTreePart]: \u001b[0mTest Epoch [21/100],Instance mIoU 92.68, Class mIoU 91.92, \n",
            " Class mIoUs tensor([100.0000,  83.8347], device='cuda:0')\n",
            "Train Epoch [22/100] Loss 0.004 : 100% 45/45 [00:07<00:00,  5.75it/s]\n",
            "100% 8/8 [00:00<00:00, 11.46it/s]\n",
            "\u001b[32m[12/12 15:57:03 FallenTreePart]: \u001b[0mTest Epoch [22/100],Instance mIoU 93.66, Class mIoU 93.01, \n",
            " Class mIoUs tensor([100.0000,  86.0118], device='cuda:0')\n",
            "Train Epoch [23/100] Loss 0.004 : 100% 45/45 [00:07<00:00,  5.72it/s]\n",
            "100% 8/8 [00:00<00:00,  9.87it/s]\n",
            "\u001b[32m[12/12 15:57:12 FallenTreePart]: \u001b[0mTest Epoch [23/100],Instance mIoU 93.34, Class mIoU 92.65, \n",
            " Class mIoUs tensor([100.0000,  85.2914], device='cuda:0')\n",
            "Train Epoch [24/100] Loss 0.004 : 100% 45/45 [00:07<00:00,  5.74it/s]\n",
            "100% 8/8 [00:00<00:00, 11.18it/s]\n",
            "\u001b[32m[12/12 15:57:21 FallenTreePart]: \u001b[0mTest Epoch [24/100],Instance mIoU 94.19, Class mIoU 93.73, \n",
            " Class mIoUs tensor([98.5714, 88.8919], device='cuda:0')\n",
            "Train Epoch [25/100] Loss 0.004 : 100% 45/45 [00:07<00:00,  5.75it/s]\n",
            "100% 8/8 [00:00<00:00, 10.98it/s]\n",
            "\u001b[32m[12/12 15:57:30 FallenTreePart]: \u001b[0mTest Epoch [25/100],Instance mIoU 94.46, Class mIoU 93.89, \n",
            " Class mIoUs tensor([100.0000,  87.7789], device='cuda:0')\n",
            "Train Epoch [26/100] Loss 0.004 : 100% 45/45 [00:07<00:00,  5.69it/s]\n",
            "100% 8/8 [00:00<00:00, 10.13it/s]\n",
            "\u001b[32m[12/12 15:57:39 FallenTreePart]: \u001b[0mTest Epoch [26/100],Instance mIoU 92.26, Class mIoU 91.61, \n",
            " Class mIoUs tensor([98.5714, 84.6402], device='cuda:0')\n",
            "Train Epoch [27/100] Loss 0.004 : 100% 45/45 [00:07<00:00,  5.68it/s]\n",
            "100% 8/8 [00:00<00:00, 11.14it/s]\n",
            "\u001b[32m[12/12 15:57:48 FallenTreePart]: \u001b[0mTest Epoch [27/100],Instance mIoU 92.14, Class mIoU 91.33, \n",
            " Class mIoUs tensor([100.0000,  82.6607], device='cuda:0')\n",
            "Train Epoch [28/100] Loss 0.004 : 100% 45/45 [00:07<00:00,  5.69it/s]\n",
            "100% 8/8 [00:00<00:00, 11.66it/s]\n",
            "\u001b[32m[12/12 15:57:56 FallenTreePart]: \u001b[0mTest Epoch [28/100],Instance mIoU 92.89, Class mIoU 92.15, \n",
            " Class mIoUs tensor([100.0000,  84.3082], device='cuda:0')\n",
            "Train Epoch [29/100] Loss 0.004 : 100% 45/45 [00:07<00:00,  5.65it/s]\n",
            "100% 8/8 [00:00<00:00,  9.93it/s]\n",
            "\u001b[32m[12/12 15:58:06 FallenTreePart]: \u001b[0mTest Epoch [29/100],Instance mIoU 93.98, Class mIoU 93.36, \n",
            " Class mIoUs tensor([100.0000,  86.7195], device='cuda:0')\n",
            "Train Epoch [30/100] Loss 0.004 : 100% 45/45 [00:07<00:00,  5.71it/s]\n",
            "100% 8/8 [00:00<00:00, 11.44it/s]\n",
            "\u001b[32m[12/12 15:58:15 FallenTreePart]: \u001b[0mTest Epoch [30/100],Instance mIoU 93.61, Class mIoU 92.95, \n",
            " Class mIoUs tensor([100.0000,  85.9078], device='cuda:0')\n",
            "Train Epoch [31/100] Loss 0.012 : 100% 45/45 [00:07<00:00,  5.74it/s]\n",
            "100% 8/8 [00:00<00:00, 10.91it/s]\n",
            "\u001b[32m[12/12 15:58:23 FallenTreePart]: \u001b[0mTest Epoch [31/100],Instance mIoU 81.79, Class mIoU 79.90, \n",
            " Class mIoUs tensor([100.0000,  59.8044], device='cuda:0')\n",
            "Train Epoch [32/100] Loss 0.022 : 100% 45/45 [00:07<00:00,  5.69it/s]\n",
            "100% 8/8 [00:00<00:00,  9.93it/s]\n",
            "\u001b[32m[12/12 15:58:32 FallenTreePart]: \u001b[0mTest Epoch [32/100],Instance mIoU 92.47, Class mIoU 91.69, \n",
            " Class mIoUs tensor([100.0000,  83.3713], device='cuda:0')\n",
            "Train Epoch [33/100] Loss 0.018 : 100% 45/45 [00:07<00:00,  5.74it/s]\n",
            "100% 8/8 [00:00<00:00, 10.98it/s]\n",
            "\u001b[32m[12/12 15:58:41 FallenTreePart]: \u001b[0mTest Epoch [33/100],Instance mIoU 92.82, Class mIoU 92.08, \n",
            " Class mIoUs tensor([100.0000,  84.1559], device='cuda:0')\n",
            "Train Epoch [34/100] Loss 0.014 : 100% 45/45 [00:07<00:00,  5.75it/s]\n",
            "100% 8/8 [00:00<00:00, 11.29it/s]\n",
            "\u001b[32m[12/12 15:58:50 FallenTreePart]: \u001b[0mTest Epoch [34/100],Instance mIoU 92.13, Class mIoU 91.31, \n",
            " Class mIoUs tensor([100.0000,  82.6212], device='cuda:0')\n",
            "Train Epoch [35/100] Loss 0.011 : 100% 45/45 [00:07<00:00,  5.69it/s]\n",
            "100% 8/8 [00:00<00:00, 10.30it/s]\n",
            "\u001b[32m[12/12 15:58:59 FallenTreePart]: \u001b[0mTest Epoch [35/100],Instance mIoU 94.01, Class mIoU 93.53, \n",
            " Class mIoUs tensor([98.5714, 88.4963], device='cuda:0')\n",
            "Train Epoch [36/100] Loss 0.009 : 100% 45/45 [00:07<00:00,  5.71it/s]\n",
            "100% 8/8 [00:00<00:00, 11.43it/s]\n",
            "\u001b[32m[12/12 15:59:08 FallenTreePart]: \u001b[0mTest Epoch [36/100],Instance mIoU 93.30, Class mIoU 92.90, \n",
            " Class mIoUs tensor([97.1429, 88.6518], device='cuda:0')\n",
            "Train Epoch [37/100] Loss 0.007 : 100% 45/45 [00:07<00:00,  5.73it/s]\n",
            "100% 8/8 [00:00<00:00, 10.98it/s]\n",
            "\u001b[32m[12/12 15:59:17 FallenTreePart]: \u001b[0mTest Epoch [37/100],Instance mIoU 93.12, Class mIoU 92.56, \n",
            " Class mIoUs tensor([98.5714, 86.5412], device='cuda:0')\n",
            "Train Epoch [38/100] Loss 0.005 : 100% 45/45 [00:07<00:00,  5.73it/s]\n",
            "100% 8/8 [00:00<00:00, 11.04it/s]\n",
            "\u001b[32m[12/12 15:59:26 FallenTreePart]: \u001b[0mTest Epoch [38/100],Instance mIoU 93.73, Class mIoU 93.23, \n",
            " Class mIoUs tensor([98.5714, 87.8803], device='cuda:0')\n",
            "Train Epoch [39/100] Loss 0.005 : 100% 45/45 [00:07<00:00,  5.75it/s]\n",
            "100% 8/8 [00:00<00:00, 10.66it/s]\n",
            "\u001b[32m[12/12 15:59:35 FallenTreePart]: \u001b[0mTest Epoch [39/100],Instance mIoU 93.15, Class mIoU 92.59, \n",
            " Class mIoUs tensor([98.5714, 86.6115], device='cuda:0')\n",
            "Train Epoch [40/100] Loss 0.004 : 100% 45/45 [00:07<00:00,  5.74it/s]\n",
            "100% 8/8 [00:00<00:00, 11.28it/s]\n",
            "\u001b[32m[12/12 15:59:44 FallenTreePart]: \u001b[0mTest Epoch [40/100],Instance mIoU 93.87, Class mIoU 93.39, \n",
            " Class mIoUs tensor([98.5714, 88.2048], device='cuda:0')\n",
            "Train Epoch [41/100] Loss 0.004 : 100% 45/45 [00:07<00:00,  5.71it/s]\n",
            "100% 8/8 [00:00<00:00, 10.46it/s]\n",
            "\u001b[32m[12/12 15:59:53 FallenTreePart]: \u001b[0mTest Epoch [41/100],Instance mIoU 93.74, Class mIoU 93.24, \n",
            " Class mIoUs tensor([98.5714, 87.9130], device='cuda:0')\n",
            "Train Epoch [42/100] Loss 0.004 : 100% 45/45 [00:07<00:00,  5.74it/s]\n",
            "100% 8/8 [00:00<00:00, 10.87it/s]\n",
            "\u001b[32m[12/12 16:00:01 FallenTreePart]: \u001b[0mTest Epoch [42/100],Instance mIoU 93.28, Class mIoU 92.74, \n",
            " Class mIoUs tensor([98.5714, 86.9027], device='cuda:0')\n",
            "Train Epoch [43/100] Loss 0.004 : 100% 45/45 [00:07<00:00,  5.71it/s]\n",
            "100% 8/8 [00:00<00:00, 10.91it/s]\n",
            "\u001b[32m[12/12 16:00:10 FallenTreePart]: \u001b[0mTest Epoch [43/100],Instance mIoU 93.59, Class mIoU 93.08, \n",
            " Class mIoUs tensor([98.5714, 87.5822], device='cuda:0')\n",
            "Train Epoch [44/100] Loss 0.003 : 100% 45/45 [00:07<00:00,  5.74it/s]\n",
            "100% 8/8 [00:00<00:00, 10.10it/s]\n",
            "\u001b[32m[12/12 16:00:19 FallenTreePart]: \u001b[0mTest Epoch [44/100],Instance mIoU 93.53, Class mIoU 93.01, \n",
            " Class mIoUs tensor([98.5714, 87.4490], device='cuda:0')\n",
            "Train Epoch [45/100] Loss 0.003 : 100% 45/45 [00:07<00:00,  5.72it/s]\n",
            "100% 8/8 [00:00<00:00, 11.29it/s]\n",
            "\u001b[32m[12/12 16:00:28 FallenTreePart]: \u001b[0mTest Epoch [45/100],Instance mIoU 93.51, Class mIoU 92.98, \n",
            " Class mIoUs tensor([98.5714, 87.3979], device='cuda:0')\n",
            "Train Epoch [46/100] Loss 0.003 : 100% 45/45 [00:07<00:00,  5.74it/s]\n",
            "100% 8/8 [00:00<00:00, 10.31it/s]\n",
            "\u001b[32m[12/12 16:00:37 FallenTreePart]: \u001b[0mTest Epoch [46/100],Instance mIoU 93.25, Class mIoU 92.70, \n",
            " Class mIoUs tensor([98.5714, 86.8330], device='cuda:0')\n",
            "Train Epoch [47/100] Loss 0.004 : 100% 45/45 [00:07<00:00,  5.73it/s]\n",
            "100% 8/8 [00:00<00:00, 10.49it/s]\n",
            "\u001b[32m[12/12 16:00:46 FallenTreePart]: \u001b[0mTest Epoch [47/100],Instance mIoU 93.93, Class mIoU 93.45, \n",
            " Class mIoUs tensor([98.5714, 88.3247], device='cuda:0')\n",
            "Train Epoch [48/100] Loss 0.004 : 100% 45/45 [00:07<00:00,  5.72it/s]\n",
            "100% 8/8 [00:00<00:00, 11.01it/s]\n",
            "\u001b[32m[12/12 16:00:55 FallenTreePart]: \u001b[0mTest Epoch [48/100],Instance mIoU 94.51, Class mIoU 94.08, \n",
            " Class mIoUs tensor([98.5714, 89.5973], device='cuda:0')\n",
            "Train Epoch [49/100] Loss 0.003 : 100% 45/45 [00:07<00:00,  5.73it/s]\n",
            "100% 8/8 [00:00<00:00, 11.48it/s]\n",
            "\u001b[32m[12/12 16:01:04 FallenTreePart]: \u001b[0mTest Epoch [49/100],Instance mIoU 93.88, Class mIoU 93.40, \n",
            " Class mIoUs tensor([98.5714, 88.2269], device='cuda:0')\n",
            "Train Epoch [50/100] Loss 0.003 : 100% 45/45 [00:07<00:00,  5.69it/s]\n",
            "100% 8/8 [00:00<00:00, 10.53it/s]\n",
            "\u001b[32m[12/12 16:01:13 FallenTreePart]: \u001b[0mTest Epoch [50/100],Instance mIoU 93.64, Class mIoU 93.13, \n",
            " Class mIoUs tensor([98.5714, 87.6909], device='cuda:0')\n",
            "Train Epoch [51/100] Loss 0.003 : 100% 45/45 [00:07<00:00,  5.72it/s]\n",
            "100% 8/8 [00:00<00:00, 11.36it/s]\n",
            "\u001b[32m[12/12 16:01:22 FallenTreePart]: \u001b[0mTest Epoch [51/100],Instance mIoU 94.26, Class mIoU 93.81, \n",
            " Class mIoUs tensor([98.5714, 89.0505], device='cuda:0')\n",
            "Train Epoch [52/100] Loss 0.003 : 100% 45/45 [00:07<00:00,  5.71it/s]\n",
            "100% 8/8 [00:00<00:00, 10.69it/s]\n",
            "\u001b[32m[12/12 16:01:31 FallenTreePart]: \u001b[0mTest Epoch [52/100],Instance mIoU 94.33, Class mIoU 93.89, \n",
            " Class mIoUs tensor([98.5714, 89.2062], device='cuda:0')\n",
            "Train Epoch [53/100] Loss 0.003 : 100% 45/45 [00:07<00:00,  5.73it/s]\n",
            "100% 8/8 [00:00<00:00, 10.05it/s]\n",
            "\u001b[32m[12/12 16:01:40 FallenTreePart]: \u001b[0mTest Epoch [53/100],Instance mIoU 93.47, Class mIoU 92.94, \n",
            " Class mIoUs tensor([98.5714, 87.3096], device='cuda:0')\n",
            "Train Epoch [54/100] Loss 0.003 : 100% 45/45 [00:07<00:00,  5.74it/s]\n",
            "100% 8/8 [00:00<00:00, 10.94it/s]\n",
            "\u001b[32m[12/12 16:01:48 FallenTreePart]: \u001b[0mTest Epoch [54/100],Instance mIoU 93.73, Class mIoU 93.23, \n",
            " Class mIoUs tensor([98.5714, 87.8905], device='cuda:0')\n",
            "Train Epoch [55/100] Loss 0.003 : 100% 45/45 [00:07<00:00,  5.75it/s]\n",
            "100% 8/8 [00:00<00:00, 10.54it/s]\n",
            "\u001b[32m[12/12 16:01:57 FallenTreePart]: \u001b[0mTest Epoch [55/100],Instance mIoU 93.47, Class mIoU 92.94, \n",
            " Class mIoUs tensor([98.5714, 87.3183], device='cuda:0')\n",
            "Train Epoch [56/100] Loss 0.003 : 100% 45/45 [00:07<00:00,  5.73it/s]\n",
            "100% 8/8 [00:00<00:00,  9.82it/s]\n",
            "\u001b[32m[12/12 16:02:06 FallenTreePart]: \u001b[0mTest Epoch [56/100],Instance mIoU 93.84, Class mIoU 93.35, \n",
            " Class mIoUs tensor([98.5714, 88.1336], device='cuda:0')\n",
            "Train Epoch [57/100] Loss 0.003 : 100% 45/45 [00:07<00:00,  5.73it/s]\n",
            "100% 8/8 [00:00<00:00, 11.39it/s]\n",
            "\u001b[32m[12/12 16:02:15 FallenTreePart]: \u001b[0mTest Epoch [57/100],Instance mIoU 93.41, Class mIoU 92.87, \n",
            " Class mIoUs tensor([98.5714, 87.1711], device='cuda:0')\n",
            "Train Epoch [58/100] Loss 0.003 : 100% 45/45 [00:07<00:00,  5.74it/s]\n",
            "100% 8/8 [00:00<00:00, 10.48it/s]\n",
            "\u001b[32m[12/12 16:02:24 FallenTreePart]: \u001b[0mTest Epoch [58/100],Instance mIoU 94.07, Class mIoU 93.60, \n",
            " Class mIoUs tensor([98.5714, 88.6270], device='cuda:0')\n",
            "Train Epoch [59/100] Loss 0.003 : 100% 45/45 [00:07<00:00,  5.67it/s]\n",
            "100% 8/8 [00:00<00:00, 10.29it/s]\n",
            "\u001b[32m[12/12 16:02:33 FallenTreePart]: \u001b[0mTest Epoch [59/100],Instance mIoU 94.01, Class mIoU 93.54, \n",
            " Class mIoUs tensor([98.5714, 88.5006], device='cuda:0')\n",
            "Train Epoch [60/100] Loss 0.003 : 100% 45/45 [00:07<00:00,  5.72it/s]\n",
            "100% 8/8 [00:00<00:00, 11.16it/s]\n",
            "\u001b[32m[12/12 16:02:42 FallenTreePart]: \u001b[0mTest Epoch [60/100],Instance mIoU 93.83, Class mIoU 93.34, \n",
            " Class mIoUs tensor([98.5714, 88.1154], device='cuda:0')\n",
            "Train Epoch [61/100] Loss 0.003 : 100% 45/45 [00:07<00:00,  5.73it/s]\n",
            "100% 8/8 [00:00<00:00, 11.54it/s]\n",
            "\u001b[32m[12/12 16:02:51 FallenTreePart]: \u001b[0mTest Epoch [61/100],Instance mIoU 94.12, Class mIoU 93.65, \n",
            " Class mIoUs tensor([98.5714, 88.7376], device='cuda:0')\n",
            "Train Epoch [62/100] Loss 0.003 : 100% 45/45 [00:07<00:00,  5.69it/s]\n",
            "100% 8/8 [00:00<00:00, 10.25it/s]\n",
            "\u001b[32m[12/12 16:03:00 FallenTreePart]: \u001b[0mTest Epoch [62/100],Instance mIoU 94.55, Class mIoU 94.13, \n",
            " Class mIoUs tensor([98.5714, 89.6882], device='cuda:0')\n",
            "\u001b[32m[12/12 16:03:00 FallenTreePart]: \u001b[0mFind a better ckpt @E62, val_ins_miou 94.55 val_cls_miou 94.13, \n",
            "cls_mious: tensor([98.5714, 89.6882], device='cuda:0')\n",
            "\u001b[32m[12/12 16:03:00 FallenTreePart]: \u001b[0mFound the best model and saved in log/shapenetpart/shapenetpart-train-custom_fallen_trees-ngpus1-seed7520-20251212-155340-E84zcZ3REWTnK2wbniefub/checkpoint/shapenetpart-train-custom_fallen_trees-ngpus1-seed7520-20251212-155340-E84zcZ3REWTnK2wbniefub_ckpt_best.pth\n",
            "Train Epoch [63/100] Loss 0.002 : 100% 45/45 [00:07<00:00,  5.76it/s]\n",
            "100% 8/8 [00:00<00:00, 11.10it/s]\n",
            "\u001b[32m[12/12 16:03:09 FallenTreePart]: \u001b[0mTest Epoch [63/100],Instance mIoU 94.12, Class mIoU 93.65, \n",
            " Class mIoUs tensor([98.5714, 88.7372], device='cuda:0')\n",
            "Train Epoch [64/100] Loss 0.002 : 100% 45/45 [00:07<00:00,  5.74it/s]\n",
            "100% 8/8 [00:00<00:00, 10.83it/s]\n",
            "\u001b[32m[12/12 16:03:18 FallenTreePart]: \u001b[0mTest Epoch [64/100],Instance mIoU 94.70, Class mIoU 94.30, \n",
            " Class mIoUs tensor([98.5714, 90.0314], device='cuda:0')\n",
            "\u001b[32m[12/12 16:03:18 FallenTreePart]: \u001b[0mFind a better ckpt @E64, val_ins_miou 94.70 val_cls_miou 94.30, \n",
            "cls_mious: tensor([98.5714, 90.0314], device='cuda:0')\n",
            "\u001b[32m[12/12 16:03:18 FallenTreePart]: \u001b[0mFound the best model and saved in log/shapenetpart/shapenetpart-train-custom_fallen_trees-ngpus1-seed7520-20251212-155340-E84zcZ3REWTnK2wbniefub/checkpoint/shapenetpart-train-custom_fallen_trees-ngpus1-seed7520-20251212-155340-E84zcZ3REWTnK2wbniefub_ckpt_best.pth\n",
            "Train Epoch [65/100] Loss 0.003 : 100% 45/45 [00:07<00:00,  5.71it/s]\n",
            "100% 8/8 [00:00<00:00,  9.60it/s]\n",
            "\u001b[32m[12/12 16:03:27 FallenTreePart]: \u001b[0mTest Epoch [65/100],Instance mIoU 94.80, Class mIoU 94.41, \n",
            " Class mIoUs tensor([98.5714, 90.2507], device='cuda:0')\n",
            "\u001b[32m[12/12 16:03:27 FallenTreePart]: \u001b[0mFind a better ckpt @E65, val_ins_miou 94.80 val_cls_miou 94.41, \n",
            "cls_mious: tensor([98.5714, 90.2507], device='cuda:0')\n",
            "\u001b[32m[12/12 16:03:27 FallenTreePart]: \u001b[0mFound the best model and saved in log/shapenetpart/shapenetpart-train-custom_fallen_trees-ngpus1-seed7520-20251212-155340-E84zcZ3REWTnK2wbniefub/checkpoint/shapenetpart-train-custom_fallen_trees-ngpus1-seed7520-20251212-155340-E84zcZ3REWTnK2wbniefub_ckpt_best.pth\n",
            "Train Epoch [66/100] Loss 0.003 : 100% 45/45 [00:07<00:00,  5.77it/s]\n",
            "100% 8/8 [00:00<00:00, 10.91it/s]\n",
            "\u001b[32m[12/12 16:03:35 FallenTreePart]: \u001b[0mTest Epoch [66/100],Instance mIoU 93.89, Class mIoU 93.41, \n",
            " Class mIoUs tensor([98.5714, 88.2440], device='cuda:0')\n",
            "Train Epoch [67/100] Loss 0.002 : 100% 45/45 [00:07<00:00,  5.76it/s]\n",
            "100% 8/8 [00:00<00:00, 11.49it/s]\n",
            "\u001b[32m[12/12 16:03:44 FallenTreePart]: \u001b[0mTest Epoch [67/100],Instance mIoU 94.71, Class mIoU 94.31, \n",
            " Class mIoUs tensor([98.5714, 90.0480], device='cuda:0')\n",
            "Train Epoch [68/100] Loss 0.002 : 100% 45/45 [00:07<00:00,  5.70it/s]\n",
            "100% 8/8 [00:00<00:00,  9.62it/s]\n",
            "\u001b[32m[12/12 16:03:53 FallenTreePart]: \u001b[0mTest Epoch [68/100],Instance mIoU 94.20, Class mIoU 93.75, \n",
            " Class mIoUs tensor([98.5714, 88.9290], device='cuda:0')\n",
            "Train Epoch [69/100] Loss 0.002 : 100% 45/45 [00:07<00:00,  5.73it/s]\n",
            "100% 8/8 [00:00<00:00, 11.39it/s]\n",
            "\u001b[32m[12/12 16:04:02 FallenTreePart]: \u001b[0mTest Epoch [69/100],Instance mIoU 93.76, Class mIoU 93.26, \n",
            " Class mIoUs tensor([98.5714, 87.9577], device='cuda:0')\n",
            "Train Epoch [70/100] Loss 0.003 : 100% 45/45 [00:07<00:00,  5.73it/s]\n",
            "100% 8/8 [00:00<00:00, 11.12it/s]\n",
            "\u001b[32m[12/12 16:04:11 FallenTreePart]: \u001b[0mTest Epoch [70/100],Instance mIoU 94.59, Class mIoU 94.17, \n",
            " Class mIoUs tensor([98.5714, 89.7755], device='cuda:0')\n",
            "Train Epoch [71/100] Loss 0.002 : 100% 45/45 [00:07<00:00,  5.68it/s]\n",
            "100% 8/8 [00:00<00:00, 10.71it/s]\n",
            "\u001b[32m[12/12 16:04:20 FallenTreePart]: \u001b[0mTest Epoch [71/100],Instance mIoU 94.61, Class mIoU 94.21, \n",
            " Class mIoUs tensor([98.5714, 89.8390], device='cuda:0')\n",
            "Train Epoch [72/100] Loss 0.002 : 100% 45/45 [00:07<00:00,  5.76it/s]\n",
            "100% 8/8 [00:00<00:00, 10.89it/s]\n",
            "\u001b[32m[12/12 16:04:29 FallenTreePart]: \u001b[0mTest Epoch [72/100],Instance mIoU 94.61, Class mIoU 94.20, \n",
            " Class mIoUs tensor([98.5714, 89.8382], device='cuda:0')\n",
            "Train Epoch [73/100] Loss 0.002 : 100% 45/45 [00:07<00:00,  5.75it/s]\n",
            "100% 8/8 [00:00<00:00, 10.88it/s]\n",
            "\u001b[32m[12/12 16:04:38 FallenTreePart]: \u001b[0mTest Epoch [73/100],Instance mIoU 94.61, Class mIoU 94.20, \n",
            " Class mIoUs tensor([98.5714, 89.8211], device='cuda:0')\n",
            "Train Epoch [74/100] Loss 0.002 : 100% 45/45 [00:07<00:00,  5.71it/s]\n",
            "100% 8/8 [00:00<00:00, 10.49it/s]\n",
            "\u001b[32m[12/12 16:04:47 FallenTreePart]: \u001b[0mTest Epoch [74/100],Instance mIoU 94.35, Class mIoU 93.91, \n",
            " Class mIoUs tensor([98.5714, 89.2449], device='cuda:0')\n",
            "Train Epoch [75/100] Loss 0.002 : 100% 45/45 [00:07<00:00,  5.74it/s]\n",
            "100% 8/8 [00:00<00:00, 11.17it/s]\n",
            "\u001b[32m[12/12 16:04:56 FallenTreePart]: \u001b[0mTest Epoch [75/100],Instance mIoU 94.36, Class mIoU 93.92, \n",
            " Class mIoUs tensor([98.5714, 89.2738], device='cuda:0')\n",
            "Train Epoch [76/100] Loss 0.002 : 100% 45/45 [00:07<00:00,  5.75it/s]\n",
            "100% 8/8 [00:00<00:00, 11.16it/s]\n",
            "\u001b[32m[12/12 16:05:04 FallenTreePart]: \u001b[0mTest Epoch [76/100],Instance mIoU 94.37, Class mIoU 93.93, \n",
            " Class mIoUs tensor([98.5714, 89.2884], device='cuda:0')\n",
            "Train Epoch [77/100] Loss 0.002 : 100% 45/45 [00:07<00:00,  5.69it/s]\n",
            "100% 8/8 [00:00<00:00, 10.15it/s]\n",
            "\u001b[32m[12/12 16:05:13 FallenTreePart]: \u001b[0mTest Epoch [77/100],Instance mIoU 94.62, Class mIoU 94.21, \n",
            " Class mIoUs tensor([98.5714, 89.8568], device='cuda:0')\n",
            "Train Epoch [78/100] Loss 0.002 : 100% 45/45 [00:07<00:00,  5.73it/s]\n",
            "100% 8/8 [00:00<00:00, 11.44it/s]\n",
            "\u001b[32m[12/12 16:05:22 FallenTreePart]: \u001b[0mTest Epoch [78/100],Instance mIoU 94.63, Class mIoU 94.22, \n",
            " Class mIoUs tensor([98.5714, 89.8677], device='cuda:0')\n",
            "Train Epoch [79/100] Loss 0.002 : 100% 45/45 [00:07<00:00,  5.69it/s]\n",
            "100% 8/8 [00:00<00:00, 11.09it/s]\n",
            "\u001b[32m[12/12 16:05:31 FallenTreePart]: \u001b[0mTest Epoch [79/100],Instance mIoU 94.63, Class mIoU 94.22, \n",
            " Class mIoUs tensor([98.5714, 89.8722], device='cuda:0')\n",
            "Train Epoch [80/100] Loss 0.002 : 100% 45/45 [00:07<00:00,  5.70it/s]\n",
            "100% 8/8 [00:00<00:00, 10.30it/s]\n",
            "\u001b[32m[12/12 16:05:40 FallenTreePart]: \u001b[0mTest Epoch [80/100],Instance mIoU 94.63, Class mIoU 94.22, \n",
            " Class mIoUs tensor([98.5714, 89.8653], device='cuda:0')\n",
            "Train Epoch [81/100] Loss 0.002 : 100% 45/45 [00:07<00:00,  5.75it/s]\n",
            "100% 8/8 [00:00<00:00, 10.80it/s]\n",
            "\u001b[32m[12/12 16:05:49 FallenTreePart]: \u001b[0mTest Epoch [81/100],Instance mIoU 94.63, Class mIoU 94.22, \n",
            " Class mIoUs tensor([98.5714, 89.8629], device='cuda:0')\n",
            "Train Epoch [82/100] Loss 0.002 : 100% 45/45 [00:07<00:00,  5.71it/s]\n",
            "100% 8/8 [00:00<00:00, 10.89it/s]\n",
            "\u001b[32m[12/12 16:05:58 FallenTreePart]: \u001b[0mTest Epoch [82/100],Instance mIoU 94.63, Class mIoU 94.22, \n",
            " Class mIoUs tensor([98.5714, 89.8720], device='cuda:0')\n",
            "Train Epoch [83/100] Loss 0.002 : 100% 45/45 [00:07<00:00,  5.69it/s]\n",
            "100% 8/8 [00:00<00:00,  9.77it/s]\n",
            "\u001b[32m[12/12 16:06:07 FallenTreePart]: \u001b[0mTest Epoch [83/100],Instance mIoU 94.63, Class mIoU 94.22, \n",
            " Class mIoUs tensor([98.5714, 89.8721], device='cuda:0')\n",
            "Train Epoch [84/100] Loss 0.002 : 100% 45/45 [00:07<00:00,  5.77it/s]\n",
            "100% 8/8 [00:00<00:00, 10.69it/s]\n",
            "\u001b[32m[12/12 16:06:16 FallenTreePart]: \u001b[0mTest Epoch [84/100],Instance mIoU 94.63, Class mIoU 94.22, \n",
            " Class mIoUs tensor([98.5714, 89.8721], device='cuda:0')\n",
            "Train Epoch [85/100] Loss 0.002 : 100% 45/45 [00:07<00:00,  5.72it/s]\n",
            "100% 8/8 [00:00<00:00, 11.22it/s]\n",
            "\u001b[32m[12/12 16:06:25 FallenTreePart]: \u001b[0mTest Epoch [85/100],Instance mIoU 94.62, Class mIoU 94.22, \n",
            " Class mIoUs tensor([98.5714, 89.8609], device='cuda:0')\n",
            "Train Epoch [86/100] Loss 0.002 : 100% 45/45 [00:07<00:00,  5.75it/s]\n",
            "100% 8/8 [00:00<00:00, 10.04it/s]\n",
            "\u001b[32m[12/12 16:06:34 FallenTreePart]: \u001b[0mTest Epoch [86/100],Instance mIoU 94.63, Class mIoU 94.22, \n",
            " Class mIoUs tensor([98.5714, 89.8685], device='cuda:0')\n",
            "Train Epoch [87/100] Loss 0.002 : 100% 45/45 [00:07<00:00,  5.77it/s]\n",
            "100% 8/8 [00:00<00:00, 10.53it/s]\n",
            "\u001b[32m[12/12 16:06:43 FallenTreePart]: \u001b[0mTest Epoch [87/100],Instance mIoU 94.63, Class mIoU 94.22, \n",
            " Class mIoUs tensor([98.5714, 89.8690], device='cuda:0')\n",
            "Train Epoch [88/100] Loss 0.002 : 100% 45/45 [00:07<00:00,  5.73it/s]\n",
            "100% 8/8 [00:00<00:00, 11.18it/s]\n",
            "\u001b[32m[12/12 16:06:51 FallenTreePart]: \u001b[0mTest Epoch [88/100],Instance mIoU 94.62, Class mIoU 94.21, \n",
            " Class mIoUs tensor([98.5714, 89.8550], device='cuda:0')\n",
            "Train Epoch [89/100] Loss 0.002 : 100% 45/45 [00:07<00:00,  5.73it/s]\n",
            "100% 8/8 [00:00<00:00,  9.56it/s]\n",
            "\u001b[32m[12/12 16:07:00 FallenTreePart]: \u001b[0mTest Epoch [89/100],Instance mIoU 94.62, Class mIoU 94.21, \n",
            " Class mIoUs tensor([98.5714, 89.8482], device='cuda:0')\n",
            "Train Epoch [90/100] Loss 0.002 : 100% 45/45 [00:07<00:00,  5.74it/s]\n",
            "100% 8/8 [00:00<00:00, 11.05it/s]\n",
            "\u001b[32m[12/12 16:07:09 FallenTreePart]: \u001b[0mTest Epoch [90/100],Instance mIoU 94.64, Class mIoU 94.23, \n",
            " Class mIoUs tensor([98.5714, 89.8849], device='cuda:0')\n",
            "Train Epoch [91/100] Loss 0.002 : 100% 45/45 [00:07<00:00,  5.72it/s]\n",
            "100% 8/8 [00:00<00:00, 10.88it/s]\n",
            "\u001b[32m[12/12 16:07:18 FallenTreePart]: \u001b[0mTest Epoch [91/100],Instance mIoU 94.63, Class mIoU 94.23, \n",
            " Class mIoUs tensor([98.5714, 89.8791], device='cuda:0')\n",
            "Train Epoch [92/100] Loss 0.002 : 100% 45/45 [00:07<00:00,  5.69it/s]\n",
            "100% 8/8 [00:00<00:00,  9.83it/s]\n",
            "\u001b[32m[12/12 16:07:27 FallenTreePart]: \u001b[0mTest Epoch [92/100],Instance mIoU 94.63, Class mIoU 94.22, \n",
            " Class mIoUs tensor([98.5714, 89.8758], device='cuda:0')\n",
            "Train Epoch [93/100] Loss 0.002 : 100% 45/45 [00:07<00:00,  5.73it/s]\n",
            "100% 8/8 [00:00<00:00, 11.51it/s]\n",
            "\u001b[32m[12/12 16:07:36 FallenTreePart]: \u001b[0mTest Epoch [93/100],Instance mIoU 94.63, Class mIoU 94.23, \n",
            " Class mIoUs tensor([98.5714, 89.8790], device='cuda:0')\n",
            "Train Epoch [94/100] Loss 0.002 : 100% 45/45 [00:07<00:00,  5.70it/s]\n",
            "100% 8/8 [00:00<00:00, 11.14it/s]\n",
            "\u001b[32m[12/12 16:07:45 FallenTreePart]: \u001b[0mTest Epoch [94/100],Instance mIoU 94.63, Class mIoU 94.22, \n",
            " Class mIoUs tensor([98.5714, 89.8765], device='cuda:0')\n",
            "Train Epoch [95/100] Loss 0.002 : 100% 45/45 [00:07<00:00,  5.70it/s]\n",
            "100% 8/8 [00:00<00:00, 10.21it/s]\n",
            "\u001b[32m[12/12 16:07:54 FallenTreePart]: \u001b[0mTest Epoch [95/100],Instance mIoU 94.63, Class mIoU 94.22, \n",
            " Class mIoUs tensor([98.5714, 89.8765], device='cuda:0')\n",
            "Train Epoch [96/100] Loss 0.002 : 100% 45/45 [00:07<00:00,  5.71it/s]\n",
            "100% 8/8 [00:00<00:00, 11.13it/s]\n",
            "\u001b[32m[12/12 16:08:03 FallenTreePart]: \u001b[0mTest Epoch [96/100],Instance mIoU 94.63, Class mIoU 94.23, \n",
            " Class mIoUs tensor([98.5714, 89.8815], device='cuda:0')\n",
            "Train Epoch [97/100] Loss 0.002 : 100% 45/45 [00:07<00:00,  5.74it/s]\n",
            "100% 8/8 [00:00<00:00, 11.17it/s]\n",
            "\u001b[32m[12/12 16:08:12 FallenTreePart]: \u001b[0mTest Epoch [97/100],Instance mIoU 94.63, Class mIoU 94.23, \n",
            " Class mIoUs tensor([98.5714, 89.8815], device='cuda:0')\n",
            "Train Epoch [98/100] Loss 0.002 : 100% 45/45 [00:07<00:00,  5.71it/s]\n",
            "100% 8/8 [00:00<00:00, 10.49it/s]\n",
            "\u001b[32m[12/12 16:08:21 FallenTreePart]: \u001b[0mTest Epoch [98/100],Instance mIoU 94.63, Class mIoU 94.23, \n",
            " Class mIoUs tensor([98.5714, 89.8810], device='cuda:0')\n",
            "Train Epoch [99/100] Loss 0.002 : 100% 45/45 [00:07<00:00,  5.72it/s]\n",
            "100% 8/8 [00:00<00:00, 11.36it/s]\n",
            "\u001b[32m[12/12 16:08:30 FallenTreePart]: \u001b[0mTest Epoch [99/100],Instance mIoU 94.63, Class mIoU 94.23, \n",
            " Class mIoUs tensor([98.5714, 89.8800], device='cuda:0')\n",
            "Train Epoch [100/100] Loss 0.002 : 100% 45/45 [00:07<00:00,  5.75it/s]\n",
            "100% 8/8 [00:00<00:00, 11.25it/s]\n",
            "\u001b[32m[12/12 16:08:38 FallenTreePart]: \u001b[0mTest Epoch [100/100],Instance mIoU 94.64, Class mIoU 94.23, \n",
            " Class mIoUs tensor([98.5714, 89.8865], device='cuda:0')\n",
            "\u001b[32m[12/12 16:08:38 FallenTreePart]: \u001b[0mBest Epoch 65,Instance mIoU 94.80, Class mIoU 94.41, \n",
            " Class mIoUs tensor([98.5714, 90.2507], device='cuda:0')\n",
            "\u001b[32m[12/12 16:08:38 FallenTreePart]: \u001b[0mSuccessful Loading the ckpt from log/shapenetpart/shapenetpart-train-custom_fallen_trees-ngpus1-seed7520-20251212-155340-E84zcZ3REWTnK2wbniefub/checkpoint/shapenetpart-train-custom_fallen_trees-ngpus1-seed7520-20251212-155340-E84zcZ3REWTnK2wbniefub_ckpt_best.pth\n",
            "\u001b[32m[12/12 16:08:38 FallenTreePart]: \u001b[0mckpts @ 65 epoch( {} )\n",
            "100% 8/8 [00:05<00:00,  1.47it/s]\n",
            "\u001b[32m[12/12 16:08:44 FallenTreePart]: \u001b[0mTest Epoch [100/100],Instance mIoU 95.67, Class mIoU 95.22, \n",
            " Class mIoUs tensor([100.0000,  90.4437], device='cuda:0')\n",
            "\u001b[32m[12/12 16:08:44 FallenTreePart]: \u001b[0m---Voting---\n",
            "Best Epoch 65,Voting Instance mIoU 95.67, Voting Class mIoU 95.22, \n",
            " Voting Class mIoUs tensor([100.0000,  90.4437], device='cuda:0')\n",
            "Traceback (most recent call last):\n",
            "  File \"/content/PointNeXt/examples/shapenetpart/main.py\", line 447, in <module>\n",
            "    main(0, cfg)\n",
            "  File \"/content/PointNeXt/examples/shapenetpart/main.py\", line 283, in main\n",
            "    dist.destroy_process_group()\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/torch/distributed/distributed_c10d.py\", line 1721, in destroy_process_group\n",
            "    assert pg is not None\n",
            "           ^^^^^^^^^^^^^^\n",
            "AssertionError\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Save to Drive"
      ],
      "metadata": {
        "id": "WiZVTd19mU_T"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Smart Backup using Glob\n",
        "import glob\n",
        "\n",
        "# 1. Define the pattern (The * acts as the regex)\n",
        "# We look for any folder starting with the project name inside the log dir\n",
        "search_pattern = \"/content/PointNeXt/log/shapenetpart/shapenetpart-train-custom_fallen_trees*\"\n",
        "\n",
        "print(f\" Searching for runs matching: {search_pattern}...\")\n",
        "\n",
        "# 2. Find all matching folders\n",
        "found_folders = glob.glob(search_pattern)\n",
        "\n",
        "if not found_folders:\n",
        "    print(\"\\u274c Error: No training folders found matching that pattern.\")\n",
        "else:\n",
        "    # 3. Pick the LATEST folder (in case trained multiple times)\n",
        "    latest_run_dir = max(found_folders, key=os.path.getmtime)\n",
        "    print(f\"\\u2705 Found latest run: {os.path.basename(latest_run_dir)}\")\n",
        "\n",
        "    # 4. Construct the checkpoint path\n",
        "    source_ckpt_dir = os.path.join(latest_run_dir, \"checkpoint\")\n",
        "    drive_model_dir = \"/content/drive/MyDrive/ML_Projects/PointNeXt/Models\"\n",
        "\n",
        "    # 5. Perform Backup\n",
        "    if os.path.exists(source_ckpt_dir):\n",
        "        os.makedirs(drive_model_dir, exist_ok=True)\n",
        "        files = glob.glob(os.path.join(source_ckpt_dir, \"*_best.pth\"))\n",
        "\n",
        "        if files:\n",
        "            best_model = files[0]\n",
        "            filename = os.path.basename(best_model)\n",
        "            dest_path = os.path.join(drive_model_dir, filename)\n",
        "            shutil.copy(best_model, dest_path)\n",
        "            print(f\"\\U0001F4BE Success! Model saved to: {dest_path}\")\n",
        "        else:\n",
        "            print(\"\\u26A0\\uFE0F Warning: No '_best.pth' file found.\")\n",
        "    else:\n",
        "        print(f\"\\u274c Error: Checkpoint folder missing in {latest_run_dir}\")"
      ],
      "metadata": {
        "id": "1tfOQoTPqBcL",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9ad77850-359c-4b9e-a305-c93d28daf5e4"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " Searching for runs matching: /content/PointNeXt/log/shapenetpart/shapenetpart-train-custom_fallen_trees*...\n",
            "‚úÖ Found latest run: shapenetpart-train-custom_fallen_trees-ngpus1-seed7520-20251212-155340-E84zcZ3REWTnK2wbniefub\n",
            "üíæ Success! Model saved to: /content/drive/MyDrive/ML_Projects/PointNeXt/Models/shapenetpart-train-custom_fallen_trees-ngpus1-seed7520-20251212-155340-E84zcZ3REWTnK2wbniefub_ckpt_best.pth\n"
          ]
        }
      ]
    }
  ]
}